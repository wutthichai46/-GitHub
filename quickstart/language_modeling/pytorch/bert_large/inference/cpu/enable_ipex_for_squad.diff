diff --git a/examples/question-answering/run_squad.py b/examples/question-answering/run_squad.py
index 2bd4e90..e18a296 100644
--- a/examples/question-answering/run_squad.py
+++ b/examples/question-answering/run_squad.py
@@ -22,6 +22,9 @@ import logging
 import os
 import random
 import timeit
+import time
+import sys
+import threading

 import numpy as np
 import torch
@@ -45,19 +48,22 @@ from transformers.data.metrics.squad_metrics import (
     squad_evaluate,
 )
 from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor
-
+import intel_extension_for_pytorch as ipex

 try:
     from torch.utils.tensorboard import SummaryWriter
 except ImportError:
     from tensorboardX import SummaryWriter

-
 logger = logging.getLogger(__name__)

 MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())
 MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

+def trace_handler(prof):
+    print(prof.key_averages().table(
+        sort_by="self_cpu_time_total", row_limit=-1))
+    prof.export_chrome_trace("./log/test_trace_" + str(prof.step_num) + ".json")

 def set_seed(args):
     random.seed(args.seed)
@@ -264,19 +270,95 @@ def train(args, train_dataset, model, tokenizer):

     return global_step, tr_loss / global_step

+def wrap_model(model, args):
+    model.eval()
+    ipex.nn.utils._model_convert.replace_dropout_with_identity(model)
+    dumpy_tensor = torch.ones((args.eval_batch_size, 384), dtype=torch.long)
+    jit_inputs = (dumpy_tensor, dumpy_tensor, dumpy_tensor)
+    print(args)
+    if args.int8:
+        conf = ipex.quantization.QuantConf(configure_file=args.int8_config)
+        # convert model to trace model.
+        if args.int8_fp32:
+            model = ipex.quantization.convert(model, conf, jit_inputs)
+        else: #int8_bf16 mix
+            with torch.cpu.amp.autocast():
+                model = ipex.quantization.convert(model, conf, jit_inputs)
+        input = {
+                 "input_ids": dumpy_tensor,
+                 "attention_mask": dumpy_tensor,
+                 "token_type_ids": dumpy_tensor,
+         }
+        # enable fusion path work(need to run two interation).
+        with torch.no_grad():
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #dumpy_tensor = torch.ones((128, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #dumpy_tensor = torch.ones((81, 384), dtype=torch.long)
+            #y = model(dumpy_tensor, dumpy_tensor, dumpy_tensor)
+            #print("############################################")
+    elif args.bf16:
+        model = ipex.optimize(model, dtype=torch.bfloat16)
+        with torch.cpu.amp.autocast(),torch.no_grad():
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            model = torch.jit.freeze(model)
+            #model = torch.jit._recursive.wrap_cpp_module(torch._C._freeze_module(model._c, preserveParameters=True))
+            print(model.graph)
+    elif args.use_jit:
+        with torch.no_grad():
+            model = torch.jit.trace(model, jit_inputs, strict=False)
+            #model = torch.jit._recursive.wrap_cpp_module(torch._C._freeze_module(model._c, preserveParameters=True))
+            model = torch.jit.freeze(model)
+    return model
+
+def benchmark_evaluate(args, model, eval_dataloader):
+    total_time = 0
+    i = 0;
+    #with torch.profiler.profile(
+    #        activities=[
+    #            torch.profiler.ProfilerActivity.CPU],
+
+    #        schedule=torch.profiler.schedule(
+    #            wait=1,
+    #            warmup=9,
+    #            active=5),
+    #        #on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/bert_bf16'),#trace_handler
+    #        on_trace_ready=trace_handler#torch.profiler.tensorboard_trace_handler('./log/bert_bf16')
+    #        # used when outputting for tensorboard
+    #        ) as prof:
+
+    for batch in tqdm(eval_dataloader, desc="Evaluating"):
+        with torch.no_grad():
+            inputs = {
+                "input_ids": batch[0],
+                "attention_mask": batch[1],
+                "token_type_ids": batch[2],
+            }
+            time_start = time.time()
+            outputs = model(**inputs)
+            #prof.step()
+            time_end = time.time()
+            i += 1
+            if i > args.perf_begin_iter:
+                total_time +=(time_end - time_start)
+            if i >= args.perf_run_iters + args.perf_begin_iter:
+                throughput = args.eval_batch_size * args.perf_run_iters / total_time
+                print("Throughput: {:.3f} sentence/s".format(throughput))
+                break

 def evaluate(args, model, tokenizer, prefix=""):
     dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)
-
     if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
         os.makedirs(args.output_dir)
-
+
     args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)

+    model = wrap_model(model, args)
     # Note that DistributedSampler samples randomly
     eval_sampler = SequentialSampler(dataset)
     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
-
+
     # multi-gpu evaluate
     if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
         model = torch.nn.DataParallel(model)
@@ -285,10 +367,38 @@ def evaluate(args, model, tokenizer, prefix=""):
     logger.info("***** Running evaluation {} *****".format(prefix))
     logger.info("  Num examples = %d", len(dataset))
     logger.info("  Batch size = %d", args.eval_batch_size)
-
+
+    if args.do_calibration:
+        conf = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)
+        for step, batch in enumerate(eval_dataloader):
+            print("calibration step: {}".format(step))
+            batch = {
+                "input_ids": batch[0],
+                "attention_mask": batch[1],
+                "token_type_ids": batch[2],
+            }
+            with ipex.quantization.calibrate(conf):
+                model(**batch)
+            if step == args.calibration_iters -1:
+                print("calibration finished, quantization info saved on file {}".format(args.int8_config))
+                conf.save(args.int8_config)
+                exit()
+    if args.benchmark:
+        if args.use_share_weight:
+            threads = []
+            num_instances = args.total_cores // args.cores_per_instance
+            for i in range(0, num_instances):
+               t = threading.Thread(target=benchmark_evaluate, args=(args, model, eval_dataloader))
+               threads.append(t)
+               t.start()
+            for t in threads:
+                t.join()
+        else:
+            benchmark_evaluate(args, model, eval_dataloader)
+        exit()
     all_results = []
     start_time = timeit.default_timer()
-
+
     for batch in tqdm(eval_dataloader, desc="Evaluating"):
         model.eval()
         batch = tuple(t.to(args.device) for t in batch)
@@ -658,6 +768,36 @@ def main():
     parser.add_argument("--server_port", type=str, default="", help="Can be used for distant debugging.")

     parser.add_argument("--threads", type=int, default=1, help="multiple threads for converting example to features")
+    parser.add_argument(
+        "--bf16",
+        action="store_true",
+        help="Whether to use 16-bit (mixed) precision  instead of 32-bit")
+    parser.add_argument("--perf_begin_iter", type=int, default=15,
+                        help="Number iterations to warm up")
+    parser.add_argument("--perf_run_iters", type=int, default=100,
+                        help="Number iterations to collection performance data begin from perf_begin_iter")
+    parser.add_argument("--iter_num", type=int, default=40,
+                        help="Number iterations to collect time")
+    parser.add_argument("--benchmark", action='store_true',
+                        help="Bench the model speed")
+    parser.add_argument("--use_jit", action='store_true', help="For jit trace")
+    parser.add_argument('--int8', dest='int8', action='store_true',
+                        help='use llga int8 in pytorch jit model')
+    parser.add_argument('--int8_fp32', dest='int8_fp32', action='store_true',
+                        help='use int8 fp32 mix precision')
+    parser.add_argument("--int8_config", type=str, default="config.json",
+                        help="quantization config file for int8 mode")
+    parser.add_argument("--do_calibration", action='store_true',
+                        help="Enable calibration process")
+    parser.add_argument("--calibration_iters", type=int, default=100,
+                        help="Number iterations to do calibration")
+    parser.add_argument("--use_share_weight", action='store_true',
+                        help="Enable share weight mode")
+    parser.add_argument("--cores_per_instance", type=int, default=4,
+                        help="Number iterations to collect time")
+    parser.add_argument("--total_cores", type=int, default=28,
+                        help="Total cores used for this process, used for share_weight mode")
+
     args = parser.parse_args()

     if args.doc_stride >= args.max_seq_length - args.max_query_length:
diff --git a/src/transformers/modeling_bert.py b/src/transformers/modeling_bert.py
index 23d25cf..b281147 100644
--- a/src/transformers/modeling_bert.py
+++ b/src/transformers/modeling_bert.py
@@ -139,7 +139,7 @@ def mish(x):
     return x * torch.tanh(nn.functional.softplus(x))


-ACT2FN = {"gelu": gelu, "relu": torch.nn.functional.relu, "swish": swish, "gelu_new": gelu_new, "mish": mish}
+ACT2FN = {"gelu": torch.nn.functional.gelu, "relu": torch.nn.functional.relu, "swish": swish, "gelu_new": gelu_new, "mish": mish}


 BertLayerNorm = torch.nn.LayerNorm
@@ -239,6 +239,8 @@ class BertSelfAttention(nn.Module):
         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
         if attention_mask is not None:
+            if attention_mask.dtype != attention_scores.dtype:
+                attention_mask = attention_mask.to(attention_scores.dtype)
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask

