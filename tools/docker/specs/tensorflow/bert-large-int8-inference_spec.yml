releases:
  versioned:
    tag_specs:
    - '{_TAG_PREFIX}{intel-tf}{language-modeling}{bert-large-int8-inference}'
slice_sets:
  bert-large-int8-inference:
  - add_to_name: -bert-large-int8-inference
    args:
    - TENSORFLOW_IMAGE=intel/intel-optimized-tensorflow
    - PACKAGE_NAME=bert-large-int8-inference
    dockerfile_subdirectory: model_containers
    documentation:
      docs:
      - name: Title
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/title.md
      - name: Description
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/description.md
      - name: Datasets
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/datasets.md
      - name: Quick Start Scripts
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/quickstart.md
      - name: Docker
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/docker.md
      - name: License
        uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8/.docs/license.md
      name: README.md
      text_replace:
        <docker image>: intel/language-modeling:tf-r2.5-icx-b631821f-bert-large-int8-inference
        <mode>: inference
        <model name>: BERT Large
        <package dir>: bert-large-int8-inference
        <package name>: bert-large-int8-inference.tar.gz
        <package url>: https://storage.googleapis.com/intel-optimized-tensorflow/models/v2_2_0/bert-large-int8-inference.tar.gz
        <precision>: Int8
        <use case>: language_modeling
      uri: models/quickstart/language_modeling/tensorflow/bert_large/inference/int8
    downloads:
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/r2.5-icx-b631821f/asymmetric_per_channel_bert_int8.pb
        destination: pretrained_model/asymmetric_per_channel_bert_int8.pb
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6_1/bert_large_checkpoints.zip
        destination: pretrained_model/bert_large_checkpoints.zip
    files:
    - destination: benchmarks/common
      source: benchmarks/common
    - destination: benchmarks/language_modeling/tensorflow/bert_large/README.md
      source: benchmarks/language_modeling/tensorflow/bert_large/README.md
    - destination: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
      source: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
    - destination: benchmarks/language_modeling/tensorflow/bert_large/inference/__init__.py
      source: benchmarks/language_modeling/tensorflow/bert_large/inference/__init__.py
    - destination: benchmarks/language_modeling/tensorflow/bert_large/inference/int8
      source: benchmarks/language_modeling/tensorflow/bert_large/inference/int8
    - destination: benchmarks/launch_benchmark.py
      source: benchmarks/launch_benchmark.py
    - destination: models/common
      source: models/common
    - destination: models/language_modeling/tensorflow/bert_large/inference
      source: models/language_modeling/tensorflow/bert_large/inference
    - destination: quickstart/common
      source: quickstart/common
    - destination: quickstart
      source: quickstart/language_modeling/tensorflow/bert_large/inference/int8
    partials:
    - unzip
    - model_package
    - entrypoint
