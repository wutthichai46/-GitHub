[
  {
    "_comment": "Transformer LT official FP32 online inference",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_translation --model-name=transformer_lt_official --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --batch-size=1 --socket-id=0  --benchmark-only --in-graph=fp32_graphdef.pb --data-location=/dataset --output-dir=/workspace/logs --file=newstest2014.en --file_out=out_translate.txt --reference=newstest2014.de --vocab_file=vocab.txt",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/inference/fp32/infer_ab.py --param_set=big --in_graph=fp32_graphdef.pb --batch_size=1 --file=newstest2014.en --file_out=/workspace/logs/out_translate.txt --vocab_file=vocab.txt --num_inter=1 --num_intra=28",
    "cpuset": "0-111"
  },
  {
    "_comment": "Transformer LT official FP32 batch inference",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_translation --model-name=transformer_lt_official --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --batch-size=64 --socket-id=0  --benchmark-only --in-graph=fp32_graphdef.pb --data-location=/dataset --output-dir=/workspace/logs --file=newstest2014.en --file_out=out_translate.txt --reference=newstest2014.de --vocab_file=vocab.txt",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/inference/fp32/infer_ab.py --param_set=big --in_graph=fp32_graphdef.pb --batch_size=64 --file=newstest2014.en --file_out=/workspace/logs/out_translate.txt --vocab_file=vocab.txt --num_inter=1 --num_intra=28",
    "cpuset": "0-111"
  }
]
