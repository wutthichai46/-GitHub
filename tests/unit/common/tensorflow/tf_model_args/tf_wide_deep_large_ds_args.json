[
  { "_comment": "Int8 benchmark command",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=int8 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_int8_pretrained_model.pb --data-location=/dataset",
    "output": "LD_PRELOAD=/usr/lib/libtcmalloc.so.4.2.6 numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --input_graph=/in_graph/wide_deep_int8_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"},

  { "_comment": "Int8 latency benchmark command",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=int8 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_int8_pretrained_model.pb --data-location=/dataset",
    "output": "LD_PRELOAD=/usr/lib/libtcmalloc.so.4.2.6 numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --batch_size=1 --input_graph=/in_graph/wide_deep_int8_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"},

  { "_comment": "Int8 command for throughput benchmark",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=int8 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=512 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_int8_pretrained_model.pb --data-location=/dataset",
    "output": "LD_PRELOAD=/usr/lib/libtcmalloc.so.4.2.6 numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --batch_size=512 --input_graph=/in_graph/wide_deep_int8_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"},

  { "_comment": "FP32 benchmark command",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data-location=/dataset",
    "output": "numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --input_graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"},

  { "_comment": "Fp32 command for throughput benchmark",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=512 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data-location=/dataset",
    "output": "numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --batch_size=512 --input_graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"},

  { "_comment": "Fp32 latency benchmark command",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=recommendation --model-name=wide_deep_large_ds --precision=fp32 --mode=inference --model-source-dir=/workspace/models --benchmark-dir=/workspace/benchmarks --intelai-models=/workspace/intelai_models --num-cores=-1 --batch-size=1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --in-graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data-location=/dataset",
    "output": "numactl --physcpubind=0--2 --membind=0 python /workspace/intelai_models/inference/inference.py --num_intra_threads=1 --num_inter_threads=28 --batch_size=1 --input_graph=/in_graph/wide_deep_fp32_pretrained_model.pb --data_location=/dataset --num_omp_threads=1"}
]


