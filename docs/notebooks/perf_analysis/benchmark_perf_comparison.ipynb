{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Compare Model Zoo Benchmark performance between Intel optimized and stock Tensorflow\n",
    "\n",
    "This jupyter notebook will help you evaluate performance benefits from Intel-optimized Tensorflow via several pre-trained models from Intel Model Zoo. \n",
    "The notebook will show users a bar chart like below for performance comparison among Stock and Intel Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\perf_comparison.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# Section 1: Display Platform Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users should change the value of os.environ['ModelZooRoot'] according to their environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If default path does not work, change ModelZooRoot path according to your environment\n",
    "## USER INPUT\n",
    "current_path = os.getcwd()\n",
    "os.environ['ModelZooRoot'] = current_path + \"/../../../\"\n",
    "os.environ['ProfileUtilsRoot'] = os.environ['ModelZooRoot'] + \"docs/notebooks/perf_analysis/profiling/\"\n",
    "print(os.environ['ModelZooRoot'])\n",
    "print(os.environ['ProfileUtilsRoot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for mandatory python scripts after ModelZooRoot and ProfileUtilsRoot are assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "benchmark_path = os.environ['ModelZooRoot'] + \"benchmarks/launch_benchmark.py\"\n",
    "if os.path.exists(benchmark_path) == True:\n",
    "    print(benchmark_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find benchmark script!\")\n",
    "    \n",
    "profile_utils_path = os.environ['ProfileUtilsRoot'] + \"profile_utils.py\"\n",
    "if os.path.exists(profile_utils_path) == True:\n",
    "    print(profile_utils_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find profile_utils script!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Run the benchmark on the selected Jupyter Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check TensorFlow version and MKL enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (\"We are using Tensorflow version\", tf.__version__)\n",
    "major_version = int(tf.__version__.split(\".\")[0])\n",
    "if major_version >= 2:\n",
    "    from tensorflow.python import _pywrap_util_port\n",
    "    on_mkl = _pywrap_util_port.IsMklEnabled()\n",
    "else:\n",
    "    on_mkl = tf.pywrap_tensorflow.IsMklEnabled()\n",
    "print(\"MKL enabled:\", on_mkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure parameters for launch_benchmark.py according to the selected Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: List out the supported topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from profiling.profile_utils import ConfigFile\n",
    "\n",
    "accuracy_only=False\n",
    "\n",
    "config = ConfigFile()\n",
    "sections = config.read_supported_section(accuracy_only=accuracy_only)\n",
    "print(\"Supported topologies: \")\n",
    "index =0 \n",
    "for section in sections:\n",
    "    print(\" %d: %s \" %(index, section))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Pick a topology. \n",
    "#### ACTION : Please select one supported topology and change topo_index accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User picks a topology, Batch Size, and number of required threads\n",
    "## USER INPUT\n",
    "topo_index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List out the selected topology name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if topo_index >= len(sections):\n",
    "    print(\"ERROR! please input a topo_index within range\")\n",
    "else:\n",
    "    topology_name=sections[topo_index]\n",
    "    print(topology_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 Import Model Zoo CPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['ModelZooRoot']+os.sep+'benchmarks/common/')\n",
    "from platform_util import PlatformUtil \n",
    "cpu_info = PlatformUtil(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: User can also manually set batch size and number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "numa_nodes = cpu_info.numa_nodes\n",
    "print(\"CPU count per socket:\" , cpu_info.cores_per_socket ,\" \\nSocket count:\", cpu_info.sockets, \" \\nNuma nodes:\",numa_nodes)\n",
    "if numa_nodes > 0:\n",
    "    socket_number = 1\n",
    "    cpu_count = cpu_info.cores_per_socket\n",
    "    inter_thread = 1\n",
    "else:\n",
    "    # on non-numa machine, we should use all the cores and don't use numactl\n",
    "    socket_number = -1\n",
    "    cpu_count = cpu_info.cores_per_socket * cpu_info.sockets\n",
    "    inter_thread = cpu_info.sockets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users can change the value of thread_number and batch_size to see different performance\n",
    "1. thread_umber: the value will apply to num_cores parameters in launch_benchmark.py  \n",
    "2. utilized_socket_number:  the value will apply to the socket-id parameter in launch_benchmark.py \n",
    "3. num_inter_threads: the value will  apply to the num-inter-threads parameter in launch_benchmark.py \n",
    "4. num_intra_threads: the value will  apply to the num-intra-threads parameter in launch_benchmark.py \n",
    "5. batch_size: the value will apply to the batch_size parameter in launch_benchmark.py \n",
    "6. log_folder: the folder where the logs are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "thread_number=cpu_count \n",
    "utilized_socket_number=socket_number\n",
    "num_inter_threads = inter_thread\n",
    "num_intra_threads = thread_number\n",
    "batch_size=32\n",
    "log_folder=os.getcwd() + os.sep + \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: get required data and files if needed.\n",
    "No action if there is no output from this below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from config\n",
    "config = ConfigFile()\n",
    "config.read_config(topology_name)\n",
    "if config.data_download != '' and config.data_location == '':\n",
    "    print(\"\\nPlease follow below command to get the data : \")\n",
    "    val = config.read_value_from_section(topology_name, 'data-download')\n",
    "    print(val)\n",
    "if config.preprocessing != '':\n",
    "    print(\"\\nPlease follow below command to get required files and installation : \")\n",
    "    val = config.read_value_from_section(topology_name, 'preprocessing')\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users set the configuration in topo.ini for those required data and files\n",
    "1. data_download_path: the value will be set as data-location parameters in topo.ini for the related topology.\n",
    "2. model_source_dir: the value will be set as the model-source-dir parameter in topo.ini for the related topology such as Wide and Deep. Those models use tensorflow-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "data_download_path = ''\n",
    "model_source_dir = ''\n",
    "\n",
    "# Overwrite configurations in topo.ini\n",
    "config = ConfigFile()\n",
    "config.read_config(topology_name)\n",
    "\n",
    "if config.data_download != '' and data_download_path != '':\n",
    "    config.write_value_from_section(topology_name, 'data-location', data_download_path)\n",
    "    config.data_location = data_download_path\n",
    "    \n",
    "if accuracy_only == True and config.data_location == '':\n",
    "    print(\"ERROR! STOP! need data for accuacy evaluatoin!\")\n",
    "    \n",
    "if config.preprocessing != '':\n",
    "    config.write_value_from_section(topology_name, 'model-source-dir', model_source_dir)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6: Prepare pre-trained model and model parameters for running the benchmark\n",
    "1. Get related parameters according to selected topology\n",
    "2. Get pretrained model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = ConfigFile()\n",
    "configvals = []\n",
    "# Get common parameters according to users' inputs  \n",
    "params = config.get_parameters(topology_name, configvals,\n",
    "                   batch_size=batch_size, thread_number=thread_number, socket_number=utilized_socket_number,\n",
    "                   num_inter_threads=num_inter_threads, num_intra_threads=num_intra_threads, accuracy_only=accuracy_only)\n",
    "\n",
    "# Get the parameters from config\n",
    "configvals=config.read_config(topology_name)\n",
    "\n",
    "# Get the pre-trained model file\n",
    "if config.wget != '' and ( config.in_graph == '' or config.checkpoint == ''  ):\n",
    "    pretrain_model_path = config.download_pretrained_model(current_path=current_path)\n",
    "    pretrain_model_path = config.uncompress_file(pretrain_model_path, current_path=current_path)\n",
    "    if config.in_graph == 'NA':\n",
    "        config.checkpoint = pretrain_model_path        \n",
    "    if config.checkpoint == 'NA':\n",
    "        config.in_graph = pretrain_model_path \n",
    "# set pre-trained model path        \n",
    "if config.checkpoint == 'NA':\n",
    "    configvals.append(\"--in-graph\")\n",
    "    configvals.append(config.in_graph)\n",
    "if config.in_graph == 'NA':\n",
    "    configvals.append(\"--checkpoint\")\n",
    "    configvals.append(config.checkpoint)\n",
    "    \n",
    "#Set output-dir folder\n",
    "if log_folder !='':\n",
    "    configvals.append(\"--output-dir\")\n",
    "    configvals.append(log_folder)\n",
    "\n",
    "# Combine common parameters and config parameters\n",
    "params = params + configvals    \n",
    "    \n",
    "sys.argv=[benchmark_path]+params\n",
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7: Create a CSV file to log the performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PerfPresenter\n",
    "job_type = topology_name.split(' ')[1]#'inference'\n",
    "csv_fname=job_type+'_'+topology_name.replace(' ', '')+'.csv'\n",
    "perfp=PerfPresenter()\n",
    "perfp.create_csv_logfile(job_type, csv_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Run the benchmark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Below section will enable Tensorflow timeline for the model by patching it, and then unpatch it after the model completes its training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set enable_tf_timeline to False if users don't want to get TF timeline information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_tf_timeline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_tf_timeline == True:\n",
    "    # patch related model script\n",
    "    repo_path = os.environ['ModelZooRoot'] #current_path + os.sep + \"../../\"\n",
    "    config.patch_model_to_enable_timeline(repopath=repo_path)\n",
    "\n",
    "# run the benchmark with the patch\n",
    "import sys\n",
    "benchmark_path = os.environ['ModelZooRoot']+os.sep+\"benchmarks/\"\n",
    "sys.path.append(benchmark_path)\n",
    "from launch_benchmark import LaunchBenchmark\n",
    "\n",
    "util = LaunchBenchmark()\n",
    "util.main()\n",
    "\n",
    "if enable_tf_timeline == True:\n",
    "    # unpatch related model script\n",
    "    config.unpatch_model_to_enable_timeline(model_path=repo_path+'/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse output for performance number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Found the file path of the related runtime log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the path of the latest log file\n",
    "configvals=config.read_config(topology_name)\n",
    "import os\n",
    "for file in os.listdir(log_folder):\n",
    "    if file.endswith(\".log\"):\n",
    "        logpath = os.path.join(log_folder, file)\n",
    "        used_logpath = logpath + \".old\"\n",
    "        os.rename(logpath, used_logpath)\n",
    "        print(used_logpath)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the logfile for performance number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy_only == False:\n",
    "    print(\"get throughput\")\n",
    "    val = config.throughput_keyword\n",
    "    index = int(config.throughput_index)\n",
    "    line = perfp.read_throughput(used_logpath, keyword=val, index=index)\n",
    "    if line!=None:\n",
    "        throughput=line\n",
    "        print(throughput)\n",
    "        # log the perf number\n",
    "        perfp.log_infer_perfcsv(0, throughput, 0, csv_fname)\n",
    "    else:\n",
    "        print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")\n",
    "else:\n",
    "    # get accuracy number and caculate throughput\n",
    "    print(\"get accuracy and throughput\")\n",
    "    #val = config.throughput_keyword\n",
    "    #index = int(config.throughput_index)\n",
    "    accuracy = perfp.read_accuracy(used_logpath)\n",
    "    iternation = perfp.read_iteration_time(used_logpath)\n",
    "    if accuracy != [] and iternation != []:\n",
    "        final_accuracy=accuracy[-1]\n",
    "        iternation_time = iternation[-1]\n",
    "        throughput = float(batch_size)/iternation_time\n",
    "        print(final_accuracy,throughput)\n",
    "        # log the perf number\n",
    "        perfp.log_infer_perfcsv(0, throughput, final_accuracy, csv_fname)\n",
    "    else:\n",
    "        print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional : print out the log file for runtime issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = open(used_logpath)\n",
    "logout = logfile.read()\n",
    "print(logout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users should be able to see a new Timeline json file after running the benchmark\n",
    "If users don't see a new timeline json file, they need to make sure that they patch the model script correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import CommonUtils\n",
    "utils = CommonUtils()\n",
    "paths = []\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"benchmarks\")\n",
    "paths.append(os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\")\n",
    "pattern = \"*.json\"\n",
    "timeline_files, timeline_paths = utils.found_files_in_folders(pattern, paths)\n",
    "if timeline_paths == []:\n",
    "    print(\"No %s files found\" %(pattern))\n",
    "else:\n",
    "    print(timeline_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Sections 1 and 2 on different Jupyter kernel\n",
    "Users can find information in docs/notebooks/perf_analysis/README.md for switching among different Juypter kernels.  \n",
    "> Go back Section 1 by clicking the link : [Section 1](#section_1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Results\n",
    "## Draw the performance comparison diagram\n",
    ">NOTE: Please go over Sections 1 and 2 on different Jupyter kernel before comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from profiling.profile_utils import PerfPresenter\n",
    "\n",
    "perfp=PerfPresenter(True)\n",
    "# inference  throughput\n",
    "perfp.draw_perf_diag_from_csv(csv_fname,'throughput','throughput (image/sec)', topology_name)\n",
    "perfp.draw_perf_ratio_diag_from_csv(csv_fname,'throughput','speedup', topology_name)\n",
    "if accuracy_only == True:\n",
    "    perfp.draw_perf_diag_from_csv(csv_fname,'accuracy','accuracy', topology_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather all generated Tensorflow Timeline Json files\n",
    "Copy the timeline json file from benchmark folder to the Timeline folder with time information.\n",
    "Those Timeline files will be analyzed in another Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import datetime\n",
    "from profiling.profile_utils import CommonUtils\n",
    "utils = CommonUtils()\n",
    "\n",
    "if timeline_paths != []:\n",
    "    timeline_dir_path = os.environ['ModelZooRoot']+os.sep + \"docs/notebooks/perf_analysis\" + os.sep + \"Timeline\"\n",
    "    if os.path.isfile(timeline_dir_path) == False:\n",
    "        os.mkdir(timeline_dir_path)\n",
    "    for path in timeline_paths:\n",
    "        shutil.move(path,timeline_dir_path)\n",
    "    # rename Timeline folder with topo and time info   \n",
    "    timeinfo = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "    target_path = timeline_dir_path+'_'+timeinfo\n",
    "    shutil.move(timeline_dir_path, target_path)\n",
    "    print(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel-tensorflow",
   "language": "python",
   "name": "intel-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
