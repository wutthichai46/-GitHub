[resnet50 infer fp32]
model-name = resnet50
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
data-download-accuracy = To get training data, please refer to the page "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet50_fp32_pretrained_model.pb
json-fname = timeline_resnet50_fp32_infer_merged_runs.json
patches = 0001-resnet50-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = True
batch-size = 128
validated = True

[resnet50 infer int8]
model-name = resnet50
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = 
data-download-accuracy = To get training data, please refer to the page "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet50_int8_pretrained_model.pb
json-fname = timeline_resnet50_int8_infer_merged_runs.json
patches = 0001-resnet50-int8-TF-timeline.patch
mkl-only = True
support-accuracy = True
batch-size = 128
validated = True

[resnet50v1_5 infer fp32]
model-name = resnet50v1_5
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://zenodo.org/record/2535873/files/resnet50_v1.pb
json-fname = timeline_resnet50v1_5_fp32_infer_merged_runs.json
patches = 0001-resnet50v1_5-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 128
validated = True

[resnet50v1_5 infer bf16]
model-name = resnet50v1_5
in-graph = 
mode = inference
precision = bfloat16
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet50_v1_5_bfloat16.pb
json-fname = timeline_resnet50v1_5_bf16_infer_merged_runs.json
patches = 0001-resnet50v1_5-fp32-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 128
validated = True

[resnet50v1_5 infer int8]
model-name = resnet50v1_5
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet50v1_5_int8_pretrained_model.pb
json-fname = timeline_resnet50v1_5_int8_infer_merged_runs.json
patches = 0001-resnet50v1_5-fp32-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 128
validated = True

[resnet50v1_5 train fp32]
model-name = resnet50v1_5
mode = training
precision = fp32
checkpoint = ./
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
throughput-keyword = global_step/sec
throughput-index = -1
json-fname = timeline_resnet50v1_5_fp32_train_merged_runs.json
patches = 0001-resnet50v1_5-train-fp32.patch
mkl-only = False
support-accuracy = False
custom-args = steps=200
validated = False

[resnet50v1_5 train bfloat16]
model-name = resnet50v1_5
mode = training
precision = bfloat16
checkpoint = ./
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/tree/master/datasets/imagenet/README.md"
throughput-keyword = global_step/sec
throughput-index = -1
json-fname = timeline_resnet50v1_5_bf16_train_merged_runs.json
patches = 0001-resnet50v1_5-train-bfloat16.patch
mkl-only = True
support-accuracy = False
custom-args = steps=200
validated = False

[densenet169 infer fp32]
model-name = densenet169
in-graph = 
mode = inference
precision = fp32
throughput-keyword = images/sec
throughput-index = 3
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/densenet169_fp32_pretrained_model.pb
json-fname = timeline_densenet169_fp32_infer_merged_runs.json
patches = 0001-densenet169-infer-fp32.patch
mkl-only = False
support-accuracy = False
batch-size = 100
validated = True

[wide_deep infer fp32]
model-name = wide_deep
mode = inference
precision = fp32
throughput-keyword = Throughput
throughput-index = -1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/wide_deep_fp32_pretrained_model.tar.gz
json-fname = timeline_wide_deep_fp32_infer_merged_runs.json
patches = 0001-wide_deep-infer-fp32.patch
model-source-dir = 
data-location = 
checkpoint = 
data-download = python benchmarks/recommendation/tensorflow/wide_deep/inference/fp32/data_download.py --data_dir widedeep_dataset #Then set data-location folder as CURRENTPATH/widedeep_dataset
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models; cd tensorflow-models; git fetch origin pull/7461/head:wide-deep-tf2 ; git checkout wide-deep-tf2 # Then set model-source-dir as CURRENTPATH/tensorflow-models
mkl-only = True
support-accuracy = False
batch-size = 1024
validated = True

[bert_large infer fp32]
model-name = bert_large
mode = inference
precision = fp32
throughput-keyword = Elapsedtime
throughput-index = -1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/bert_large_checkpoints.zip
json-fname = timeline_bert_large_fp32_infer_merged_runs.json
patches = 0001-bert_large-infer-fp32.patch
data-location = 
checkpoint = 
output-dir = 
infer-option = =SQuAD
profile = =True
data-download = wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip; unzip wwm_uncased_L-24_H-1024_A-16.zip; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -P wwm_uncased_L-24_H-1024_A-16 #Then set data-location folder as CURRENTPATH/wwm_uncased_L-24_H-1024_A-16
mkl-only = False
support-accuracy = False
batch-size = 32
validated = False

[bert_large train fp32]
model-name = bert_large
mode = training
precision = fp32
data-location = 
data-download = wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip; unzip wwm_uncased_L-24_H-1024_A-16.zip; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -P wwm_uncased_L-24_H-1024_A-16; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -P wwm_uncased_L-24_H-1024_A-16 #Then set data-location folder as CURRENTPATH/wwm_uncased_L-24_H-1024_A-16
throughput-keyword = examples/sec
throughput-index = -1
wget = https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
json-fname = timeline_bert_large_fp32_train_merged_runs.json
patches = 0001-bert_large-train-fp32.patch
mkl-only = False
support-accuracy = False
batch-size = 24
custom-args = train_option=SQuAD  vocab_file=$data-location$/vocab.txt  config_file=$data-location$/bert_config.json init_checkpoint=$data-location$/bert_model.ckpt do_train=True train_file=$data-location$/train-v1.1.json do_predict=True  predict_file=$data-location$/dev-v1.1.json learning_rate=3e-5 num_train_epochs=1 max_seq_length=384 doc_stride=128 optimized_softmax=True experimental_gelu=False do_lower_case=True
validated = False

[bert_large train bfloat16]
model-name = bert_large
mode = training
precision = bfloat16
data-location = 
data-download = wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip; unzip wwm_uncased_L-24_H-1024_A-16.zip; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -P wwm_uncased_L-24_H-1024_A-16; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -P wwm_uncased_L-24_H-1024_A-16  #Then set data-location folder as CURRENTPATH/wwm_uncased_L-24_H-1024_A-16
throughput-keyword = examples/sec
throughput-index = -1
wget = https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
json-fname = timeline_bert_large_bf16_train_merged_runs.json
patches = 0001-bert_large-train-bfloat16.patch
mkl-only = True
support-accuracy = False
batch-size = 24
custom-args = train_option=SQuAD  vocab_file=$data-location$/vocab.txt  config_file=$data-location$/bert_config.json init_checkpoint=$data-location$/bert_model.ckpt do_train=True train_file=$data-location$/train-v1.1.json do_predict=True  predict_file=$data-location$/dev-v1.1.json learning_rate=3e-5 num_train_epochs=1 max_seq_length=384 doc_stride=128 optimized_softmax=True experimental_gelu=False do_lower_case=True
validated = False

[faster_rcnn infer fp32]
model-name = faster_rcnn
checkpoint = 
mode = inference
precision = fp32
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md, The coco_val.record file will be used in the following inference examples."
throughput-keyword = BATCH:
throughput-index = -2
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/faster_rcnn_resnet50_fp32_coco_pretrained_model.tar.gz
json-fname = timeline_faster_rcnn_fp32_infer_merged_runs.json
patches = 0001-faster_rcnn-fp32-TF-timeline.patch
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models-frcnn; cd tensorflow-models-frcnn; git checkout tags/v1.12.0 # Then set model-source-dir as CURRENTPATH/tensorflow-models-frcnn
mkl-only = False
support-accuracy = False
model-source-dir = 
custom-args = config_file=pipeline.config
validated = False

[faster_rcnn infer int8]
model-name = faster_rcnn
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md"
throughput-keyword = Avg.
throughput-index = -1
throughput-splitter = :
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/faster_rcnn_int8_pretrained_model.pb
json-fname = timeline_faster_rcnn_int8_infer_merged_runs.json
patches = 0001-faster_rcnn-int8-TF-timeline.patch
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models-frcnn; cd tensorflow-models-frcnn; git checkout tags/v1.12.0 # Then set model-source-dir as CURRENTPATH/tensorflow-models-frcnn
mkl-only = True
support-accuracy = False
model-source-dir = 
custom-args = number_of_steps=20
validated = False

[rfcn infer fp32]
model-name = rfcn
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md"
throughput-keyword = Avg.
throughput-index = -1
throughput-splitter = :
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/rfcn_resnet101_fp32_coco_pretrained_model.tar.gz
json-fname = timeline_rfcn_fp32_infer_merged_runs.json
patches = 0001-rfcn-fp32-TF-timeline.patch
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models-rfcn; cd tensorflow-models-rfcn; git checkout 6c21084503b27a9ab118e1db25f79957d5ef540b ; git apply ../../../models/object_detection/tensorflow/rfcn/inference/tf-2.0.patch; git clone https://github.com/cocodataset/cocoapi.git; cd research; protoc object_detection/protos/*.proto --python_out=.  # Then set model-source-dir as CURRENTPATH/tensorflow-models-rfcn
mkl-only = False
support-accuracy = False
model-source-dir = 
perf-bkm = rfcn only support batch_size=1 by default, and some issues to set socket-id. Therefore, please set utilized_socket_number=-1 and batch_size=-1
custom-args = number_of_steps=20
batch-size = 1
validated = True

[rfcn infer int8]
model-name = rfcn
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md"
throughput-keyword = Avg.
throughput-index = -1
throughput-splitter = :
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/rfcn_resnet101_int8_coco_pretrained_model.pb
json-fname = timeline_rfcn_int8_infer_merged_runs.json
patches = 0001-rfcn-int8-TF-timeline.patch
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models-rfcn; cd tensorflow-models-rfcn; git checkout 6c21084503b27a9ab118e1db25f79957d5ef540b ; git apply ../../../models/object_detection/tensorflow/rfcn/inference/tf-2.0.patch; git clone https://github.com/cocodataset/cocoapi.git; cd research; protoc object_detection/protos/*.proto --python_out=.  # Then set model-source-dir as CURRENTPATH/tensorflow-models-rfcn
mkl-only = True
support-accuracy = False
model-source-dir = 
perf-bkm = rfcn only support batch_size=1 by default, and some issues to set socket-id. Therefore, please set utilized_socket_number=-1 and batch_size=-1
custom-args = number_of_steps=20
batch-size = 1
validated = True

[inceptionv3 infer fp32]
model-name = inceptionv3
in-graph = 
mode = inference
precision = fp32
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/inceptionv3_fp32_pretrained_model.pb
json-fname = timeline_inceptionv3_fp32_infer_merged_runs.json
patches = 0001-inceptionv3-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 128
validated = True

[inceptionv4 infer fp32]
model-name = inceptionv4
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
throughput-keyword = images/sec
throughput-index = 3
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/inceptionv4_fp32_pretrained_model.pb
json-fname = timeline_inceptionv4_fp32_infer_merged_runs.json
patches = 0001-inceptionv4-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 240
validated = True

[inceptionv4 infer int8]
model-name = inceptionv4
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = 
throughput-keyword = images/sec
throughput-index = 3
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/inceptionv4_int8_pretrained_model.pb
json-fname = timeline_inceptionv4_int8_infer_merged_runs.json
patches = 0001-inceptionv4-int8-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 240
validated = True

[mobilenet_v1 infer fp32]
model-name = mobilenet_v1
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 2
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenet_v1_1.0_224_frozen.pb
json-fname = timeline_mobilenet_v1_fp32_infer_merged_runs.json
patches = 0001-mobilenet_v1-fp32-TF-timeline.patch
preprocessing = git clone https://github.com/tensorflow/models.git tensorflow-models # Then set model-source-dir as CURRENTPATH/tensorflow-models
mkl-only = False
support-accuracy = False
custom-args = input_height=224 input_width=224 warmup_steps=10 steps=50 input_layer="input" output_layer="MobilenetV1/Predictions/Reshape_1"
batch-size = 100
model-source-dir = 
validated = True

[mobilenet_v1 infer int8]
model-name = mobilenet_v1
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 2
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/mobilenetv1_int8_pretrained_model.pb
json-fname = timeline_mobilenet_v1_int8_infer_merged_runs.json
patches = 0001-mobilenet_v1-int8-TF-timeline.patch
mkl-only = True
support-accuracy = False
custom-args = input_height=224 input_width=224 warmup_steps=10 steps=50 input_layer="input" output_layer="MobilenetV1/Predictions/Reshape_1"
batch-size = 240
validated = False

[resnet101 infer fp32]
model-name = resnet101
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet101_fp32_pretrained_model.pb
json-fname = timeline_resnet101_fp32_infer_merged_runs.json
patches = 0001-resnet101-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 128
validated = True

[resnet101 infer int8]
model-name = resnet101
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/resnet101_int8_pretrained_model.pb
json-fname = timeline_resnet101_int8_infer_merged_runs.json
patches = 0001-resnet101-int8-TF-timeline.patch
mkl-only = True
support-accuracy = False
custom-args = warmup_steps=3 steps=10
batch-size = 128
validated = True

[wide_deep_large infer fp32]
model-name = wide_deep_large_ds
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = To get inference data, please refer to the page "https://github.com/IntelAI/models/blob/master/benchmarks/recommendation/tensorflow/wide_deep_large_ds/README.md#prepare-dataset"
throughput-keyword = Throughput
throughput-index = -1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/wide_deep_fp32_pretrained_model.pb
json-fname = timeline_wide_deep_large_fp32_infer_merged_runs.json
patches = 0001-wide_deep_large-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 512
validated = True

[wide_deep_large infer int8]
model-name = wide_deep_large_ds
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = To get inference data, please refer to the page "https://github.com/IntelAI/models/blob/master/benchmarks/recommendation/tensorflow/wide_deep_large_ds/README.md#prepare-dataset"
throughput-keyword = Throughput
throughput-index = -1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/wide_deep_int8_pretrained_model.pb
json-fname = timeline_wide_deep_large_int8_infer_merged_runs.json
patches = 0001-wide_deep_large-int8-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 512
validated = False

[bert_base infer fp32]
model-name = bert
mode = inference
precision = fp32
checkpoint = 
data-location = 
data-download = To get inference data, please refer to the page "https://github.com/IntelAI/models/blob/master/benchmarks/language_translation/tensorflow/bert/README.md#fp32-inference-instructions"
throughput-keyword = samples_per_sec
throughput-index = -1
wget = https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
json-fname = timeline_bert_fp32_infer_merged_runs.json
patches = 0001-bert-fp32-TF-timeline.patch
preprocessing = git clone --single-branch https://github.com/google-research/bert.git ; cd bert ; git checkout 88a817c37f788702a363ff935fd173b6dc6ac0d6 # Then set model-source-dir as CURRENTPATH/bert
mkl-only = False
support-accuracy = False
batch-size = 32
custom-args = task-name=MRPC max-seq-length=128 learning-rate=2e-5 num_train_epochs=1.0
model-source-dir = 
validated = True
#throughput-keyword-online = spent #Time spent per iteration (after warmup): 199.93 ms
#throughput-index-online = -2

[transformer_mlperf train fp32]
model-name = transformer_mlperf
mode = training
precision = fp32
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/benchmarks/language_translation/tensorflow/transformer_mlperf/README.md#fp32-training-instructions"
throughput-keyword = exp/sec
throughput-index = -1
json-fname = timeline_transformer_mlperf_fp32_train_merged_runs.json
patches = 0001-transformer_mlperf-train-fp32.patch
mkl-only = False
support-accuracy = False
custom-args = random_seed=11 train_steps=200 steps_between_eval=200 params=big save_checkpoints="No" do_eval="No" print_iter=50
validated = True

[transformer_mlperf train bfloat16]
model-name = transformer_mlperf
mode = training
precision = bfloat16
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/benchmarks/language_translation/tensorflow/transformer_mlperf/README.md#bfloat16-training-instructions"
throughput-keyword = exp/sec
throughput-index = -1
json-fname = timeline_transformer_mlperf_bfloat16_train_merged_runs.json
patches = 0001-transformer_mlperf-train-bfloat16.patch
mkl-only = True
support-accuracy = False
custom-args = random_seed=11 train_steps=200 steps_between_eval=200 params=big save_checkpoints="No" do_eval="No" print_iter=50
validated = True

[transformer_lt_official infer fp32]
model-name = transformer_lt_official
in-graph = 
mode = inference
precision = fp32
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/transformer_lt_official_fp32_pretrained_model.tar.gz
json-fname = timeline_transformer_lt_official_fp32_infer_merged_runs.json
patches = 0001-transformer_lt_official-fp32-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 64
validated = True

[ssd-mobilenet infer fp32]
model-name = ssd-mobilenet
in-graph = 
mode = inference
precision = fp32
data-location =
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md, The coco_val.record file will be used in the following inference examples."
throughput-keyword = samples/sec
throughput-index = -2
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_fp32_pretrained_model_combinedNMS.pb
json-fname = timeline_ssd_mobilenet_fp32_infer_merged_runs.json
patches = 0001-ssdmobilenet-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 1
validated = True

[ssd-mobilenet infer int8]
model-name = ssd-mobilenet
in-graph = 
mode = inference
precision = int8
data-location = 
data-download = To get training data, please refer to the page "https://github.com/IntelAI/models/blob/master/datasets/coco/README.md, The coco_val.record file will be used in the following inference examples."
throughput-keyword = samples/sec
throughput-index = -2
wget = wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/ssdmobilenet_int8_pretrained_model_combinedNMS_s8.pb
json-fname = timeline_ssd_mobilenet_int8_infer_merged_runs.json
patches = 0001-ssdmobilenet-int8-TF-timeline.patch
mkl-only = True
support-accuracy = False
batch-size = 1
validated = True

[ncf infer fp32]
model-name = ncf
mode = inference
precision = fp32
checkpoint = 
data-location = 
data-download = 
throughput-keyword = Throughput
throughput-index = 1
wget = https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_5/ncf_fp32_pretrained_model.tar.gz
json-fname = timeline_ncf_fp32_infer_merged_runs.json
patches = 0001-ncf-fp32-TF-timeline.patch
mkl-only = False
support-accuracy = False
batch-size = 256
validated = False

