From 5775e9b0e5f81fa4d851a9224ab9a7843d559eb8 Mon Sep 17 00:00:00 2001
From: ltsai1 <louie.tsai@intel.com>
Date: Fri, 21 Jan 2022 16:56:41 -0800
Subject: [PATCH] transformer_mlperf train fp32

---
 .../training/fp32/transformer/transformer_main.py      | 10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

diff --git a/models/language_translation/tensorflow/transformer_mlperf/training/fp32/transformer/transformer_main.py b/models/language_translation/tensorflow/transformer_mlperf/training/fp32/transformer/transformer_main.py
index a9b626b35..47725297a 100644
--- a/models/language_translation/tensorflow/transformer_mlperf/training/fp32/transformer/transformer_main.py
+++ b/models/language_translation/tensorflow/transformer_mlperf/training/fp32/transformer/transformer_main.py
@@ -44,6 +44,9 @@ from utils import dataset
 from utils import metrics
 from utils import tokenizer
 
+sys.path.append(os.environ['ProfileUtilsRoot'])
+from profile_utils import ConfigFile, tfProfileHook
+
 tf.compat.v1.disable_eager_execution()
 #Horovod support
 global is_mpi 
@@ -321,6 +324,11 @@ def train_schedule(
   if FLAGS.save_profile == "Yes":
     profile_hooks = [tf.compat.v1.train.ProfilerHook(save_steps=1, output_dir=FLAGS.profile_dir)] # the json file 
   #profile file will be saved in in profile_dir
+
+  config = ConfigFile(confpath=os.environ['ProfileUtilsRoot']+"/topo.ini")
+  config.read_config("transformer_mlperf train fp32")
+  profile_hook = [tfProfileHook(save_steps=1, json_fname=config.json_fname)]
+
   #Creating hooks for printing Examples per Second, used with estimator.train
   training_batch_size = estimator.params.batch_size
   if FLAGS.batch_size is not -1:
@@ -335,7 +343,7 @@ def train_schedule(
   if FLAGS.save_profile == "Yes":
     hooks = profile_hooks
   else:
-    hooks = train_hooks
+    hooks = train_hooks + profile_hook
   
   for i in xrange(train_eval_iterations):
     print("Starting iteration", i + 1)
-- 
2.32.0

