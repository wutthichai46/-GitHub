{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT fine tuning for Question-Answering\n",
    "\n",
    "This notebook demonstrates fine tuning BERT models from TF Hub using the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). Scripts from the [TensorFlow Model Garden](https://github.com/tensorflow/models) are used for preprocessing the training dataset and fine tuning.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Install dependencies and setup parameters](#1.-Install-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Fine tuning and evaluation](#3.-Fine-tuning-and-evaluation)\n",
    "4. [Export the saved model](#4.-Export-the-saved-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies and setup parameters\n",
    "\n",
    "The notebook assumes that you have already followed the README.md instructions that install Intel-optimized TensorFlow or use the Intel-optimized TensorFlow jupyter docker container. Additional installations needed to run the notebook are done in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pip\n",
    "!pip install -q gin-config==0.5.0 \\\n",
    "                sentencepiece==0.1.96 \\\n",
    "                tensorflow-addons==0.15.0 \\\n",
    "                tensorflow-datasets==4.5.2 \\\n",
    "                tensorflow-hub==0.12.0 \\\n",
    "                'pandas>=1.1.5' \\\n",
    "                pyyaml==6.0 \\\n",
    "                wget==3.2\n",
    "!pip install --no-deps -q tf-models-official==2.7.0\n",
    "!apt-get -q update && apt-get -q install -y git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import wget\n",
    "\n",
    "from bert_qa_utils import create_mini_dataset_file, \\\n",
    "                          display_predictions, \\\n",
    "                          get_config_and_vocab_from_zip, \\\n",
    "                          predict_squad_customized\n",
    "\n",
    "from bert_utils import get_model_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run one of the supported [BERT models from TF Hub](https://tfhub.dev/google/collections/bert/1). The table below has a list of the available models and links to their URLs in TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_model_map, models_df = get_model_map(\"tfhub_bert_model_map_qa.json\", return_data_frame=True)\n",
    "models_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the BERT model to use. This string must match one of the models listed in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert_en_wwm_uncased_L-24_H-1024_A-16\"\n",
    "if model_name not in tfhub_model_map.keys():\n",
    "    raise ValueError(\"The specified model name ({}) is not supported\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a working directory where the dataset and TensorFlow models repo will be downloaded\n",
    "if \"WORKING_DIR\" in os.environ and os.environ[\"WORKING_DIR\"] != \"\":\n",
    "    working_dir = os.environ[\"WORKING_DIR\"]\n",
    "else:\n",
    "    working_dir = input(\"Path to a working directory (to download datasets, vocab files, etc): \")\n",
    "\n",
    "# Define an output directory for the saved model to be exported\n",
    "if \"OUTPUT_DIR\" in os.environ and os.environ[\"OUTPUT_DIR\"] != \"\":\n",
    "    output_dir = os.environ[\"OUTPUT_DIR\"]\n",
    "else:\n",
    "    output_dir = input(\"Path to an output directory (for checkpoints and the saved model): \")\n",
    "    \n",
    "# Directory for downloading the BERT config and vocab file\n",
    "bert_dir = os.path.join(working_dir, model_name)\n",
    "\n",
    "# Output directory for logs and checkpoints generated during training\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Directory to download the bert checkpoint zip so to get the vocab.txt and bert_config.json\n",
    "if not os.path.isdir(bert_dir):\n",
    "    os.makedirs(bert_dir)\n",
    "    \n",
    "# Get the BERT TF Hub URL from the model map\n",
    "tfhub_bert_encoder = tfhub_model_map[model_name][\"bert_encoder\"]\n",
    "checkpoint_url = tfhub_model_map[model_name][\"checkpoint_zip\"]\n",
    "\n",
    "# Extract the vocab.txt and bert_config.json from the checkpoint zip file\n",
    "vocab_txt, bert_config = get_config_and_vocab_from_zip(checkpoint_url, bert_dir)\n",
    "\n",
    "if not os.path.exists(vocab_txt):\n",
    "    ValueError(\"The vocab file could not be found at\", vocab_txt)\n",
    "    \n",
    "if not os.path.exists(bert_config):\n",
    "    ValueError(\"The bert config could not be found at\", bert_config)\n",
    "    \n",
    "print(\"Using TF Hub model:\", model_name)\n",
    "print(\"BERT encoder URL:\", tfhub_bert_encoder)\n",
    "print(\"Vocab file:\", vocab_txt)\n",
    "print(\"BERT config:\", bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the https://github.com/tensorflow/models repo will be cloned\n",
    "tf_models_dir = os.path.join(working_dir, \"tensorflow-models\")\n",
    "os.environ[\"TF_MODELS_DIR\"] = tf_models_dir\n",
    "tf_models_branch = \"v2.7.0\"\n",
    "\n",
    "# Clone the TensorFlow models repo\n",
    "if not os.path.exists(tf_models_dir):\n",
    "    !git clone --depth=1 --branch=$tf_models_branch https://github.com/tensorflow/models.git $tf_models_dir\n",
    "        \n",
    "os.environ[\"PYTHONPATH\"] = tf_models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "Download the SQuAD dataset, create smaller json files with a subset of the dev and train datasets, and then create TF records for the mini training dataset. The SQuAD dataset has json files for a train and dev datasets. The json files are formatted like:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"...\",\n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"...\",\n",
    "                            \"id\": \"<unique id>\",\n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"text\": \"...\",\n",
    "                                    \"answer_start\": <index>\n",
    "                                },\n",
    "                                {\n",
    "                                    \"text\": \"...\",\n",
    "                                    \"answer_start\": <index>\n",
    "                                },\n",
    "                                {\n",
    "                                    \"text\": \"...\",\n",
    "                                    \"answer_start\": <index>\n",
    "                                }\n",
    "                            ],\n",
    "                            \"is_impossible\": <true/false>\n",
    "                        },\n",
    "                        ...\n",
    "                    ],\n",
    "                    \"context\": \".....\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"v2.0\"\n",
    "}\n",
    "```\n",
    "\n",
    "Each item in the data list has a title, a list of paragraphs that with questions/answers and a context string. The answer to each question is a segment of text from the context paragraph (unless the question is impossible).\n",
    "\n",
    "For this example, we will be using a subset of the dev and train dataset in order to speed up the execution time. The size of the datasets can be increased (or the full dataset can be used) to try to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify to use SQuAD v1.1 or v2.0\n",
    "squad_version = \"v1.1\"\n",
    "\n",
    "# Maximum sequence length\n",
    "max_seq_length = 384\n",
    "\n",
    "# Specify the number of dataset items to grab from the dev and train datasets.\n",
    "# More dataset items can increase accuracy, but will also increase the training/evaluation time.\n",
    "num_dev_dataset_items = 2\n",
    "num_train_dataset_items = 12\n",
    "\n",
    "# Flag to overwrite previously generated mini dataset .json files and the TF records file\n",
    "overwrite = False\n",
    "\n",
    "# Dataset download directory\n",
    "squad_dir = os.path.join(working_dir, \"squad\")\n",
    "\n",
    "squad_dev_dataset = os.path.join(squad_dir, \"dev-{}.json\".format(squad_version))\n",
    "squad_train_dataset = os.path.join(squad_dir, \"train-{}.json\".format(squad_version))\n",
    "version_2_with_negative = squad_version == \"v2.0\"\n",
    "\n",
    "# Create a directory for the SQuAD files, if the folder does not exist\n",
    "if not os.path.isdir(squad_dir):\n",
    "    os.makedirs(squad_dir)\n",
    "\n",
    "# Download the SQuAD dev dataset file, if it doesn't exist\n",
    "if not os.path.exists(squad_dev_dataset):\n",
    "    squad_dev_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-{}.json\".format(squad_version)\n",
    "    wget.download(squad_dev_url, squad_dir)\n",
    "\n",
    "# Download the SQuAD train dataset file, if it doesn't exist\n",
    "if not os.path.exists(squad_train_dataset):\n",
    "    squad_train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-{}.json\".format(squad_version)\n",
    "    wget.download(squad_train_url, squad_dir)\n",
    "    \n",
    "# Create a smaller version of the dev dataset\n",
    "squad_mini_file = \"mini-dev-{}.json\".format(squad_version)\n",
    "mini_dataset_path = os.path.join(squad_dir, squad_mini_file)\n",
    "create_mini_dataset_file(squad_dev_dataset, mini_dataset_path, num_dev_dataset_items, overwrite=overwrite)\n",
    "\n",
    "# Create a smaller version of the train dataset\n",
    "squad_mini_train_file = \"mini-train-{}.json\".format(squad_version)\n",
    "mini_train_dataset_path = os.path.join(squad_dir, squad_mini_train_file)\n",
    "create_mini_dataset_file(squad_train_dataset, mini_train_dataset_path, num_train_dataset_items, overwrite=overwrite)\n",
    "\n",
    "# Create TF Records for the mini training dataset\n",
    "train_mini_tfrecords_path = os.path.join(squad_dir, \"squad_mini_{}_train.tf_record\".format(squad_version))\n",
    "squad_metadata_path = os.path.join(squad_dir, \"squad_{}_meta_data\".format(squad_version))\n",
    "\n",
    "# Preprocess the dataset, if we don't already have the files\n",
    "if not os.path.exists(train_mini_tfrecords_path) or not os.path.exists(squad_metadata_path) or overwrite:\n",
    "    !python $tf_models_dir/official/nlp/data/create_finetuning_data.py \\\n",
    "        --squad_data_file=$mini_train_dataset_path \\\n",
    "        --vocab_file=$vocab_txt \\\n",
    "        --version_2_with_negative=$version_2_with_negative \\\n",
    "        --train_data_output_path=$train_mini_tfrecords_path \\\n",
    "        --meta_data_file_path=$squad_metadata_path \\\n",
    "        --fine_tuning_task_type=squad \\\n",
    "        --max_seq_length=$max_seq_length\n",
    "    \n",
    "    if os.path.exists(train_mini_tfrecords_path):\n",
    "        print(\"Preprocessed dataset: \", train_mini_tfrecords_path)\n",
    "else:\n",
    "    print(\"The preprocessed training dataset was found at:\", train_mini_tfrecords_path)\n",
    "    print(\"The SQuAD metadata file was found at:\", squad_metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine tuning and evaluation\n",
    "\n",
    "Train the model using the `run_squad.py` script from the [TensorFlow Model Garden](https://github.com/tensorflow/models/blob/v2.7.0/official/nlp/bert/run_squad.py) with the mode set to `train_and_eval`. The [TF Hub](https://tfhub.dev) model URL is being passed as the `hub_module_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 8e-5\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs=1\n",
    "\n",
    "# Batch sizes\n",
    "train_batch_size = 4\n",
    "predict_batch_size = 4\n",
    "\n",
    "# Directory for checkpoints\n",
    "checkpoint_dir = os.path.join(output_dir, \"{}_checkpoints\".format(model_name))\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    if len(os.listdir(checkpoint_dir)) > 0:\n",
    "        print(\"WARNING: The model checkpoint directory is not empty and fine tuning may pick up \" \n",
    "              \"previously generated checkpoint files.\\n\")\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "!python $tf_models_dir/official/nlp/bert/run_squad.py \\\n",
    "  --mode=train_and_eval \\\n",
    "  --input_meta_data_path=$squad_metadata_path \\\n",
    "  --train_data_path=$train_mini_tfrecords_path \\\n",
    "  --predict_file=$mini_dataset_path \\\n",
    "  --vocab_file=$vocab_txt \\\n",
    "  --bert_config_file=$bert_config \\\n",
    "  --hub_module_url=$tfhub_bert_encoder \\\n",
    "  --train_batch_size=$train_batch_size \\\n",
    "  --predict_batch_size=$predict_batch_size \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --num_train_epochs=$num_train_epochs \\\n",
    "  --model_dir=$checkpoint_dir \\\n",
    "  --distribution_strategy=one_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(mini_dataset_path, os.path.join(checkpoint_dir, \"predictions.json\"), n=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export the saved model\n",
    "\n",
    "Using the TensorFlow Model Garden API, export the saved model using the checkpoint files that were generated during fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from official.nlp.bert import bert_models\n",
    "from official.nlp.bert import configs as bert_configs\n",
    "from official.nlp.bert import model_saving_utils\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "bert_config_obj = bert_configs.BertConfig.from_json_file(bert_config)\n",
    "squad_model, _ = bert_models.squad_model(bert_config_obj,\n",
    "                                         max_seq_length,\n",
    "                                         hub_module_url=tfhub_bert_encoder)\n",
    "\n",
    "saved_model_dir = os.path.join(output_dir, \"{}_saved_model\".format(model_name))\n",
    "\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "model_saving_utils.export_bert_model(saved_model_dir, model=squad_model, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@misc{tensorflowmodelgarden2020,\n",
    "  author = {Hongkun Yu and Chen Chen and Xianzhi Du and Yeqing Li and\n",
    "            Abdullah Rashwan and Le Hou and Pengchong Jin and Fan Yang and\n",
    "            Frederick Liu and Jaeyoun Kim and Jing Li},\n",
    "  title = {{TensorFlow Model Garden}},\n",
    "  howpublished = {\\url{https://github.com/tensorflow/models}},\n",
    "  year = {2020}\n",
    "}\n",
    "\n",
    "@article{2016arXiv160605250R,\n",
    "       author = { {Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
    "                 Konstantin and {Liang}, Percy},\n",
    "        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
    "      journal = {arXiv e-prints},\n",
    "         year = 2016,\n",
    "          eid = {arXiv:1606.05250},\n",
    "        pages = {arXiv:1606.05250},\n",
    "archivePrefix = {arXiv},\n",
    "       eprint = {1606.05250},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
