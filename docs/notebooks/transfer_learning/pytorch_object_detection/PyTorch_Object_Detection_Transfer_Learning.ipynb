{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbea021c",
   "metadata": {},
   "source": [
    "# Transfer Learning for Object Detection\n",
    "\n",
    "This notebook is adapted from a [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and demonstrates transfer learning with [Intel Extension for PyTorch (IPEX)](https://github.com/intel/intel-extension-for-pytorch) for an object detection task. It uses object detection models from [torchvision](https://pytorch.org/vision/stable/index.html) that were originally trained using [COCO](https://cocodataset.org/) and does transfer learning with the [PennFudan dataset](https://www.cis.upenn.edu/~jshi/ped_html/), consisting of 170 images with 345 labeled pedestrians.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Predict using the original model](#3.-Predict-using-the-original-model)\n",
    "4. [Transfer learning](#4.-Transfer-Learning)\n",
    "5. [Visualize the model output](#5.-Visualize-the-model-output)\n",
    "6. [Export the saved model](#6.-Export-the-saved-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b4222",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589653d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from PIL import Image\n",
    "from pydoc import locate\n",
    "import warnings\n",
    "\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.utils import make_grid, draw_bounding_boxes\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_utils import torchvision_model_map, get_retrainable_model\n",
    "from dataset_utils import PennFudanDataset, COCO_LABELS\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Supported models:')\n",
    "print('\\n'.join(torchvision_model_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38992a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a model from the list above\n",
    "model_name = \"fasterrcnn_resnet50_fpn\"\n",
    "\n",
    "# Specify the location for the dataset to be downloaded\n",
    "dataset_directory = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "    \n",
    "# Specify a directory for output\n",
    "output_directory = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "# Batch size\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63792ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name not in torchvision_model_map.keys():\n",
    "    raise ValueError(\"The specified model_name ({}) is invalid. Please select from: {}\".\n",
    "                     format(model_name, torchvision_model_map.keys()))\n",
    "    \n",
    "# Get the info for the specified model from the map\n",
    "model_map_values = torchvision_model_map[model_name]\n",
    "predictor_handle = torchvision_model_map[model_name][\"predictor_model\"]\n",
    "print(\"Pretrained Object Detection Model:\", model_name)\n",
    "print(\"Bounding Box Predictor/Classifier:\", predictor_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68bdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference scripts from the torchvision repo that are not in the package\n",
    "if not os.path.exists(\"vision\"):\n",
    "    !git clone --depth 1 --branch v0.11.3 https://github.com/pytorch/vision.git\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"vision/references/detection\")\n",
    "\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# Define transform function for image inputs\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f194c4",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43321f2c",
   "metadata": {},
   "source": [
    "Download and extract the [PennFudan dataset](https://www.cis.upenn.edu/~jshi/ped_html/). If the dataset is not found in the dataset directory it is downloaded. Subsequent runs will reuse the already downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "dataset_path = os.path.join(dataset_directory, \"PennFudanPed\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
    "    !unzip PennFudanPed.zip -d $dataset_directory\n",
    "    !rm PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f96fd",
   "metadata": {},
   "source": [
    "For data performance tuning, see the PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#multi-process-data-loading) documentation. Setting num_workers optimally will depend on hardware and batch size, but 2, 4, or 8 workers will probably work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset(dataset_path, get_transform(train=True))\n",
    "dataset_test = PennFudanDataset(dataset_path, get_transform(train=False))\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=4, \n",
    "                                          collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0b588",
   "metadata": {},
   "source": [
    "## 3. Predict using the original model\n",
    "\n",
    "Use the pretrained model that was trained using COCO to do predictions from the new dataset and view the results for a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d231d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detection model pre-trained on COCO\n",
    "pretrained_model_class = locate('torchvision.models.detection.{}'.format(model_name))\n",
    "predictor_class = locate('torchvision.models.detection.{}'.format(predictor_handle))\n",
    "model = pretrained_model_class(pretrained=True)\n",
    "\n",
    "# Get a batch of data\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "\n",
    "model.eval()\n",
    "predictions = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show_image(img, objects_detected):\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "    fix, axs = plt.subplots(ncols=len(img), squeeze=False)\n",
    "    for i, im in enumerate(img):\n",
    "        im = im.detach()\n",
    "        im = F.to_pil_image(im)\n",
    "        axs[0, i].imshow(np.asarray(im))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        plt.title(objects_detected)\n",
    "        \n",
    "def show_image_results(images, predictions, label_map, score_threshold=0.8):\n",
    "    for i in range(len(images)):\n",
    "        if 'scores' in predictions[i]:\n",
    "            indices_over_threshold = predictions[i]['scores'] > score_threshold\n",
    "        else:\n",
    "            # If there are no scores, show them all\n",
    "            indices_over_threshold = [k for k in range(len(predictions[i]['labels']))]\n",
    "        result = draw_bounding_boxes(convert_image_dtype(images[i], dtype=torch.uint8), \n",
    "                                     predictions[i]['boxes'][indices_over_threshold], \n",
    "                                     width=5)\n",
    "        c = Counter(predictions[i]['labels'][indices_over_threshold].tolist())                         \n",
    "        d = [\"{}: {}\".format(label_map[a], c[a]) for a in c.keys()]\n",
    "        show_image(result, '\\n'.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9551d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_results(images, predictions, COCO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495e1af",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning\n",
    "\n",
    "Replace the pretrained head of the network with a new layer based on the number of classes in our dataset. Train and evaluate the model using the new dataset for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3387b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16116e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "def main(num_classes, dataset, dataset_test):\n",
    "    # Train on the CPU\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # Split the dataset into train and test subsets\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # Define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # Get the model using helper function\n",
    "    model = get_retrainable_model(model_name, num_classes, \n",
    "                              pretrained_model_class, \n",
    "                              predictor_class)\n",
    "\n",
    "    # Move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # Construct optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # Construct learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    # Apply the IPEX optimize function\n",
    "    model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "    for epoch in range(training_epochs):\n",
    "        # Train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # Evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = main(num_classes, dataset, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ae370",
   "metadata": {},
   "source": [
    "## 5. Visualize the model output\n",
    "\n",
    "After the training completes, test the model's predictions on the original batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show object detections from fine-tuned model\n",
    "model.eval()\n",
    "predictions = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predicted results for the fine-tuned model\n",
    "show_image_results(images, predictions, COCO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6bceb",
   "metadata": {},
   "source": [
    "## 6. Export the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    !mkdir -p $output_directory\n",
    "file_path = \"{}/object_detection.pt\".format(output_directory)\n",
    "torch.save(model.state_dict(), file_path)\n",
    "print(\"Saved to {}\".format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4a99c",
   "metadata": {},
   "source": [
    "## Dataset citation\n",
    "```\n",
    "@InProceedings{10.1007/978-3-540-76386-4_17,\n",
    "    author=\"Wang, Liming\n",
    "    and Shi, Jianbo\n",
    "    and Song, Gang\n",
    "    and Shen, I-fan\",\n",
    "    editor=\"Yagi, Yasushi\n",
    "    and Kang, Sing Bing\n",
    "    and Kweon, In So\n",
    "    and Zha, Hongbin\",\n",
    "    title=\"Object Detection Combining Recognition and Segmentation\",\n",
    "    booktitle=\"Computer Vision -- ACCV 2007\",\n",
    "    year=\"2007\",\n",
    "    publisher=\"Springer Berlin Heidelberg\",\n",
    "    address=\"Berlin, Heidelberg\",\n",
    "    pages=\"189--199\",\n",
    "    abstract=\"We develop an object detection method combining top-down recognition with bottom-up image segmentation. There are two main steps in this method: a hypothesis generation step and a verification step. In the top-down hypothesis generation step, we design an improved Shape Context feature, which is more robust to object deformation and background clutter. The improved Shape Context is used to generate a set of hypotheses of object locations and figure-ground masks, which have high recall and low precision rate. In the verification step, we first compute a set of feasible segmentations that are consistent with top-down object hypotheses, then we propose a False Positive Pruning(FPP) procedure to prune out false positives. We exploit the fact that false positive regions typically do not align with any feasible image segmentation. Experiments show that this simple framework is capable of achieving both high recall and high precision with only a few positive training examples and that this method can be generalized to many object classes.\",\n",
    "    isbn=\"978-3-540-76386-4\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab314f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
