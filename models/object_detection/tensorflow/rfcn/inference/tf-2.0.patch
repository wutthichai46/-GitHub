diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 51c29455..e3970e4f 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -30,8 +30,6 @@ from object_detection.utils import label_map_util
 from object_detection.utils import ops
 from object_detection.utils import visualization_utils as vis_utils
 
-slim = tf.contrib.slim
-
 
 def write_metrics(metrics, global_step, summary_dir):
   """Write metrics to a summary directory.
@@ -42,10 +40,10 @@ def write_metrics(metrics, global_step, summary_dir):
     summary_dir: Directory to write tensorflow summaries to.
   """
   logging.info('Writing metrics to tf summary.')
-  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
+  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)
   for key in sorted(metrics):
-    summary = tf.Summary(value=[
-        tf.Summary.Value(tag=key, simple_value=metrics[key]),
+    summary = tf.compat.v1.Summary(value=[
+        tf.compat.v1.Summary.Value(tag=key, simple_value=metrics[key]),
     ])
     summary_writer.add_summary(summary, global_step)
     logging.info('%s: %f', key, metrics[key])
@@ -183,14 +181,14 @@ def visualize_detection_results(result_dict,
       export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
     vis_utils.save_image_array_as_png(image, export_path)
 
-  summary = tf.Summary(value=[
-      tf.Summary.Value(
+  summary = tf.compat.v1.Summary(value=[
+      tf.compat.v1.Summary.Value(
           tag=tag,
-          image=tf.Summary.Image(
+          image=tf.compat.v1.Summary.Image(
               encoded_image_string=vis_utils.encode_image_array_as_png_str(
                   image)))
   ])
-  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
+  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)
   summary_writer.add_summary(summary, global_step)
 
   logging.info('Detection visualizations written to summary with tag %s.', tag)
@@ -258,25 +256,25 @@ def _run_checkpoint_once(tensor_dict,
   """
   if save_graph and not save_graph_dir:
     raise ValueError('`save_graph_dir` must be defined.')
-  sess = tf.Session(master, graph=tf.get_default_graph())
-  sess.run(tf.global_variables_initializer())
-  sess.run(tf.local_variables_initializer())
-  sess.run(tf.tables_initializer())
+  sess = tf.compat.v1.Session(master, graph=tf.compat.v1.get_default_graph())
+  sess.run(tf.compat.v1.global_variables_initializer())
+  sess.run(tf.compat.v1.local_variables_initializer())
+  sess.run(tf.compat.v1.tables_initializer())
   if restore_fn:
     restore_fn(sess)
   else:
     if not checkpoint_dirs:
       raise ValueError('`checkpoint_dirs` must have at least one entry.')
     checkpoint_file = tf.train.latest_checkpoint(checkpoint_dirs[0])
-    saver = tf.train.Saver(variables_to_restore)
+    saver = tf.compat.v1.train.Saver(variables_to_restore)
     saver.restore(sess, checkpoint_file)
 
   if save_graph:
-    tf.train.write_graph(sess.graph_def, save_graph_dir, 'eval.pbtxt')
+    tf.io.write_graph(sess.graph_def, save_graph_dir, 'eval.pbtxt')
 
   counters = {'skipped': 0, 'success': 0}
   aggregate_result_losses_dict = collections.defaultdict(list)
-  with tf.contrib.slim.queues.QueueRunners(sess):
+  with queues.QueueRunners(sess):
     try:
       for batch in range(int(num_batches)):
         if (batch + 1) % 100 == 0:
@@ -322,7 +320,7 @@ def _run_checkpoint_once(tensor_dict,
         if any(key in all_evaluator_metrics for key in metrics):
           raise ValueError('Metric names between evaluators must not collide.')
         all_evaluator_metrics.update(metrics)
-      global_step = tf.train.global_step(sess, tf.train.get_global_step())
+      global_step = tf.compat.v1.train.global_step(sess, tf.compat.v1.train.get_global_step())
 
       for key, value in iter(aggregate_result_losses_dict.items()):
         all_evaluator_metrics['Losses/' + key] = np.mean(value)
@@ -506,17 +504,17 @@ def result_dict_for_single_example(image,
 
   detection_fields = fields.DetectionResultFields
   detection_boxes = detections[detection_fields.detection_boxes][0]
-  image_shape = tf.shape(image)
+  image_shape = tf.shape(input=image)
   detection_scores = detections[detection_fields.detection_scores][0]
 
   if class_agnostic:
     detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)
   else:
     detection_classes = (
-        tf.to_int64(detections[detection_fields.detection_classes][0]) +
+        tf.cast(detections[detection_fields.detection_classes][0], dtype=tf.int64) +
         label_id_offset)
 
-  num_detections = tf.to_int32(detections[detection_fields.num_detections][0])
+  num_detections = tf.cast(detections[detection_fields.num_detections][0], dtype=tf.int32)
   detection_boxes = tf.slice(
       detection_boxes, begin=[0, 0], size=[num_detections, -1])
   detection_classes = tf.slice(
@@ -558,11 +556,10 @@ def result_dict_for_single_example(image,
     if input_data_fields.groundtruth_instance_masks in groundtruth:
       masks = groundtruth[input_data_fields.groundtruth_instance_masks]
       masks = tf.expand_dims(masks, 3)
-      masks = tf.image.resize_images(
+      masks = tf.image.resize(
           masks,
           image_shape[1:3],
-          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
-          align_corners=True)
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
       masks = tf.squeeze(masks, 3)
       groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(
           masks, tf.uint8)
diff --git a/research/object_detection/inference/detection_inference.py b/research/object_detection/inference/detection_inference.py
index dc66686f..cbdf106b 100644
--- a/research/object_detection/inference/detection_inference.py
+++ b/research/object_detection/inference/detection_inference.py
@@ -31,16 +31,16 @@ def build_input(tfrecord_paths):
     image_tensor: The decoded image of the example. Uint8 tensor,
         shape=[1, None, None,3]
   """
-  filename_queue = tf.train.string_input_producer(
+  filename_queue = tf.compat.v1.train.string_input_producer(
       tfrecord_paths, shuffle=False, num_epochs=1)
 
-  tf_record_reader = tf.TFRecordReader()
+  tf_record_reader = tf.compat.v1.TFRecordReader()
   _, serialized_example_tensor = tf_record_reader.read(filename_queue)
-  features = tf.parse_single_example(
-      serialized_example_tensor,
+  features = tf.io.parse_single_example(
+      serialized=serialized_example_tensor,
       features={
           standard_fields.TfExampleFields.image_encoded:
-              tf.FixedLenFeature([], tf.string),
+              tf.io.FixedLenFeature([], tf.string),
       })
   encoded_image = features[standard_fields.TfExampleFields.image_encoded]
   image_tensor = tf.image.decode_image(encoded_image, channels=3)
@@ -65,15 +65,15 @@ def build_inference_graph(image_tensor, inference_graph_path):
     detected_labels_tensor: Detected labels. Int64 tensor,
         shape=[num_detections]
   """
-  with tf.gfile.Open(inference_graph_path, 'r') as graph_def_file:
+  with tf.io.gfile.GFile(inference_graph_path, 'rb') as graph_def_file:
     graph_content = graph_def_file.read()
-  graph_def = tf.GraphDef()
+  graph_def = tf.compat.v1.GraphDef()
   graph_def.MergeFromString(graph_content)
 
   tf.import_graph_def(
       graph_def, name='', input_map={'image_tensor': image_tensor})
 
-  g = tf.get_default_graph()
+  g = tf.compat.v1.get_default_graph()
 
   num_detections_tensor = tf.squeeze(
       g.get_tensor_by_name('num_detections:0'), 0)
@@ -114,7 +114,7 @@ def infer_detections_and_add_to_example(
   """
   tf_example = tf.train.Example()
   (serialized_example, detected_boxes, detected_scores,
-   detected_classes) = tf.get_default_session().run([
+   detected_classes) = tf.compat.v1.get_default_session().run([
        serialized_example_tensor, detected_boxes_tensor, detected_scores_tensor,
        detected_labels_tensor
    ])
diff --git a/research/object_detection/inference/infer_detections.py b/research/object_detection/inference/infer_detections.py
index a251009e..23a61f20 100644
--- a/research/object_detection/inference/infer_detections.py
+++ b/research/object_detection/inference/infer_detections.py
@@ -37,24 +37,27 @@ metrics).
 import itertools
 import tensorflow as tf
 from object_detection.inference import detection_inference
+import argparse
 
-tf.flags.DEFINE_string('input_tfrecord_paths', None,
-                       'A comma separated list of paths to input TFRecords.')
-tf.flags.DEFINE_string('output_tfrecord_path', None,
-                       'Path to the output TFRecord.')
-tf.flags.DEFINE_string('inference_graph', None,
-                       'Path to the inference graph with embedded weights.')
-tf.flags.DEFINE_boolean('discard_image_pixels', False,
-                        'Discards the images in the output TFExamples. This'
-                        ' significantly reduces the output size and is useful'
-                        ' if the subsequent tools don\'t need access to the'
-                        ' images (e.g. when computing evaluation measures).')
-
-FLAGS = tf.flags.FLAGS
-
+parser = argparse.ArgumentParser()
+parser.add_argument('-i', '--input_tfrecord_paths',
+                    help='A comma separated list of paths to input TFRecords.',
+                    default=None)
+parser.add_argument('-o', '--output_tfrecord_path',
+                    help='Path to the output TFRecord.', default=None)
+parser.add_argument('-g', '--inference_graph',
+                    help='Path to the inference graph with embedded weights.',
+                    default=None)
+parser.add_argument('-d', '--discard_image_pixels',
+                    help='Discards the images in the output TFExamples. This'
+                    ' significantly reduces the output size and is useful'
+                    ' if the subsequent tools don\'t need access to the'
+                    ' images (e.g. when computing evaluation measures).',
+                    action='store_true', default=False)
+FLAGS = parser.parse_args()
 
 def main(_):
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   required_flags = ['input_tfrecord_paths', 'output_tfrecord_path',
                     'inference_graph']
@@ -62,26 +65,26 @@ def main(_):
     if not getattr(FLAGS, flag_name):
       raise ValueError('Flag --{} is required'.format(flag_name))
 
-  with tf.Session() as sess:
+  with tf.compat.v1.Session() as sess:
     input_tfrecord_paths = [
         v for v in FLAGS.input_tfrecord_paths.split(',') if v]
-    tf.logging.info('Reading input from %d files', len(input_tfrecord_paths))
+    tf.compat.v1.logging.info('Reading input from %d files', len(input_tfrecord_paths))
     serialized_example_tensor, image_tensor = detection_inference.build_input(
         input_tfrecord_paths)
-    tf.logging.info('Reading graph and building model...')
+    tf.compat.v1.logging.info('Reading graph and building model...')
     (detected_boxes_tensor, detected_scores_tensor,
      detected_labels_tensor) = detection_inference.build_inference_graph(
          image_tensor, FLAGS.inference_graph)
 
-    tf.logging.info('Running inference and writing output to {}'.format(
+    tf.compat.v1.logging.info('Running inference and writing output to {}'.format(
         FLAGS.output_tfrecord_path))
-    sess.run(tf.local_variables_initializer())
-    tf.train.start_queue_runners()
-    with tf.python_io.TFRecordWriter(
+    sess.run(tf.compat.v1.local_variables_initializer())
+    tf.compat.v1.train.start_queue_runners()
+    with tf.io.TFRecordWriter(
         FLAGS.output_tfrecord_path) as tf_record_writer:
       try:
         for counter in itertools.count():
-          tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 10,
+          tf.compat.v1.logging.log_every_n(tf.compat.v1.logging.INFO, 'Processed %d images...', 10,
                                  counter)
           tf_example = detection_inference.infer_detections_and_add_to_example(
               serialized_example_tensor, detected_boxes_tensor,
@@ -89,8 +92,8 @@ def main(_):
               FLAGS.discard_image_pixels)
           tf_record_writer.write(tf_example.SerializeToString())
       except tf.errors.OutOfRangeError:
-        tf.logging.info('Finished processing records')
+        tf.compat.v1.logging.info('Finished processing records')
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 71b747bc..145e93ba 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -106,7 +106,7 @@ class COCOWrapper(coco.COCO):
     results = coco.COCO()
     results.dataset['images'] = [img for img in self.dataset['images']]
 
-    tf.logging.info('Loading and preparing annotation results...')
+    tf.compat.v1.logging.info('Loading and preparing annotation results...')
     tic = time.time()
 
     if not isinstance(annotations, list):
@@ -128,7 +128,7 @@ class COCOWrapper(coco.COCO):
         ann['bbox'] = mask.toBbox(ann['segmentation'])
         ann['id'] = idx + 1
         ann['iscrowd'] = 0
-    tf.logging.info('DONE (t=%0.2fs)', (time.time() - tic))
+    tf.compat.v1.logging.info('DONE (t=%0.2fs)', (time.time() - tic))
 
     results.dataset['annotations'] = annotations
     results.createIndex()
@@ -479,7 +479,7 @@ def ExportGroundtruthToCOCO(image_ids,
       'categories': categories
   }
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(groundtruth_dict, fid, float_digits=4, indent=2)
   return groundtruth_dict
 
@@ -666,7 +666,7 @@ def ExportDetectionsToCOCO(image_ids,
         scores,
         classes))
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(detections_export_list, fid, float_digits=4, indent=2)
   return detections_export_list
 
@@ -746,7 +746,7 @@ def ExportSegmentsToCOCO(image_ids,
         image_id, category_id_set, np.squeeze(masks, axis=3), scores, classes))
 
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(segment_export_list, fid, float_digits=4, indent=2)
   return segment_export_list
 
@@ -845,6 +845,6 @@ def ExportKeypointsToCOCO(image_ids,
         })
 
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(keypoints_export_list, fid, float_digits=4, indent=2)
   return keypoints_export_list
diff --git a/research/object_detection/metrics/offline_eval_map_corloc.py b/research/object_detection/metrics/offline_eval_map_corloc.py
index ff2efbaf..dbf6a82d 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc.py
@@ -35,6 +35,7 @@ import csv
 import os
 import re
 import tensorflow as tf
+import argparse
 
 from object_detection.core import standard_fields
 from object_detection.legacy import evaluator
@@ -42,16 +43,16 @@ from object_detection.metrics import tf_example_parser
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
 
-flags = tf.app.flags
-tf.logging.set_verbosity(tf.logging.INFO)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
-flags.DEFINE_string('eval_dir', None, 'Directory to write eval summaries to.')
-flags.DEFINE_string('eval_config_path', None,
-                    'Path to an eval_pb2.EvalConfig config file.')
-flags.DEFINE_string('input_config_path', None,
-                    'Path to an eval_pb2.InputConfig config file.')
-
-FLAGS = flags.FLAGS
+parser = argparse.ArgumentParser()
+parser.add_argument('-d', '--eval_dir',
+                    help='Directory to write eval summaries to.', default=None)
+parser.add_argument('-e', '--eval_config_path',
+                    help='Path to an eval_pb2.EvalConfig config file.', default=None)
+parser.add_argument('-p', '--input_config_path',
+                    help='Path to an eval_pb2.InputConfig config file.', default=None)
+FLAGS = parser.parse_args()
 
 
 def _generate_sharded_filenames(filename):
@@ -104,13 +105,13 @@ def read_data_and_evaluate(input_config, eval_config):
     skipped_images = 0
     processed_images = 0
     for input_path in _generate_filenames(input_paths):
-      tf.logging.info('Processing file: {0}'.format(input_path))
+      tf.compat.v1.logging.info('Processing file: {0}'.format(input_path))
 
-      record_iterator = tf.python_io.tf_record_iterator(path=input_path)
+      record_iterator = tf.compat.v1.python_io.tf_record_iterator(path=input_path)
       data_parser = tf_example_parser.TfExampleDetectionAndGTParser()
 
       for string_record in record_iterator:
-        tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 1000,
+        tf.compat.v1.logging.log_every_n(tf.compat.v1.logging.INFO, 'Processed %d images...', 1000,
                                processed_images)
         processed_images += 1
 
@@ -127,7 +128,7 @@ def read_data_and_evaluate(input_config, eval_config):
               decoded_dict)
         else:
           skipped_images += 1
-          tf.logging.info('Skipped images: {0}'.format(skipped_images))
+          tf.compat.v1.logging.info('Skipped images: {0}'.format(skipped_images))
 
     return object_detection_evaluator.evaluate()
 
@@ -141,7 +142,7 @@ def write_metrics(metrics, output_dir):
     metrics: A dictionary containing metric names and values.
     output_dir: Directory to write metrics to.
   """
-  tf.logging.info('Writing metrics.')
+  tf.compat.v1.logging.info('Writing metrics.')
 
   with open(os.path.join(output_dir, 'metrics.csv'), 'w') as csvfile:
     metrics_writer = csv.writer(csvfile, delimiter=',')
@@ -170,4 +171,4 @@ def main(argv):
 
 
 if __name__ == '__main__':
-  tf.app.run(main)
+  tf.compat.v1.app.run(main)
diff --git a/research/object_detection/metrics/tf_example_parser.py b/research/object_detection/metrics/tf_example_parser.py
index 9a5f130f..fa361bf5 100644
--- a/research/object_detection/metrics/tf_example_parser.py
+++ b/research/object_detection/metrics/tf_example_parser.py
@@ -44,7 +44,7 @@ class StringParser(data_parser.DataToNumpyParser):
     self.field_name = field_name
 
   def parse(self, tf_example):
-    return "".join(tf_example.features.feature[self.field_name]
+    return b"".join(tf_example.features.feature[self.field_name]
                    .bytes_list.value) if tf_example.features.feature[
                        self.field_name].HasField("bytes_list") else None
 
diff --git a/research/object_detection/queues.py b/research/object_detection/queues.py
new file mode 100644
index 00000000..1bc8d2c8
--- /dev/null
+++ b/research/object_detection/queues.py
@@ -0,0 +1,74 @@
+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Contains a helper context for running queue runners.
+
+@@NestedQueueRunnerError
+@@QueueRunners
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from contextlib import contextmanager
+import threading
+
+from tensorflow.python.framework import ops
+from tensorflow.python.training import coordinator
+
+__all__ = [
+    'NestedQueueRunnerError',
+    'QueueRunners',
+]
+
+_queue_runner_lock = threading.Lock()
+
+
+class NestedQueueRunnerError(Exception):
+  pass
+
+
+@contextmanager
+def QueueRunners(session):
+  """Creates a context manager that handles starting and stopping queue runners.
+
+  Args:
+    session: the currently running session.
+
+  Yields:
+    a context in which queues are run.
+
+  Raises:
+    NestedQueueRunnerError: if a QueueRunners context is nested within another.
+  """
+  if not _queue_runner_lock.acquire(False):
+    raise NestedQueueRunnerError('QueueRunners cannot be nested')
+
+  coord = coordinator.Coordinator()
+  threads = []
+  for qr in ops.get_collection(ops.GraphKeys.QUEUE_RUNNERS):
+    threads.extend(
+        qr.create_threads(
+            session, coord=coord, daemon=True, start=True))
+  try:
+    yield
+  finally:
+    coord.request_stop()
+    try:
+      coord.join(threads, stop_grace_period_secs=120)
+    except RuntimeError:
+      session.close()
+
+    _queue_runner_lock.release()
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index e7835223..4e8125ab 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -89,7 +89,7 @@ def get_configs_from_pipeline_file(pipeline_config_path):
       corresponding config objects.
   """
   pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
-  with tf.gfile.GFile(pipeline_config_path, "r") as f:
+  with tf.io.gfile.GFile(pipeline_config_path, "r") as f:
     proto_str = f.read()
     text_format.Merge(proto_str, pipeline_config)
   return create_configs_from_pipeline_proto(pipeline_config)
@@ -128,7 +128,7 @@ def get_graph_rewriter_config_from_file(graph_rewriter_config_file):
     graph_rewriter_pb2.GraphRewriter proto
   """
   graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
-  with tf.gfile.GFile(graph_rewriter_config_file, "r") as f:
+  with tf.io.gfile.GFile(graph_rewriter_config_file, "r") as f:
     text_format.Merge(f.read(), graph_rewriter_config)
   return graph_rewriter_config
 
@@ -168,8 +168,8 @@ def save_pipeline_config(pipeline_config, directory):
     file_io.recursive_create_dir(directory)
   pipeline_config_path = os.path.join(directory, "pipeline.config")
   config_text = text_format.MessageToString(pipeline_config)
-  with tf.gfile.Open(pipeline_config_path, "wb") as f:
-    tf.logging.info("Writing pipeline config file to %s",
+  with tf.io.gfile.GFile(pipeline_config_path, "wb") as f:
+    tf.compat.v1.logging.info("Writing pipeline config file to %s",
                     pipeline_config_path)
     f.write(config_text)
 
@@ -198,31 +198,31 @@ def get_configs_from_multiple_files(model_config_path="",
   configs = {}
   if model_config_path:
     model_config = model_pb2.DetectionModel()
-    with tf.gfile.GFile(model_config_path, "r") as f:
+    with tf.io.gfile.GFile(model_config_path, "r") as f:
       text_format.Merge(f.read(), model_config)
       configs["model"] = model_config
 
   if train_config_path:
     train_config = train_pb2.TrainConfig()
-    with tf.gfile.GFile(train_config_path, "r") as f:
+    with tf.io.gfile.GFile(train_config_path, "r") as f:
       text_format.Merge(f.read(), train_config)
       configs["train_config"] = train_config
 
   if train_input_config_path:
     train_input_config = input_reader_pb2.InputReader()
-    with tf.gfile.GFile(train_input_config_path, "r") as f:
+    with tf.io.gfile.GFile(train_input_config_path, "r") as f:
       text_format.Merge(f.read(), train_input_config)
       configs["train_input_config"] = train_input_config
 
   if eval_config_path:
     eval_config = eval_pb2.EvalConfig()
-    with tf.gfile.GFile(eval_config_path, "r") as f:
+    with tf.io.gfile.GFile(eval_config_path, "r") as f:
       text_format.Merge(f.read(), eval_config)
       configs["eval_config"] = eval_config
 
   if eval_input_config_path:
     eval_input_config = input_reader_pb2.InputReader()
-    with tf.gfile.GFile(eval_input_config_path, "r") as f:
+    with tf.io.gfile.GFile(eval_input_config_path, "r") as f:
       text_format.Merge(f.read(), eval_input_config)
       configs["eval_input_config"] = eval_input_config
 
@@ -325,7 +325,7 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   if hparams:
     kwargs.update(hparams.values())
   for key, value in kwargs.items():
-    tf.logging.info("Maybe overwriting %s: %s", key, value)
+    tf.compat.v1.logging.info("Maybe overwriting %s: %s", key, value)
     # pylint: disable=g-explicit-bool-comparison
     if value == "" or value is None:
       continue
@@ -366,7 +366,7 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
     elif _is_generic_key(key):
       _update_generic(configs, key, value)
     else:
-      tf.logging.info("Ignoring config override key: %s", key)
+      tf.compat.v1.logging.info("Ignoring config override key: %s", key)
   return configs
 
 
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index aef46c1d..8ccad0cb 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -128,7 +128,7 @@ def load_labelmap(path):
   Returns:
     a StringIntLabelMapProto
   """
-  with tf.gfile.GFile(path, 'r') as fid:
+  with tf.io.gfile.GFile(path, 'r') as fid:
     label_map_string = fid.read()
     label_map = string_int_label_map_pb2.StringIntLabelMap()
     try:
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 8a38d8c2..99af2324 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -839,9 +839,9 @@ class ObjectDetectionEvaluation(object):
       if self.use_weighted_mean_ap:
         all_scores = np.append(all_scores, scores)
         all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)
-      print 'Scores and tpfp per class label: {}'.format(class_index)
-      print tp_fp_labels
-      print scores
+      print('Scores and tpfp per class label: {}'.format(class_index))
+      print(tp_fp_labels)
+      print(scores)
       precision, recall = metrics.compute_precision_recall(
           scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])
       self.precisions_per_class.append(precision)
