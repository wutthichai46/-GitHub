diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 21f086b..7a630d3 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -27,5 +27,3 @@ information on using pull requests.
 *   We format all our python code with
     [autopep8](https://pypi.python.org/pypi/autopep8). If you use Vim, check
     out [Vim-CodeFmt](https://github.com/google/vim-codefmt).
-*   We use [pylint](https://www.pylint.org/) to check the syntax of our code.
-    Not all of our code yet passes pylint, but we're working on it!
diff --git a/WORKSPACE b/WORKSPACE
index cabf233..1977a70 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -12,11 +12,8 @@ http_archive(
 
 http_archive(
     name = "com_google_absl",
-    strip_prefix = "abseil-cpp-666fc1266bccfd8e6eaaa084e7b42580bb8eb199",
-    urls = [
-        "http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/666fc1266bccfd8e6eaaa084e7b42580bb8eb199.tar.gz",
-        "https://github.com/abseil/abseil-cpp/archive/666fc1266bccfd8e6eaaa084e7b42580bb8eb199.tar.gz",
-    ],
+    strip_prefix = "abseil-cpp-93dfcf74cb5fccae3da07897d8613ae6cab958a0",
+    urls = ["https://github.com/abseil/abseil-cpp/archive/93dfcf74cb5fccae3da07897d8613ae6cab958a0.tar.gz"],
 )
 
 http_archive(
diff --git a/build.sh b/build.sh
new file mode 100755
index 0000000..1f682cc
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,4 @@
+#!/bin/bash
+. ./set_avx2_build
+bazel build  --incompatible_remove_native_http_archive=false -c opt  --verbose_failures --define=tf=1  --define=board_size=9 $BAZEL_BUILD_OPTS cc:selfplay cc:eval
+
diff --git a/cc/configure_tensorflow.sh b/cc/configure_tensorflow.sh
index 8b39178..3bc9cea 100755
--- a/cc/configure_tensorflow.sh
+++ b/cc/configure_tensorflow.sh
@@ -4,109 +4,99 @@ set -e
 
 script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
 dst_dir="${script_dir}/tensorflow"
-tmp_dir="/tmp/minigo_tf"
-tmp_pkg_dir="/tmp/tensorflow_pkg"
+tmp_dir="${script_dir}/minigo_tf"
+tmp_pkg_dir="${script_dir}/tensorflow_pkg"
 
-rm -rfd ${tmp_dir}
 rm -rfd ${tmp_pkg_dir}
-mkdir -p ${tmp_dir}
 
 rm -rf ${dst_dir}/*
 mkdir -p ${dst_dir}
 
+if [ -d "${script_dir}/../ml_perf/tools" ]; then
+  echo "Intel AI tools exist."
+else
+  git clone https://github.com/IntelAI/tools.git ${script_dir}/../ml_perf/tools/
+fi
+
 # TODO(tommadams): we should probably switch to Clang at some point.
-commit_tag="v1.11.0"
 
-echo "Cloning tensorflow to ${tmp_dir}"
-git clone https://github.com/tensorflow/tensorflow "${tmp_dir}"
+if [ -d "${tmp_dir}" ]; then
+  pushd "${tmp_dir}"
+else
+  echo "Cloning tensorflow to ${tmp_dir}"
+  git clone https://github.com/tensorflow/tensorflow "${tmp_dir}"
+
+  pushd "${tmp_dir}"
 
-pushd "${tmp_dir}"
+  cherry_pick_tag="02c111ab4269ab73a506164e4b54ba996d28a8cf"
+  prev_tag="8be9158c7a701d933bbe532f5d54df17f47a4284"
 
-echo "Checking out ${commit_tag}"
-git checkout "${commit_tag}"
+  git diff "${prev_tag}" "${cherry_pick_tag}" > sample.patch
+
+  commit_tag="961bb02b882a8bb921e5be1c09c34b51fffd25dc"
+  echo "Checking out ${commit_tag}"
+  git checkout "${commit_tag}"
+  git apply sample.patch
+  cp ${script_dir}/../ml_perf/tools/tensorflow_quantization/graph_transforms/fuse_quantized_convolution.cc ${tmp_dir}/tensorflow/tools/graph_transforms/
+fi
 
 # Run the TensorFlow configuration script, setting reasonable values for most
 # of the options.
 echo "Configuring tensorflow"
 cc_opt_flags="${CC_OPT_FLAGS:--march=native}"
 
+PYTHON_BIN_PATH=`which python`
+
 CC_OPT_FLAGS="${cc_opt_flags}" \
-TF_NEED_JEMALLOC=${TF_NEED_JEMALLOC:-1} \
-TF_NEED_GCP=${TF_NEED_GCP:-1} \
+PYTHON_BIN_PATH=${PYTHON_BIN_PATH} \
+USE_DEFAULT_PYTHON_LIB_PATH="${USE_DEFAULT_PYTHON_LIB_PATH:-1}" \
+TF_NEED_JEMALLOC=${TF_NEED_JEMALLOC:-0} \
+TF_NEED_GCP=${TF_NEED_GCP:-0} \
 TF_NEED_HDFS=${TF_NEED_HDFS:-0} \
 TF_NEED_S3=${TF_NEED_S3:-0} \
 TF_NEED_KAFKA=${TF_NEED_KAFKA:-0} \
-TF_NEED_CUDA=${TF_NEED_CUDA:-1} \
+TF_NEED_CUDA=${TF_NEED_CUDA:-0} \
 TF_NEED_GDR=${TF_NEED_GDR:-0} \
 TF_NEED_VERBS=${TF_NEED_VERBS:-0} \
 TF_NEED_OPENCL_SYCL=${TF_NEED_OPENCL_SYCL:-0} \
+TF_NEED_ROCM=${TF_NEED_ROCM:-0} \
 TF_CUDA_CLANG=${TF_CUDA_CLANG:-0} \
+TF_DOWNLOAD_CLANG=${TF_DOWNLOAD_CLANG:-0} \
 TF_NEED_TENSORRT=${TF_NEED_TENSORRT:-0} \
 TF_NEED_MPI=${TF_NEED_MPI:-0} \
 TF_SET_ANDROID_WORKSPACE=${TF_SET_ANDROID_WORKSPACE:-0} \
 TF_NCCL_VERSION=${TF_NCCL_VERSION:-1.3} \
+TF_ENABLE_XLA=${TF_ENABLE_XLA:-0} \
 ./configure
 
+. ${script_dir}/../set_avx2_build
+BAZEL_OPTS="-c opt --config=mkl \
+            --action_env=PATH \
+            --action_env=LD_LIBRARY_PATH \
+            $BAZEL_BUILD_OPTS \
+            --copt=-DINTEL_MKLDNN"
 echo "Building tensorflow package"
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" //tensorflow/tools/pip_package:build_pip_package
+bazel build -s $BAZEL_OPTS //tensorflow/tools/pip_package:build_pip_package
 bazel-bin/tensorflow/tools/pip_package/build_pip_package ${tmp_pkg_dir}
 
 echo "Tensorflow built-ish"
 echo "Unpacking tensorflow package..."
 unzip -q ${tmp_pkg_dir}/tensorflow-*.whl -d ${tmp_dir}
 
+
 echo "Copying tensor flow headers to ${dst_dir}"
 cp -r ${tmp_dir}/tensorflow-*.data/purelib/tensorflow/include/* "${dst_dir}"
-
 echo "Building tensorflow libraries"
 
-# Add a custom BUILD target for the gRPC runtime.
-# TODO(tommadams): Remove this once the gRPC runtime is linked in to TensorFlow.
-cat <<EOF >> tensorflow/BUILD
-
-tf_cc_shared_object(
-    name = "libgrpc_runtime.so",
-    linkopts = select({
-        "//tensorflow:darwin": [
-            "-Wl,-exported_symbols_list",  # This line must be directly followed by the exported_symbols.lds file
-            "\$(location //tensorflow:tf_exported_symbols.lds)",
-        ],
-        "//tensorflow:windows": [],
-        "//conditions:default": [
-            "-z defs",
-            "-Wl,--version-script",  #  This line must be directly followed by the version_script.lds file
-            "\$(location //tensorflow:tf_version_script.lds)",
-        ],
-    }),
-    deps = [
-        "//tensorflow:tf_exported_symbols.lds",
-        "//tensorflow:tf_version_script.lds",
-       "//tensorflow/core/distributed_runtime/rpc:grpc_runtime",
-    ]
-)
-EOF
-
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" \
-    //tensorflow:libgrpc_runtime.so \
+bazel build -s $BAZEL_OPTS \
     //tensorflow:libtensorflow_cc.so \
     //tensorflow:libtensorflow_framework.so
 
 echo "Copying tensorflow libraries to ${dst_dir}"
-cp bazel-bin/tensorflow/{libgrpc_runtime,libtensorflow_*}.so "${dst_dir}"
-
-echo "Building toco"
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" //tensorflow/contrib/lite/toco:toco
-cp bazel-bin/tensorflow/contrib/lite/toco/toco "${dst_dir}"
-
-echo "Building TF Lite"
-
-./tensorflow/contrib/lite/tools/make/download_dependencies.sh
-make -j $(nproc) -f tensorflow/contrib/lite/tools/make/Makefile
-cp tensorflow/contrib/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a $dst_dir/libtensorflow_lite.a
-for dir in contrib/lite contrib/lite/kernels contrib/lite/profiling contrib/lite/schema; do
-  mkdir -p $dst_dir/tensorflow/$dir
-  cp tensorflow/$dir/*.h $dst_dir/tensorflow/$dir/
-done
-cp -r tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include/flatbuffers $dst_dir/
+cp bazel-bin/tensorflow/libtensorflow_*.so "${dst_dir}"
+cp bazel-bin/tensorflow/libtensorflow_*.so.1 "${dst_dir}"
+
+cp `find ${tmp_dir} |grep libiomp5.so` ${dst_dir}
+cp `find ${tmp_dir} |grep libmklml_intel.so` ${dst_dir}
 
 popd
diff --git a/cc/dual_net/tf_dual_net.cc b/cc/dual_net/tf_dual_net.cc
index a400cc2..3bee107 100644
--- a/cc/dual_net/tf_dual_net.cc
+++ b/cc/dual_net/tf_dual_net.cc
@@ -58,6 +58,9 @@ class TfDualNet : public DualNet {
    public:
     TfWorker(const GraphDef& graph_def) : batch_capacity_(0) {
       SessionOptions options;
+      options.config.set_intra_op_parallelism_threads(1);
+      options.config.set_inter_op_parallelism_threads(0);
+      options.config.set_use_per_session_threads(false);
       options.config.mutable_gpu_options()->set_allow_growth(true);
       session_.reset(NewSession(options));
       TF_CHECK_OK(session_->Create(graph_def));
diff --git a/cc/eval.cc b/cc/eval.cc
index bde9011..525c840 100644
--- a/cc/eval.cc
+++ b/cc/eval.cc
@@ -68,6 +68,7 @@ DEFINE_string(model, "",
               "engine=lite, the model should be .tflite flatbuffer.");
 DEFINE_string(model_two, "", "Descriptor for the second model");
 DEFINE_int32(parallel_games, 32, "Number of games to play in parallel.");
+DEFINE_int32(instance_id, 0, "Unique id with multi-instance.");
 
 // Output flags.
 DEFINE_string(output_bigtable, "",
@@ -170,7 +171,10 @@ class Evaluator {
     ParseOptionsFromFlags(&game_options_, &player_options_);
 
     int num_games = FLAGS_parallel_games;
-    for (int thread_id = 0; thread_id < num_games; ++thread_id) {
+    int instance_id = FLAGS_instance_id;
+    int thread_id_begin = instance_id*num_games;
+    for (int thread_id = thread_id_begin;
+             thread_id < thread_id_begin+num_games; ++thread_id) {
       bool swap_models = (thread_id & 1) != 0;
       threads_.emplace_back(std::bind(&Evaluator::ThreadRun, this, thread_id,
                                       swap_models ? &model_b : &model_a,
diff --git a/cc/selfplay.cc b/cc/selfplay.cc
index a3d4d9e..9d3cfc0 100644
--- a/cc/selfplay.cc
+++ b/cc/selfplay.cc
@@ -119,6 +119,7 @@ DEFINE_int32(parallel_games, 32, "Number of games to play in parallel.");
 DEFINE_int32(num_games, 0,
              "Total number of games to play. Defaults to parallel_games. "
              "Only one of num_games and run_forever must be set.");
+DEFINE_int32(instance_id, 0, "Unique id with multi-instance.");
 
 // Output flags.
 DEFINE_string(output_dir, "",
@@ -244,7 +245,10 @@ class SelfPlayer {
       batcher_ =
           absl::make_unique<BatchingDualNetFactory>(std::move(model_factory));
     }
-    for (int i = 0; i < FLAGS_parallel_games; ++i) {
+    int instance_id = FLAGS_instance_id;
+    int thread_id_begin = instance_id * FLAGS_parallel_games;
+    for (int i = thread_id_begin;
+             i < thread_id_begin+FLAGS_parallel_games; ++i) {
       threads_.emplace_back(std::bind(&SelfPlayer::ThreadRun, this, i));
     }
     for (auto& t : threads_) {
diff --git a/common.py b/common.py
new file mode 100644
index 0000000..37516cf
--- /dev/null
+++ b/common.py
@@ -0,0 +1,31 @@
+import os
+
+class Config():
+  def __init__(self, tf_root):
+    self.demo_dir = os.path.join(tf_root, 'demo')
+    self.demo_tmp_dir = os.path.join(tf_root, '../demo_tmp')
+
+    self.pb_dir = os.path.join(self.demo_dir, 'pb')
+    if not os.path.exists(self.pb_dir):
+      os.makedirs(self.pb_dir)
+    self.fp32_optimized_graph = os.path.join(self.pb_dir, 'freezed_resnet50_opt.pb')
+    self.int8_graph = os.path.join(self.pb_dir, 'int8_resnet50.pb')
+    self.int8_graph_logged = os.path.join(self.pb_dir, 'int8_resnet50_logged.pb')
+    self.int8_graph_freese = os.path.join(self.pb_dir, 'int8_resnet50_freese.pb')
+    self.int8_graph_final = os.path.join(self.pb_dir, 'int8_resnet50_final.pb')
+
+    self.accuracy_script = os.path.join(self.demo_dir, 'accuracy.py')
+    self.benchmark_script = os.path.join(self.demo_dir, 'benchmark.py')
+    self.quantize_script = os.path.join(self.demo_dir, 'quantize_graph.py')
+
+    self.min_max_log = os.path.join(self.demo_dir, 'min_max.log')
+
+
+  input_names = 'input'
+  output_names = 'predict'
+
+  def set_fp32_graph(self, pb):
+    self.fp32_original_graph = pb
+
+  def set_dataset(self, ds):
+    self.imagenet_data = ds
diff --git a/dual_net.py b/dual_net.py
index edf946d..83ddcba 100644
--- a/dual_net.py
+++ b/dual_net.py
@@ -36,6 +36,15 @@ import features as features_lib
 import go
 import symmetries
 
+import horovod.tensorflow as hvd
+
+from tensorflow.python.framework import dtypes
+from tensorflow.core.framework import graph_pb2
+from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference
+from tensorflow.tools.graph_transforms import TransformGraph
+from ml_perf.utils import *
+
+import quantize_graph
 
 flags.DEFINE_integer('train_batch_size', 256,
                      'Batch size to use for train/eval evaluation. For GPU '
@@ -120,6 +129,18 @@ flags.DEFINE_integer(
 flags.DEFINE_integer(
     'keep_checkpoint_max', default=5, help='Number of checkpoints to keep.')
 
+flags.DEFINE_integer(
+    'num_inter_threads', default=0,
+    help=('Number of inter threads.'))
+
+flags.DEFINE_integer(
+    'num_intra_threads', default=0,
+    help=('Number of intra threads.'))
+
+flags.DEFINE_bool(
+    'dist_train', default=False,
+    help=('Using distributed training or not.'))
+
 flags.DEFINE_bool(
     'use_random_symmetry', True,
     help='If true random symmetries be used when doing inference.')
@@ -157,7 +178,9 @@ class DualNetwork():
         self.save_file = save_file
         self.inference_input = None
         self.inference_output = None
-        config = tf.ConfigProto()
+        config = tf.ConfigProto(
+                      intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                      inter_op_parallelism_threads=FLAGS.num_inter_threads)
         config.gpu_options.allow_growth = True
         self.sess = tf.Session(graph=tf.Graph(), config=config)
         self.initialize_graph()
@@ -273,6 +296,8 @@ def model_fn(features, labels, mode, params):
 
     optimizer = tf.train.MomentumOptimizer(
         learning_rate, params['sgd_momentum'])
+    if(params['dist_train']):
+        optimizer = hvd.DistributedOptimizer(optimizer)
     if params['use_tpu']:
         optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)
     with tf.control_dependencies(update_ops):
@@ -428,7 +453,8 @@ def model_inference_fn(features, training, params):
 
     def mg_res_layer(inputs):
         residual = residual_inner(inputs)
-        output = mg_activation(inputs + residual)
+        fixed = tf.math.add_n([inputs, residual])
+        output = mg_activation(fixed)
         return output
 
     def mg_squeeze_excitation_layer(inputs):
@@ -538,15 +564,26 @@ def get_estimator():
 
 
 def _get_nontpu_estimator():
-    session_config = tf.ConfigProto()
+    session_config = tf.ConfigProto(
+                        intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                        inter_op_parallelism_threads=FLAGS.num_inter_threads)
     session_config.gpu_options.allow_growth = True
+    model_dir = None
+    if(not FLAGS.dist_train) or (hvd.rank()==0):
+        model_dir = FLAGS.work_dir
+        step_count_steps = 50
+        summary_steps = FLAGS.summary_steps
+    else:
+        step_count_steps = 1000000
+        summary_steps = 1000000
     run_config = tf.estimator.RunConfig(
-        save_summary_steps=FLAGS.summary_steps,
+        log_step_count_steps = step_count_steps,
+        save_summary_steps=summary_steps,
         keep_checkpoint_max=FLAGS.keep_checkpoint_max,
         session_config=session_config)
     return tf.estimator.Estimator(
         model_fn,
-        model_dir=FLAGS.work_dir,
+        model_dir=model_dir,
         config=run_config,
         params=FLAGS.flag_values_dict())
 
@@ -618,14 +655,85 @@ def export_model(model_path):
         print("Copying {} to {}".format(filename, destination_path))
         tf.gfile.Copy(filename, destination_path)
 
+def generate_min_max_log(log_graph_file, tf_records, log_file):
+  cmd = 'numactl -N 0 -l python3 produce_min_max_log.py'
+  cmd += ' --input_graph={0}'.format(log_graph_file)
+  cmd += ' --data_location={0}'.format(tf_records)
+  cmd += ' --num_steps={0}'.format(FLAGS.quantize_test_steps)
+  cmd += ' --batch_size={0}'.format(FLAGS.quantize_test_batch_size)
+  cmd += ' --random_rotation={0}'.format(FLAGS.random_rotation)
+  cmd += ' 2> {0}'.format(log_file)
+  print(cmd)
+  subprocess.call(cmd, shell=True)
+
+def quantization(opt_graph, model_path, tf_records, eval_min_max_every_epoch):
+  # first_quantize
+  rewriter = quantize_graph.GraphRewriter(opt_graph, 'eightbit', None, None, True, [], [])
+  first_quantize_graph = rewriter.rewrite(["policy_output", "value_output"])
+
+  if eval_min_max_every_epoch:
+    # insert_min_max_log
+    transform = 'insert_logging(op=RequantizationRange, show_name=true, message="__requant_min_max:")'
+    log_graph = TransformGraph(first_quantize_graph, ["pos_tensor"],
+                                      ["policy_output", "value_output"], [transform])
+    with tf.gfile.FastGFile(model_path + '_for_min_max.pb', 'wb') as f:
+        f.write(log_graph.SerializeToString())
+
+    # generate_min_max_log
+    with logged_timer('minmax time'):
+      generate_min_max_log(model_path + '_for_min_max.pb', tf_records, model_path + 'log.txt')
+
+  # apply_calibration
+  transform = 'freeze_requantization_ranges(min_max_log_file="{0}")'.format(model_path + 'log.txt')
+  calibration_graph = TransformGraph(first_quantize_graph, ["pos_tensor"],
+                                    ["policy_output", "value_output"], [transform])
+
+  # fuse_requantize
+  transform = 'fuse_quantized_conv_and_requantize strip_unused_nodes'
+  output_graph = TransformGraph(calibration_graph, ["pos_tensor"],
+                                    ["policy_output", "value_output"], [transform])
+  return output_graph
+
+def optimize_graph(input_graph, model_path, quantizing_graph, tf_records, eval_min_max_every_epoch):
+  fp32_graph = graph_pb2.GraphDef()
+  with tf.gfile.Open(input_graph, "rb") as read_f:
+    data = read_f.read()
+    fp32_graph.ParseFromString(data)
+
+  opt_graph = optimize_for_inference(
+      fp32_graph,
+      ["pos_tensor"],
+      ["policy_output", "value_output"],
+      dtypes.float32.as_datatype_enum,
+      False)
+
+  if(quantizing_graph):
+    output_graph = quantization(opt_graph, model_path, tf_records, eval_min_max_every_epoch)
+  else:
+    output_graph = opt_graph
+
+  with tf.gfile.GFile(model_path + '.pb', 'wb') as write_f:
+      write_f.write(output_graph.SerializeToString())
+
+def get_input_tensor(graph):
+  return graph.get_tensor_by_name('pos_tensor:0')
+def get_output_tensor(graph):
+  policy_output = graph.get_tensor_by_name('policy_output:0')
+  value_output = graph.get_tensor_by_name('value_output:0')
+  return policy_output, value_output
 
 def freeze_graph(model_path):
     n = DualNetwork(model_path)
     out_graph = tf.graph_util.convert_variables_to_constants(
         n.sess, n.sess.graph.as_graph_def(), ["policy_output", "value_output"])
+    output_graph_def = optimize_for_inference(
+        out_graph,
+        ["pos_tensor"],
+        ["policy_output", "value_output"],
+        dtypes.float32.as_datatype_enum,
+        False)
     with tf.gfile.GFile(model_path + '.pb', 'wb') as f:
-        f.write(out_graph.SerializeToString())
-
+        f.write(output_graph_def.SerializeToString())
 
 def freeze_graph_tpu(model_path):
     """Custom freeze_graph implementation for Cloud TPU."""
diff --git a/ml_perf/eval_models.py b/ml_perf/eval_models.py
index 74702e4..b552f42 100644
--- a/ml_perf/eval_models.py
+++ b/ml_perf/eval_models.py
@@ -23,7 +23,7 @@ import os
 from absl import app
 from reference_implementation import evaluate_model, wait
 from rl_loop import fsdb
-
+import ml_perf.mlp_log as mll
 
 def load_train_times():
   models = []
@@ -43,10 +43,17 @@ def main(unused_argv):
   target = 'tf,' + os.path.join(fsdb.models_dir(), 'target.pb')
   models = load_train_times()
   for i, (timestamp, name, path) in enumerate(models):
+    mll.eval_start(i)
     winrate = wait(evaluate_model(path, target, sgf_dir, i + 1))
+    mll.eval_stop(i)
+    mll.eval_accuracy(i, winrate)
     if winrate >= 0.50:
       print('Model {} beat target after {}s'.format(name, timestamp))
-      break
+      mll.eval_result(i, timestamp)
+      mll.run_stop('success')
+      return
+  mll.eval_result(i, 0)
+  mll.run_stop('aborted')
 
 
 if __name__ == '__main__':
diff --git a/ml_perf/execute.py b/ml_perf/execute.py
new file mode 100644
index 0000000..00a6bed
--- /dev/null
+++ b/ml_perf/execute.py
@@ -0,0 +1,69 @@
+# Copyright 2019 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Run the command in multi-instance mode
+
+If there is a --seed parameter from input, change seed to generate randomness among instances
+
+Args:
+  num_instance: the number of instance needed to start
+"""
+
+import sys
+sys.path.insert(0, '.')  # nopep8
+
+import asyncio
+from ml_perf.utils import *
+
+from absl import app, flags
+
+flags.DEFINE_integer('num_instance', 1, 'Number of instances for selfplay')
+
+FLAGS = flags.FLAGS
+
+# Self-play a number of games.
+async def do_execute_mi():
+
+  num_instance = FLAGS.num_instance
+
+  start_copy = False
+  arg_list = []
+  for arg in sys.argv:
+    if start_copy:
+      arg_list.append(arg)
+    if arg == '--':
+      start_copy = True
+
+  if num_instance > 1:
+    result_list = checked_run_mi(
+      num_instance,
+      *arg_list
+    )
+    for result in result_list:
+      # TODO needs to be more generic
+      print ('\n'.join(result.split('\n')[-7:]))
+  else:
+    result = checked_run(
+      *arg_list
+    )
+    print (result)
+
+def main(unused_argv):
+  try:
+    wait(do_execute_mi())
+  finally:
+    asyncio.get_event_loop().close()
+
+if __name__ == '__main__':
+  app.run(main)
diff --git a/ml_perf/flags/9.mn/architecture.flags b/ml_perf/flags/9.mn/architecture.flags
new file mode 100644
index 0000000..ec2abf4
--- /dev/null
+++ b/ml_perf/flags/9.mn/architecture.flags
@@ -0,0 +1,7 @@
+# architecture.flags: Flags that control the model architecture.
+
+--conv_width=32
+--fc_width=64
+--trunk_layers=9
+--value_cost_weight=0.25
+--summary_steps=64
diff --git a/ml_perf/flags/9.mn/bootstrap.flags b/ml_perf/flags/9.mn/bootstrap.flags
new file mode 100644
index 0000000..0283a92
--- /dev/null
+++ b/ml_perf/flags/9.mn/bootstrap.flags
@@ -0,0 +1,9 @@
+# bootstrap.flags
+# Flags for the first bootstrap round of selfplay.
+
+--flagfile=ml_perf/flags/9.mn/selfplay.flags
+
+# Don't perform holdout for the first bootstrap round.
+--holdout_pct=0
+
+--num_readouts=20
diff --git a/ml_perf/flags/9.mn/bootstrap_mi.flags b/ml_perf/flags/9.mn/bootstrap_mi.flags
new file mode 100644
index 0000000..f4da7c1
--- /dev/null
+++ b/ml_perf/flags/9.mn/bootstrap_mi.flags
@@ -0,0 +1,3 @@
+--num_games=8192
+--parallel_games=4
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/eval.flags b/ml_perf/flags/9.mn/eval.flags
new file mode 100644
index 0000000..f07d715
--- /dev/null
+++ b/ml_perf/flags/9.mn/eval.flags
@@ -0,0 +1,6 @@
+# eval.flags: Flags for playing eval games.
+
+--flagfile=ml_perf/flags/9.mn/selfplay.flags
+
+# Play fewer games for eval than selfplay.
+--parallel_games=1
diff --git a/ml_perf/flags/9.mn/eval_mi.flags b/ml_perf/flags/9.mn/eval_mi.flags
new file mode 100644
index 0000000..00e960c
--- /dev/null
+++ b/ml_perf/flags/9.mn/eval_mi.flags
@@ -0,0 +1,3 @@
+--num_games=100
+--parallel_games=1
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/rl_loop.flags b/ml_perf/flags/9.mn/rl_loop.flags
new file mode 100644
index 0000000..0f85640
--- /dev/null
+++ b/ml_perf/flags/9.mn/rl_loop.flags
@@ -0,0 +1,11 @@
+# rl_loop.flags: Flags for the reinforcement learning loop.
+
+--flags_dir=ml_perf/flags/9.mn/
+--checkpoint_dir=ml_perf/checkpoint/9/
+
+--iterations=30
+--gating_win_rate=0.49
+--window_size=10
+--engine=tf
+--parallel_post_train=2
+--train_instance_per_numa=2
diff --git a/ml_perf/flags/9.mn/selfplay.flags b/ml_perf/flags/9.mn/selfplay.flags
new file mode 100644
index 0000000..7084ec0
--- /dev/null
+++ b/ml_perf/flags/9.mn/selfplay.flags
@@ -0,0 +1,14 @@
+# selfplay.flags: Flags for selfplay.
+
+# This flagfile also serves as the base for the boostrap & eval stages of
+# the RL loop.
+
+--num_readouts=240
+--value_init_penalty=0.2
+--holdout_pct=0.03
+--disable_resign_pct=0.1
+--resign_threshold=-0.99
+
+# Device-specific selfplay flags.
+--parallel_games=2
+--virtual_losses=8
diff --git a/ml_perf/flags/9.mn/selfplay_mi.flags b/ml_perf/flags/9.mn/selfplay_mi.flags
new file mode 100644
index 0000000..43df7a2
--- /dev/null
+++ b/ml_perf/flags/9.mn/selfplay_mi.flags
@@ -0,0 +1,3 @@
+--num_games=4096
+--parallel_games=2
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/train.flags b/ml_perf/flags/9.mn/train.flags
new file mode 100644
index 0000000..19f8b9d
--- /dev/null
+++ b/ml_perf/flags/9.mn/train.flags
@@ -0,0 +1,15 @@
+# train.flags: Flags for training.
+
+--flagfile=ml_perf/flags/9.mn/architecture.flags
+
+--shuffle_buffer_size=10000
+--filter_amount=0.5
+
+# Device specific hyperparameters re: batch size and LR schedules.
+--train_batch_size=8192
+--lr_rates=0.32
+--lr_rates=0.032
+--lr_rates=0.0032
+--lr_boundaries=12500
+--lr_boundaries=18750
+--l2_strength=0.0001
diff --git a/ml_perf/flags/9.mn/validate.flags b/ml_perf/flags/9.mn/validate.flags
new file mode 100644
index 0000000..de4f22d
--- /dev/null
+++ b/ml_perf/flags/9.mn/validate.flags
@@ -0,0 +1,7 @@
+# validate.flags Flags for validation.
+
+--flagfile=ml_perf/flags/9.mn/architecture.flags
+
+--examples_to_validate=256
+--train_batch_size=64
+--summary_steps=2
diff --git a/ml_perf/flags/9/bootstrap.flags b/ml_perf/flags/9/bootstrap.flags
index 4e7341e..29c66d6 100644
--- a/ml_perf/flags/9/bootstrap.flags
+++ b/ml_perf/flags/9/bootstrap.flags
@@ -6,5 +6,4 @@
 # Don't perform holdout for the first bootstrap round.
 --holdout_pct=0
 
---num_games=8192
 --num_readouts=20
diff --git a/ml_perf/flags/9/bootstrap_mi.flags b/ml_perf/flags/9/bootstrap_mi.flags
new file mode 100644
index 0000000..f4da7c1
--- /dev/null
+++ b/ml_perf/flags/9/bootstrap_mi.flags
@@ -0,0 +1,3 @@
+--num_games=8192
+--parallel_games=4
+--multi_instance=True
diff --git a/ml_perf/flags/9/eval.flags b/ml_perf/flags/9/eval.flags
index aecf855..9f8759e 100644
--- a/ml_perf/flags/9/eval.flags
+++ b/ml_perf/flags/9/eval.flags
@@ -3,5 +3,4 @@
 --flagfile=ml_perf/flags/9/selfplay.flags
 
 # Play fewer games for eval than selfplay.
---num_games=100
---parallel_games=100
+--parallel_games=1
diff --git a/ml_perf/flags/9/eval_mi.flags b/ml_perf/flags/9/eval_mi.flags
new file mode 100644
index 0000000..00e960c
--- /dev/null
+++ b/ml_perf/flags/9/eval_mi.flags
@@ -0,0 +1,3 @@
+--num_games=100
+--parallel_games=1
+--multi_instance=True
diff --git a/ml_perf/flags/9/rl_loop.flags b/ml_perf/flags/9/rl_loop.flags
index c6b6dc2..4b8dc29 100644
--- a/ml_perf/flags/9/rl_loop.flags
+++ b/ml_perf/flags/9/rl_loop.flags
@@ -3,8 +3,8 @@
 --flags_dir=ml_perf/flags/9/
 --checkpoint_dir=ml_perf/checkpoint/9/
 
---iterations=50
+--iterations=30
 --gating_win_rate=0.49
 --window_size=10
 --engine=tf
---parallel_post_train=true
+--train_instance_per_numa=2
diff --git a/ml_perf/flags/9/selfplay.flags b/ml_perf/flags/9/selfplay.flags
index 3d8d64c..26e58ef 100644
--- a/ml_perf/flags/9/selfplay.flags
+++ b/ml_perf/flags/9/selfplay.flags
@@ -3,7 +3,6 @@
 # This flagfile also serves as the base for the boostrap & eval stages of
 # the RL loop.
 
---num_games=4096
 --num_readouts=240
 --value_init_penalty=0.2
 --holdout_pct=0.03
@@ -11,5 +10,5 @@
 --resign_threshold=-0.99
 
 # Device-specific selfplay flags.
---parallel_games=2048
+--parallel_games=8
 --virtual_losses=8
diff --git a/ml_perf/flags/9/selfplay_mi.flags b/ml_perf/flags/9/selfplay_mi.flags
new file mode 100644
index 0000000..b1db0e8
--- /dev/null
+++ b/ml_perf/flags/9/selfplay_mi.flags
@@ -0,0 +1,3 @@
+--num_games=4096
+--parallel_games=8
+--multi_instance=True
diff --git a/ml_perf/flags/9/train.flags b/ml_perf/flags/9/train.flags
index aa1a3cf..a65044d 100644
--- a/ml_perf/flags/9/train.flags
+++ b/ml_perf/flags/9/train.flags
@@ -6,10 +6,10 @@
 --filter_amount=0.5
 
 # Device specific hyperparameters re: batch size and LR schedules.
---train_batch_size=4096
---lr_rates=0.16
---lr_rates=0.016
---lr_rates=0.0016
---lr_boundaries=25000
---lr_boundaries=37500
+--train_batch_size=8192
+--lr_rates=0.32
+--lr_rates=0.032
+--lr_rates=0.0032
+--lr_boundaries=12500
+--lr_boundaries=18750
 --l2_strength=0.0001
diff --git a/ml_perf/hostlist.sh b/ml_perf/hostlist.sh
new file mode 100755
index 0000000..94465f2
--- /dev/null
+++ b/ml_perf/hostlist.sh
@@ -0,0 +1,3 @@
+# generate a list of host ip or hostname
+# one ip/hostname per line
+cat $HOSTLIST.txt
diff --git a/ml_perf/mlp_log.py b/ml_perf/mlp_log.py
new file mode 100644
index 0000000..501baf1
--- /dev/null
+++ b/ml_perf/mlp_log.py
@@ -0,0 +1,118 @@
+# Copyright 2019 MLBenchmark Group. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Utilities for compliance logging."""
+
+import logging
+import time
+import inspect
+import sys
+
+def init_start():
+  log('init_start', caller_depth=3)
+
+def init_stop():
+  log('init_stop', caller_depth=3)
+
+def run_start():
+  log('run_start', caller_depth=3)
+
+def run_stop(status):
+  assert status == 'success' or status == 'aborted'
+  log('run_stop',
+      meta_data = {'status':status},
+      caller_depth=3)
+
+def block_start(epoch, count):
+  log('block_start',
+      meta_data = {'first_epoch_num':epoch,
+                   'epoch_count':count},
+      caller_depth=3)
+
+def block_stop(epoch):
+  log('block_stop',
+      meta_data = {'first_epoch_num':epoch},
+      caller_depth=3)
+
+def epoch_start(epoch):
+  log('epoch_start',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def epoch_stop(epoch):
+  log('epoch_stop',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_start(epoch):
+  log('eval_start',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_stop(epoch):
+  log('eval_stop',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_accuracy(epoch, accuracy):
+  log('eval_accuracy',
+      val = '{}'.format(accuracy),
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def global_batch_size(batch_size):
+  log('global_batch_size',
+      val = '{}'.format(batch_size),
+      caller_depth=3)
+
+def lr_rates(rates):
+  log('opt_base_learning_rate',
+      val = '{}'.format(rates),
+      caller_depth=3)
+
+def lr_boundaries(boundaries):
+  log('opt_learning_rate_decay_boundary_steps',
+      val = '{}'.format(boundaries),
+      caller_depth=3)
+
+def save_model(iteration):
+  log('save_model',
+      meta_data = {'iteration':iteration},
+      caller_depth=3)
+
+def eval_result(iteration, timestamp):
+  log('eval_result',
+      meta_data = {'iteration':iteration, 'timestamp':timestamp},
+      caller_depth=3)
+
+def log(key, val='null', meta_data = None, caller_depth=2):
+  filename, lineno = get_caller(caller_depth)
+  meta_dict = {'lineno': lineno, 'file': filename}
+  if meta_data != None:
+    meta_dict.update(meta_data)
+  meta_string = '{}'.format(meta_dict)
+  print(':::MLL %f %s: {"value": %s, "metadata": %s}'%(time.time(), key, val, meta_string), file=sys.stderr)
+
+def get_caller(stack_index=2, root_dir=None):
+  ''' Returns file.py:lineno of your caller. A stack_index of 2 will provide
+      the caller of the function calling this function. Notice that stack_index
+      of 2 or more will fail if called from global scope. '''
+  caller = inspect.getframeinfo(inspect.stack()[stack_index][0])
+
+  # Trim the filenames for readability.
+  filename = caller.filename
+  if root_dir is not None:
+    filename = re.sub("^" + root_dir + "/", "", filename)
+  return (filename, caller.lineno)
diff --git a/ml_perf/reference_implementation.py b/ml_perf/reference_implementation.py
index 1ca724e..e04d873 100644
--- a/ml_perf/reference_implementation.py
+++ b/ml_perf/reference_implementation.py
@@ -26,17 +26,23 @@ import random
 import re
 import shutil
 import subprocess
+import functools
 import tensorflow as tf
 import time
+import copy
+import multiprocessing as mp
 from ml_perf.utils import *
+import ml_perf.mlp_log as mll
 
 from absl import app, flags
 from rl_loop import example_buffer, fsdb
-from tensorflow import gfile
+import dual_net
+
+from tensorflow.python.platform import gfile
 
 N = int(os.environ.get('BOARD_SIZE', 19))
 
-flags.DEFINE_string('checkpoint_dir', 'ml_perf/checkpoint/{}'.format(N),
+flags.DEFINE_string('checkpoint_dir', None,
                     'The checkpoint directory specify a start model and a set '
                     'of golden chunks used to start training.  If not '
                     'specified, will start from scratch.')
@@ -58,15 +64,33 @@ flags.DEFINE_string('flags_dir', None,
 
 flags.DEFINE_integer('window_size', 10,
                      'Maximum number of recent selfplay rounds to train on.')
+flags.DEFINE_integer('golden_chunk_split', 16,
+                     'Golden chunk of each selfplay is splited to accelerate write golden chunk')
 
-flags.DEFINE_boolean('parallel_post_train', False,
-                     'If true, run the post-training stages (eval, validation '
-                     '& selfplay) in parallel.')
+flags.DEFINE_integer('parallel_post_train', 0,
+                     '0: run the post-training stages in serial mode'
+                     '1: run the post-training stages (eval, validation '
+                     '& selfplay) in parallel.'
+                     '2: run the post-train stage in pipeline mode.')
 
 flags.DEFINE_string('engine', 'tf', 'The engine to use for selfplay.')
 
-FLAGS = flags.FLAGS
+flags.DEFINE_integer('physical_cores', None, 'The number of cores for each node.')
+flags.DEFINE_integer('virtual_cores', None, 'The number of SMT for each node.')
+flags.DEFINE_integer('numa_cores', None, 'The number of core for each numa node.')
+flags.DEFINE_integer('train_instance_per_numa', 2, 'The number of instance for each numa node.')
+
+flags.DEFINE_multi_string('train_node', [], 'The node:core list for training')
+flags.DEFINE_multi_string('eval_node', [], 'The node list for evaluation')
+flags.DEFINE_multi_string('selfplay_node', [], 'The node list for selfplay.')
 
+flags.DEFINE_bool('quantization', True, 'Using Int8 if true.')
+flags.DEFINE_bool('eval_min_max_every_epoch', True, 'Genereting min max log every epoch if true.')
+flags.DEFINE_boolean('random_rotation', True, 'Do random rotation when running for min&max log.')
+flags.DEFINE_integer('quantize_test_steps', 5, 'The steps to run for min&max log.')
+flags.DEFINE_integer('quantize_test_batch_size', 16, 'The batch size for running inference for min&max log.')
+
+FLAGS = flags.FLAGS
 
 class State:
   """State data used in each iteration of the RL loop.
@@ -133,17 +157,15 @@ class WinStats:
     pattern = '\s*(\S+)' + '\s+(\d+)' * 8
     match = re.search(pattern, line)
     if match is None:
-      raise ValueError('Can\t parse line "{}"'.format(line))
+      raise ValueError('Can\'t parse line "{}"'.format(line))
     self.model_name = match.group(1)
     raw_stats = [float(x) for x in match.groups()[1:]]
     self.black_wins = ColorWinStats(*raw_stats[:4])
     self.white_wins = ColorWinStats(*raw_stats[4:])
     self.total_wins = self.black_wins.total + self.white_wins.total
 
-
 def initialize_from_checkpoint(state):
   """Initialize the reinforcement learning loop from a checkpoint."""
-
   # The checkpoint's work_dir should contain the most recently trained model.
   model_paths = glob.glob(os.path.join(FLAGS.checkpoint_dir,
                                        'work_dir/model.ckpt-*.pb'))
@@ -152,18 +174,19 @@ def initialize_from_checkpoint(state):
                        'got [{}]'.format(', '.join(model_paths)))
   start_model_path = model_paths[0]
 
-  # Copy the latest trained model into the models directory and use it on the
-  # first round of selfplay.
-  state.best_model_name = 'checkpoint'
-  shutil.copy(start_model_path,
-              os.path.join(fsdb.models_dir(), state.best_model_name + '.pb'))
-
   # Copy the training chunks.
   golden_chunks_dir = os.path.join(FLAGS.checkpoint_dir, 'golden_chunks')
   for basename in os.listdir(golden_chunks_dir):
     path = os.path.join(golden_chunks_dir, basename)
     shutil.copy(path, fsdb.golden_chunk_dir())
 
+  # Copy the latest trained model into the models directory and use it on the
+  # first round of selfplay.
+  state.best_model_name = 'checkpoint'
+  best_model_path = os.path.join(fsdb.models_dir(), state.best_model_name)
+
+  dual_net.optimize_graph(start_model_path, best_model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz', FLAGS.eval_min_max_every_epoch)
+
   # Copy the training files.
   work_dir = os.path.join(FLAGS.checkpoint_dir, 'work_dir')
   for basename in os.listdir(work_dir):
@@ -171,22 +194,72 @@ def initialize_from_checkpoint(state):
     shutil.copy(path, fsdb.working_dir())
 
 
+
 def parse_win_stats_table(stats_str, num_lines):
   result = []
   lines = stats_str.split('\n')
-  while True:
-    # Find the start of the win stats table.
-    assert len(lines) > 1
-    if 'Black' in lines[0] and 'White' in lines[0] and 'm.lmt.' in lines[1]:
-        break
-    lines = lines[1:]
-
-  # Parse the expected number of lines from the table.
-  for line in lines[2:2 + num_lines]:
-    result.append(WinStats(line))
 
-  return result
+  while True:
+    while True:
+      # Find the start of the win stats table.
+      if len(lines) == 0:
+        return result
+      if 'Black' in lines[0] and 'White' in lines[0] and 'm.lmt.' in lines[1]:
+          break
+      lines = lines[1:]
+
+    # Parse the expected number of lines from the table.
+    for line in lines[2:2 + num_lines]:
+      stat = WinStats(line)
+      for s in result:
+        if s.model_name == stat.model_name:
+          s.black_wins.total += stat.black_wins.total
+          s.white_wins.total += stat.white_wins.total
+          s.total_wins += stat.total_wins
+          stat = None
+          break
+      if stat != None:
+        result.append(stat)
+    lines = lines[2 + num_lines:]
+
+def extract_multi_instance(cmd):
+  cmd_list = flags.FlagValues().read_flags_from_files(cmd)
+  new_cmd_list = []
+  multi_instance = False
+  num_instance = 0
+  num_games = 0
+  parallel_games = 0
+
+  for arg in cmd_list:
+    argsplit = arg.split('=', 1)
+    flag = argsplit[0]
+    if flag == '--multi_instance':
+      if argsplit[1] == 'True':
+        multi_instance = True
+      else:
+        multi_instance = False
+    elif flag == '--num_games':
+      num_games = int(argsplit[1])
+    elif flag == '--parallel_games':
+      parallel_games = int(argsplit[1])
+
+  if multi_instance:
+    if num_games % parallel_games != 0:
+      logging.error('Error num_games must be multiply of %d', parallel_games)
+      raise RuntimeError('incompatible num_games/parallel_games combination')
+    num_instance = num_games//parallel_games
+
+  for arg in cmd_list:
+    argsplit = arg.split('=', 1)
+    flag = argsplit[0]
+    if flag == '--multi_instance':
+      pass
+    elif multi_instance and flag == '--num_games':
+      pass
+    else:
+      new_cmd_list.append(arg)
 
+  return multi_instance, num_instance, new_cmd_list
 
 async def run(*cmd):
   """Run the given subprocess command in a coroutine.
@@ -214,8 +287,35 @@ async def run(*cmd):
   # Split stdout into lines.
   return stdout.split('\n')
 
+async def run_distributed(genvs, num_instance, hosts, proclists, numa_nodes,
+                          seed, *cmd):
+  """Run the given subprocess command in a coroutine.
+
+  Args:
+    *cmd: the command to run and its arguments.
+
+  Returns:
+    The output that the command wrote to stdout as a list of strings, one line
+    per element (stderr output is piped to stdout).
 
-def get_golden_chunk_records():
+  Raises:
+    RuntimeError: if the command returns a non-zero result.
+  """
+
+  stdout = await checked_run_distributed(genvs, num_instance, hosts, proclists,
+                                         numa_nodes, seed, fsdb.mpi_log_dir(), *cmd)
+
+  log_path = os.path.join(FLAGS.base_dir, get_cmd_name(cmd) + '.log')
+  with gfile.Open(log_path, 'a') as f:
+    f.write(expand_cmd_str(cmd))
+    f.write('\n')
+    f.write(stdout)
+    f.write('\n')
+
+  # Split stdout into lines.
+  return stdout.split('\n')
+
+def get_golden_chunk_records(window_size):
   """Return up to num_records of golden chunks to train on.
 
   Returns:
@@ -223,9 +323,17 @@ def get_golden_chunk_records():
   """
 
   pattern = os.path.join(fsdb.golden_chunk_dir(), '*.zz')
-  return sorted(tf.gfile.Glob(pattern), reverse=True)[:FLAGS.window_size]
+  if window_size > FLAGS.golden_chunk_split * FLAGS.window_size:
+    window_size = FLAGS.golden_chunk_split * FLAGS.window_size
+  return sorted(tf.gfile.Glob(pattern), reverse=True)[:window_size]
 
 
+def gen_golden_chunk(files, state):
+  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+  buffer.parallel_fill(files[1], threads=1)
+  buffer.flush(os.path.join(fsdb.golden_chunk_dir(),
+                            state.output_model_name + '-{}.tfrecord.zz'.format(files[0])))
+
 # Self-play a number of games.
 async def selfplay(state, flagfile='selfplay'):
   """Run selfplay and write a training chunk to the fsdb golden_chunk_dir.
@@ -239,39 +347,80 @@ async def selfplay(state, flagfile='selfplay'):
   output_dir = os.path.join(fsdb.selfplay_dir(), state.output_model_name)
   holdout_dir = os.path.join(fsdb.holdout_dir(), state.output_model_name)
 
-  lines = await run(
-      'bazel-bin/cc/selfplay',
-      '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
-      '--model={}'.format(state.best_model_path),
-      '--output_dir={}'.format(output_dir),
-      '--holdout_dir={}'.format(holdout_dir),
-      '--seed={}'.format(state.seed))
-  result = '\n'.join(lines[-6:])
-  logging.info(result)
-  stats = parse_win_stats_table(result, 1)[0]
-  num_games = stats.total_wins
-  logging.info('Black won %0.3f, white won %0.3f',
-               stats.black_wins.total / num_games,
-               stats.white_wins.total / num_games)
-
-  # Write examples to a single record.
-  pattern = os.path.join(output_dir, '*', '*.zz')
-  random.seed(state.seed)
-  tf.set_random_seed(state.seed)
-  np.random.seed(state.seed)
-  # TODO(tommadams): This method of generating one golden chunk per generation
-  # is sub-optimal because each chunk gets reused multiple times for training,
-  # introducing bias. Instead, a fresh dataset should be uniformly sampled out
-  # of *all* games in the training window before the start of each training run.
-  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
-
-  # TODO(tommadams): parallel_fill is currently non-deterministic. Make it not
-  # so.
-  logging.info('Writing golden chunk from "{}"'.format(pattern))
-  buffer.parallel_fill(tf.gfile.Glob(pattern))
-  buffer.flush(os.path.join(fsdb.golden_chunk_dir(),
-                            state.output_model_name + '.tfrecord.zz'))
-
+  multi_instance, num_instance, flag_list = extract_multi_instance(
+      ['--flagfile={}_mi.flags'.format(os.path.join(FLAGS.flags_dir, flagfile))])
+  sp_cmd = ['bazel-bin/cc/selfplay',
+            '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
+            '--model={}'.format(state.best_model_path),
+            '--output_dir={}'.format(output_dir),
+            '--holdout_dir={}'.format(holdout_dir)]
+  if not multi_instance:
+    lines = await run(
+        *sp_cmd,
+        '--seed={}'.format(state.seed))
+  else:
+    if FLAGS.selfplay_node == []:
+      # run selfplay locally
+      lines = await run(
+          'python3', 'ml_perf/execute.py',
+          '--num_instance={}'.format(num_instance),
+          '--',
+          *sp_cmd,
+          '--seed={}'.format(state.seed))
+    else:
+      with logged_timer('selfplay mn'):
+        # run one selfplay instance per host
+        lines = await run_distributed(
+            ['LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow'],
+            num_instance, FLAGS.selfplay_node, None, None, state.seed,
+            *sp_cmd)
+
+  result = '\n'.join(lines)
+  with logged_timer('parse win stats'):
+    stats = parse_win_stats_table(result, 1)[0]
+    num_games = stats.total_wins
+    black_total = stats.black_wins.total
+    white_total = stats.white_wins.total
+
+    logging.info('Black won %0.3f, white won %0.3f',
+                 black_total / num_games,
+                 white_total / num_games)
+    bias = abs(white_total - black_total)/num_games
+    logging.info('Black total %d, white total %d, total games %d, bias %0.3f.',
+                 black_total, white_total, num_games, bias)
+
+  with logged_timer('generate golden chunk'):
+    # Write examples to a single record.
+    pattern = os.path.join(output_dir, '*', '*.zz')
+    files = tf.gfile.Glob(pattern)
+
+    random.seed(state.seed)
+    tf.set_random_seed(state.seed)
+    np.random.seed(state.seed)
+
+    # TODO(tommadams): This method of generating one golden chunk per generation
+    # is sub-optimal because each chunk gets reused multiple times for training,
+    # introducing bias. Instead, a fresh dataset should be uniformly sampled out
+    # of *all* games in the training window before the start of each training run.
+
+    # TODO(tommadams): parallel_fill is currently non-deterministic. Make it not
+    # so.
+    logging.info('Writing golden chunk from "{}"'.format(pattern))
+    threads = FLAGS.golden_chunk_split
+    file_list = []
+    files_number = len(files)
+    chunk_size = files_number // threads
+
+    # split files into N seperate parts
+    for i in range(threads):
+      if i == threads - 1:
+        file_list += [[i, files[chunk_size * i :]]]
+      else:
+        file_list += [[i, files[chunk_size * i : chunk_size * (i + 1)]]]
+    pool = mp.Pool(threads)
+    pool.map(functools.partial(gen_golden_chunk, state=state), file_list)
+
+  return bias
 
 async def train(state, tf_records):
   """Run training and write a new model to the fsdb models_dir.
@@ -280,15 +429,66 @@ async def train(state, tf_records):
     state: the RL loop State instance.
     tf_records: a list of paths to TensorFlow records to train on.
   """
+  train_node = FLAGS.train_node
+  num_node = len(train_node)
+  if num_node == 0:
+    dist_train = False
+  else:
+    dist_train = True
+
+  if dist_train:
+    intra_threads = FLAGS.numa_cores // FLAGS.train_instance_per_numa - 1
+    numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+    instance_per_node = numa_per_node * FLAGS.train_instance_per_numa
+
+    mpi_async_progress = ''
+    for i in range(numa_per_node):
+      for j in range(FLAGS.train_instance_per_numa):
+        if (not i==0) or (not j==0):
+          mpi_async_progress += ','
+        mpi_async_progress += '{}'.format(i * FLAGS.numa_cores + j)
+  else:
+    intra_threads = FLAGS.physical_cores
 
   model_path = os.path.join(fsdb.models_dir(), state.train_model_name)
-  await run(
-      'python3', 'train.py', *tf_records,
+  cmd = ['python3', 'train.py', *tf_records,
       '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags')),
       '--work_dir={}'.format(fsdb.working_dir()),
       '--export_path={}'.format(model_path),
       '--training_seed={}'.format(state.seed),
-      '--freeze=true')
+      '--freeze=True',
+      '--num_inter_threads=1',
+      '--num_intra_threads={}'.format(intra_threads)]
+
+  if(dist_train):
+    genvs = ['HOROVOD_FUSION_THRESHOLD=134217728',
+             'KMP_BLOCKTIME=0',
+             'KMP_HW_SUBSET=1T',
+             'OMP_BIND_PROC=true',
+             'I_MPI_ASYNC_PROGRESS_PIN=' + mpi_async_progress,
+             'OMP_NUM_THREADS={}'.format(intra_threads)]
+    hosts = []
+    proclists = []
+    numa_nodes = []
+    for node in range(num_node):
+      # add all instance to the list
+      for numa in range(numa_per_node):
+        for instance in range(FLAGS.train_instance_per_numa):
+          hosts += [train_node[node]]
+          proclist = numa * FLAGS.numa_cores + FLAGS.train_instance_per_numa + instance * intra_threads
+          proclists += ['{}'.format(proclist)]
+          numa_nodes += ['{}'.format(numa)]
+
+    lines = await run_distributed(genvs, 1, hosts, proclists, numa_nodes, None, *cmd, '--dist_train=True')
+  else:
+    lines = await run(*cmd)
+  print('\n'.join(lines), file=sys.stderr)
+
+def post_train(state):
+  model_path = os.path.join(fsdb.models_dir(), state.train_model_name)
+  dual_net.optimize_graph(model_path + '.pb', model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz', FLAGS.eval_min_max_every_epoch)
+  mll.save_model(state.iter_num-1)
+
   # Append the time elapsed from when the RL was started to when this model
   # was trained.
   elapsed = time.time() - state.start_time
@@ -315,7 +515,7 @@ async def validate(state, holdout_glob):
         '--work_dir={}'.format(fsdb.working_dir()))
 
 
-async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed):
+async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed, flagfile='eval'):
   """Evaluate one model against a target.
 
   Args:
@@ -328,20 +528,49 @@ async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed):
     The win-rate of eval_model against target_model in the range [0, 1].
   """
 
-  lines = await run(
-      'bazel-bin/cc/eval',
-      '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'eval.flags')),
-      '--model={}'.format(eval_model_path),
-      '--model_two={}'.format(target_model_path),
-      '--sgf_dir={}'.format(sgf_dir),
-      '--seed={}'.format(seed))
-  result = '\n'.join(lines[-7:])
+  multi_instance, num_instance, flag_list = extract_multi_instance(
+      ['--flagfile={}_mi.flags'.format(os.path.join(FLAGS.flags_dir, flagfile))])
+  eval_cmd = ['bazel-bin/cc/eval',
+              '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
+              '--model={}'.format(eval_model_path),
+              '--model_two={}'.format(target_model_path),
+              '--sgf_dir={}'.format(sgf_dir)]
+  if not multi_instance:
+    lines = await run(*eval_cmd, '--seed={}'.format(seed))
+  else:
+    if FLAGS.eval_node == []:
+      # run eval locally
+      lines = await run(
+          'python3', 'ml_perf/execute.py',
+          '--num_instance={}'.format(num_instance),
+          '--',
+          *eval_cmd,
+          '--seed={}'.format(seed))
+    else:
+      # run one selfplay instance per host
+      lines = await run_distributed(
+          ['LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow'],
+          num_instance, FLAGS.eval_node, None, None, seed,
+          *eval_cmd)
+  result = '\n'.join(lines)
   logging.info(result)
   eval_stats, target_stats = parse_win_stats_table(result, 2)
   num_games = eval_stats.total_wins + target_stats.total_wins
   win_rate = eval_stats.total_wins / num_games
+  eval_total = eval_stats.total_wins
+  black_total = eval_stats.black_wins.total
+  white_total = eval_stats.white_wins.total
+
+  if eval_total != 0:
+    bias = abs(white_total - black_total) / eval_total
+  else:
+    # by definition bias = 0.0 if eval model win zero games
+    bias = 0.0
   logging.info('Win rate %s vs %s: %.3f', eval_stats.model_name,
                target_stats.model_name, win_rate)
+  logging.info('Black total %d, white total %d, eval total %d, bias %0.3f.',
+               black_total, white_total, eval_total, bias)
+
   return win_rate
 
 
@@ -357,24 +586,45 @@ async def evaluate_trained_model(state):
       os.path.join(fsdb.eval_dir(), state.train_model_name), state.seed)
 
 
+async def evaluate_target_model(state):
+  sgf_dir = os.path.join(fsdb.eval_dir(), 'target')
+  target = 'tf,' + os.path.join(fsdb.models_dir(), 'target.pb')
+  return await evaluate_model(
+      state.train_model_path, target, sgf_dir, state.iter_num)
+
 def rl_loop():
   """The main reinforcement learning (RL) loop."""
 
+  # The 'window_size' reflect the split of golden chunk after selfplay
+  # basically each selfplay generate N golden chunks instead of one to
+  # accelerate write golden chunks (N determined by FLAGS.golden_chunk_slit).
+  # Yet this make effective_window_size dynamic.   It should increase by N-1
+  # to keep the effective window size not change.  Then increase by N if no big
+  # chunk left.  Until it reach FLAGS.window_size * FLAGS.golden_chunk_split
+
+  window_size = 0
+  big_chunk_remaining = 0
+
   state = State()
 
-  if FLAGS.checkpoint_dir:
+  if FLAGS.checkpoint_dir != None:
     # Start from a partially trained model.
     initialize_from_checkpoint(state)
+    window_size = len(get_golden_chunk_records(FLAGS.window_size))
+    big_chunk_remaining = window_size
   else:
     # Play the first round of selfplay games with a fake model that returns
     # random noise. We do this instead of playing multiple games using a single
     # model bootstrapped with random noise to avoid any initial bias.
+    mll.epoch_start(state.iter_num)
     wait(selfplay(state, 'bootstrap'))
+    window_size += FLAGS.golden_chunk_split
 
     # Train a real model from the random selfplay games.
-    tf_records = get_golden_chunk_records()
+    tf_records = get_golden_chunk_records(window_size)
     state.iter_num += 1
     wait(train(state, tf_records))
+    post_train(state)
 
     # Select the newly trained model as the best.
     state.best_model_name = state.train_model_name
@@ -382,45 +632,124 @@ def rl_loop():
 
     # Run selfplay using the new model.
     wait(selfplay(state))
+    window_size += FLAGS.golden_chunk_split
+    mll.epoch_stop(state.iter_num - 1)
 
+  first_iter = True
+  state_copy = None
+  model_win_rate = -1.0
   # Now start the full training loop.
   while state.iter_num <= FLAGS.iterations:
-    # Build holdout glob before incrementing the iteration number because we
-    # want to run validation on the previous generation.
-    holdout_glob = os.path.join(fsdb.holdout_dir(), '%06d-*' % state.iter_num,
-                                '*')
-
-    # Train on shuffled game data from recent selfplay rounds.
-    tf_records = get_golden_chunk_records()
-    state.iter_num += 1
-    wait(train(state, tf_records))
-
-    if FLAGS.parallel_post_train:
-      # Run eval, validation & selfplay in parallel.
-      model_win_rate, _, _ = wait([
-          evaluate_trained_model(state),
-          validate(state, holdout_glob),
-          selfplay(state)])
+    with logged_timer('iteration time {}'.format(state.iter_num)):
+      mll.epoch_start(state.iter_num)
+      # Build holdout glob before incrementing the iteration number because we
+      # want to run validation on the previous generation.
+      holdout_glob = os.path.join(fsdb.holdout_dir(), '%06d-*' % state.iter_num,
+                                  '*')
+
+      # Train on shuffled game data from recent selfplay rounds.
+      tf_records = get_golden_chunk_records(window_size)
+
+      if FLAGS.parallel_post_train == 0:
+        state.iter_num += 1
+        wait(train(state, tf_records))
+        post_train(state)
+        # Run eval, validation & selfplay sequentially.
+        wait(selfplay(state))
+        model_win_rate = wait(evaluate_trained_model(state))
+        if model_win_rate >= FLAGS.gating_win_rate:
+          # Promote the trained model to the best model and increment the generation
+          # number.
+          state.best_model_name = state.train_model_name
+          state.gen_num += 1
+        mll.epoch_stop(state.iter_num - 1)
+        #                               ^ compensate iter_num += 1 above
+
+      if FLAGS.parallel_post_train == 1:
+        state.iter_num += 1
+        wait([train(state, tf_records),
+            selfplay(state)])
+        post_train(state)
+        # Run eval, validation & selfplay in parallel.
+        model_win_rate = wait(evaluate_trained_model(state))
+        if model_win_rate >= FLAGS.gating_win_rate:
+          # Promote the trained model to the best model and increment the generation
+          # number.
+          state.best_model_name = state.train_model_name
+          state.gen_num += 1
+        mll.epoch_stop(state.iter_num - 1)
+        #                               ^ compensate iter_num += 1 above
+
+      if FLAGS.parallel_post_train == 2:
+        state_copy = copy.copy(state)
+        state.iter_num += 1
+        # run training and evaluation/validation/selfplay in parallel
+        # this is software pipeline-ish parallelism
+        # start train[iter]
+        # |   start valiation[iter-1]
+        # |   wait for validation
+        # |   if not first time start evaluation[iter-1]
+        # |   if not first time wait for evaluation
+        # |   if not first time check for promotion
+        # |   start selfplay[iter]
+        # |   wait selfplay
+        # wait train
+        train_handle = asyncio.gather(train(state, tf_records), return_exceptions=True)
+        if not first_iter:
+          post_train(state_copy)
+          model_win_rate = wait(evaluate_trained_model(state_copy))
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # Promote the trained model to the best model
+            state.best_model_name = state_copy.train_model_name
+          mll.epoch_stop(state.iter_num - 1 - 1)
+          #                               ^---^-- compensate iter_num += 1 above
+          #                                   +-- it is actually last iteration
+        else:
+          first_iter = False
+        wait(selfplay(state))
+        asyncio.get_event_loop().run_until_complete(train_handle)
+        if not first_iter:
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # Increment the generation number.
+            train_model_name_before = state.train_model_name
+            state.gen_num += 1
+
+            # Output dependency:
+            # In parallel post train mode 1, there is output dependence between
+            # evaluation of iteration i (gen_num++)  and train of iteration i+1
+            # (use gen_num for export model path).  In parallel post train mode
+            # 2 (this mode), the evluation of iteration i is postponed to
+            # iteration i+1 after the training started, thus train of iteration
+            # i+1 won't generate correct model name when promotion needs to
+            # happen.  This part fix up the model name when evaluation decides
+            # there's a promotion
+            train_model_name_after = state.train_model_name
+            model_paths = glob.glob(os.path.join(fsdb.models_dir(), '{}.*'.format(train_model_name_before)))
+            for model in model_paths:
+              logging.info('moving {} --> {}'.format(model,
+                train_model_name_after.join(model.rsplit(train_model_name_before, 1))))
+              shutil.copy(model, train_model_name_after.join(model.rsplit(train_model_name_before, 1)))
+
+    if big_chunk_remaining > 0:
+      window_size += FLAGS.golden_chunk_split - 1
+      big_chunk_remaining -= 1
     else:
-      # Run eval, validation & selfplay sequentially.
-      model_win_rate = wait(evaluate_trained_model(state))
-      wait(validate(state, holdout_glob))
-      wait(selfplay(state))
-
-    if model_win_rate >= FLAGS.gating_win_rate:
-      # Promote the trained model to the best model and increment the generation
-      # number.
-      state.best_model_name = state.train_model_name
-      state.gen_num += 1
+      window_size += FLAGS.golden_chunk_split
 
+  # after the main loop, if parallel_post_train = 2
+  # needs to print epoch_stop for last epoch
+  if FLAGS.parallel_post_train == 2:
+    mll.epoch_stop(state.iter_num - 1)
 
 def main(unused_argv):
   """Run the reinforcement learning loop."""
 
+  mll.init_start()
   print('Wiping dir %s' % FLAGS.base_dir, flush=True)
   shutil.rmtree(FLAGS.base_dir, ignore_errors=True)
   dirs = [fsdb.models_dir(), fsdb.selfplay_dir(), fsdb.holdout_dir(),
-          fsdb.eval_dir(), fsdb.golden_chunk_dir(), fsdb.working_dir()]
+          fsdb.eval_dir(), fsdb.golden_chunk_dir(), fsdb.working_dir(),
+          fsdb.mpi_log_dir()]
   for d in dirs:
     ensure_dir_exists(d);
 
@@ -440,8 +769,14 @@ def main(unused_argv):
   for handler in logging.getLogger().handlers:
     handler.setFormatter(formatter)
 
+  logging.info('Selfplay nodes = {}'.format(FLAGS.selfplay_node))
+  logging.info('Train nodes = {}'.format(FLAGS.train_node))
+  logging.info('Eval nodes = {}'.format(FLAGS.eval_node))
+
   with logged_timer('Total time'):
     try:
+      mll.init_stop()
+      mll.run_start()
       rl_loop()
     finally:
       asyncio.get_event_loop().close()
diff --git a/ml_perf/utils.py b/ml_perf/utils.py
index 8e6b7c6..bf30f75 100644
--- a/ml_perf/utils.py
+++ b/ml_perf/utils.py
@@ -20,18 +20,45 @@ sys.path.insert(0, '.')  # nopep8
 import asyncio
 import logging
 import os
+import multiprocessing
+import subprocess
+import fcntl
 
 from absl import flags
 from utils import *
 
 
 def expand_cmd_str(cmd):
-  return '  '.join(flags.FlagValues().read_flags_from_files(cmd))
+  result = ' '.join(flags.FlagValues().read_flags_from_files(cmd))
+  if cmd[0] == 'mpiexec' or cmd[0] == 'mpirun':
+    result = ' \\\n-host '.join(result.split(' -host '))
+  # avoid buffer too big to block I/O
+  return result[:8192]
 
 
 def get_cmd_name(cmd):
   if cmd[0] == 'python' or cmd[0] == 'python3':
     path = cmd[1]
+    for index in range(len(cmd)):
+      if cmd[index] == 'bazel-bin/cc/selfplay':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/eval':
+        path = cmd[index]
+        break
+  elif cmd[0] == 'mpirun' or cmd[0] == 'mpiexec':
+    for index in range(len(cmd)):
+      if cmd[index] == 'train.py':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/selfplay':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/eval':
+        path = cmd[index]
+        break
+      if cmd[index] == 'python' or cmd[index] == 'python3':
+        path = cmd[index+1]
   else:
     path = cmd[0]
   return os.path.splitext(os.path.basename(path))[0]
@@ -73,6 +100,127 @@ async def checked_run(*cmd):
 
     return stdout
 
+async def checked_run_distributed(genvs, num_instance, hosts, proclists, numa_nodes, seed, log_path, *cmd):
+  mpi_cmd = ['mpiexec',
+             '-outfile-pattern',
+             '{}/out-{}-{}-%r.txt'.format(log_path, get_cmd_name(cmd), seed)]
+  for genv in genvs:
+    mpi_cmd = mpi_cmd + ['-genv', genv]
+  num_nodes = len(hosts)
+  instance_per_node = num_instance // num_nodes
+  instance_remaining = num_instance - num_nodes * instance_per_node
+  for index in range(num_nodes):
+    if index < instance_remaining:
+      instance_to_launch = instance_per_node + 1
+    else:
+      instance_to_launch = instance_per_node
+
+    if index > 0:
+      mpi_cmd = mpi_cmd + [':']
+    mpi_cmd = mpi_cmd + ['-host', hosts[index]]
+
+    if proclists != None:
+      mpi_cmd = mpi_cmd + ['-env', 'KMP_AFFINITY=granularity=fine,compact,1,{}'.format(proclists[index])]
+
+    if numa_nodes != None:
+      mpi_cmd = mpi_cmd + ['numactl', '-l', '-N', numa_nodes[index]]
+
+    if num_instance > 1:
+      mpi_cmd = mpi_cmd + ['python3', 'ml_perf/execute.py',
+                           '--num_instance={}'.format(instance_to_launch),
+                           '--']
+    mpi_cmd = mpi_cmd + [*cmd]
+
+    if seed != None:
+      # ensure different seed for different node
+      mpi_cmd = mpi_cmd + ['--seed={}'.format(seed + index*1023779831)]
+
+  result = await checked_run(*mpi_cmd)
+  for index in range(num_nodes):
+    filename = '{}/out-{}-{}-{}.txt'.format(log_path, get_cmd_name(cmd), seed,
+                                            index)
+    outfile = open(filename, 'r')
+    result += outfile.read()
+    outfile.close()
+  return result
+
+def checked_run_mi(num_instance, *cmd):
+  name = get_cmd_name(cmd)
+  logging.debug('Running %s*%d: %s', name, num_instance, expand_cmd_str(cmd))
+  num_parallel_instance = int(multiprocessing.cpu_count())
+  procs=[None]*num_parallel_instance
+  results = [""]*num_parallel_instance
+  result_list = []
+
+  cur_instance = 0
+  # add new proc into procs
+  while cur_instance < num_instance or not all (
+      proc is None for proc in procs):
+    if None in procs and cur_instance < num_instance:
+      index = procs.index(None)
+      subproc_cmd = [
+              'OMP_NUM_THREADS=1',
+              'KMP_AFFINITY=granularity=fine,proclist=[{}],explicit'.format(
+                  ','.join(str(i) for i in list(range(
+                      index, index+1)))),
+              *cmd,
+              '--instance_id={}'.format(cur_instance),
+      ]
+      subproc_cmd = ' '.join(subproc_cmd)
+      if (cur_instance == 0):
+        logging.debug("subproc_cmd = {}".format(subproc_cmd))
+      procs[index] = subprocess.Popen(subproc_cmd, shell=True,
+                                      stdout=subprocess.PIPE,
+                                      stderr=subprocess.STDOUT)
+
+      proc_count = 0
+      for i in range(num_parallel_instance):
+        if procs[i] != None:
+          proc_count += 1
+      logging.debug('started instance {} in proc {}. proc count = {}'.format(
+          cur_instance, index, proc_count))
+
+      # change stdout of the process to non-blocking
+      # this is for collect output in a single thread
+      flags = fcntl.fcntl(procs[index].stdout, fcntl.F_GETFL)
+      fcntl.fcntl(procs[index].stdout, fcntl.F_SETFL, flags | os.O_NONBLOCK)
+
+      cur_instance += 1
+    for index in range(num_parallel_instance):
+      if procs[index] != None:
+        # collect proc output
+        while True:
+          try:
+            line = procs[index].stdout.readline()
+            if line == b'':
+              break
+            results[index] = results[index] + line.decode()
+          except IOError:
+            break
+
+        ret_val = procs[index].poll()
+        if ret_val == None:
+          continue
+        elif ret_val != 0:
+          logging.info(results[index])
+          raise RuntimeError(
+            'Non-zero return code (%d) executing %s' % (
+                ret_val, subproc_cmd))
+
+        if index == 0:
+          logging.debug(results[index])
+        result_list.append(results[index])
+        results[index] = ""
+        procs[index] = None
+
+        proc_count = 0
+        for i in range(num_parallel_instance):
+          if procs[i] != None:
+            proc_count += 1
+        logging.debug('proc {} finished. proc count = {}'.format(
+            index, proc_count))
+    time.sleep(0.001)  # avoid busy loop
+  return result_list
 
 def wait(aws):
   """Waits for all of the awaitable objects (e.g. coroutines) in aws to finish.
diff --git a/preprocessing.py b/preprocessing.py
index 595db38..d5a99a6 100644
--- a/preprocessing.py
+++ b/preprocessing.py
@@ -26,6 +26,9 @@ import symmetries
 import numpy as np
 import tensorflow as tf
 
+import horovod.tensorflow as hvd
+from tensorflow.python.data.experimental.ops import optimization
+
 TF_RECORD_CONFIG = tf.python_io.TFRecordOptions(
     tf.python_io.TFRecordCompressionType.ZLIB)
 
@@ -84,11 +87,11 @@ def batch_parse_tf_example(batch_size, example_batch):
         'outcome': tf.FixedLenFeature([], tf.float32),
     }
     parsed = tf.parse_example(example_batch, features)
-    x = tf.decode_raw(parsed['x'], tf.uint8)
+    x = tf.io.decode_raw(parsed['x'], tf.uint8)
     x = tf.cast(x, tf.float32)
     x = tf.reshape(x, [batch_size, go.N, go.N,
                        features_lib.NEW_FEATURES_PLANES])
-    pi = tf.decode_raw(parsed['pi'], tf.float32)
+    pi = tf.io.decode_raw(parsed['pi'], tf.float32)
     pi = tf.reshape(pi, [batch_size, go.N * go.N + 1])
     outcome = parsed['outcome']
     outcome.set_shape([batch_size])
@@ -98,7 +101,7 @@ def batch_parse_tf_example(batch_size, example_batch):
 def read_tf_records(batch_size, tf_records, num_repeats=1,
                     shuffle_records=True, shuffle_examples=True,
                     shuffle_buffer_size=None, interleave=True,
-                    filter_amount=1.0):
+                    filter_amount=1.0, dist_train=False, seed = 0):
     """
     Args:
         batch_size: batch size to return
@@ -116,6 +119,9 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
         raise ValueError("Must set shuffle buffer size if shuffling examples")
 
     tf_records = list(tf_records)
+
+    random.seed(seed)
+
     if shuffle_records:
         random.shuffle(tf_records)
     record_list = tf.data.Dataset.from_tensor_slices(tf_records)
@@ -126,20 +132,29 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
         buffer_size=8 * 1024 * 1024,
         compression_type='ZLIB')
 
+    if dist_train:
+        # no need to interleave in data parallelism
+        interleave = False
+
     if interleave:
         # cycle_length = how many tfrecord files are read in parallel
         # The idea is to shuffle both the order of the files being read,
         # and the examples being read from the files.
-        dataset = record_list.apply(tf.contrib.data.parallel_interleave(
+        dataset = record_list.apply(tf.data.experimental.parallel_interleave(
             map_func, cycle_length=64, sloppy=True))
     else:
         dataset = record_list.flat_map(map_func)
 
     if filter_amount < 1.0:
         dataset = dataset.filter(
-            lambda _: tf.random_uniform([]) < filter_amount)
+            lambda _: tf.random.uniform([], seed=seed) < filter_amount)
+        dataset = dataset.apply(optimization.optimize(["filter_with_random_uniform_fusion"]))
+
+    if dist_train:
+        dataset = dataset.shard(hvd.size(), hvd.rank())
 
     dataset = dataset.repeat(num_repeats)
+
     if shuffle_examples:
         dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)
 
@@ -181,7 +196,8 @@ def _random_rotation_pure_tf(x_tensor, outcome_tensor):
 def get_input_tensors(batch_size, tf_records, num_repeats=1,
                       shuffle_records=True, shuffle_examples=True,
                       shuffle_buffer_size=None,
-                      filter_amount=0.05, random_rotation=True):
+                      filter_amount=0.05, random_rotation=True,
+                      dist_train=False, seed = 0, make_one_shot = False):
     """Read tf.Records and prepare them for ingestion by dual_net.
 
     See `read_tf_records` for parameter documentation.
@@ -197,18 +213,22 @@ def get_input_tensors(batch_size, tf_records, num_repeats=1,
         shuffle_examples=shuffle_examples,
         shuffle_buffer_size=shuffle_buffer_size,
         filter_amount=filter_amount,
-        interleave=True)
+        interleave=True,
+        dist_train=dist_train, seed=seed)
     dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
     dataset = dataset.map(
         functools.partial(batch_parse_tf_example, batch_size))
     if random_rotation:
         dataset = dataset.map(_random_rotation_pyfunc)
 
-    return dataset.make_one_shot_iterator().get_next()
-
+    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
+    if make_one_shot:
+      return dataset.make_one_shot_iterator().get_next()
+    else:
+      return dataset
 
 def get_tpu_input_tensors(batch_size, tf_records, num_repeats=1,
-                          filter_amount=1, random_rotation=True):
+                          filter_amount=1, random_rotation=True, seed=0):
     # TPUs trains on sequential golden chunks to simplify preprocessing and
     # reproducibility.
     assert len(tf_records) < 101, "Use example_buffer to build a golden_chunk"
@@ -221,7 +241,7 @@ def get_tpu_input_tensors(batch_size, tf_records, num_repeats=1,
         shuffle_examples=False,
         shuffle_buffer_size=None,
         filter_amount=filter_amount,
-        interleave=False)
+        interleave=False, seed=seed)
     dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
     dataset = dataset.map(
         functools.partial(batch_parse_tf_example, batch_size))
diff --git a/produce_min_max_log.py b/produce_min_max_log.py
new file mode 100644
index 0000000..493ce38
--- /dev/null
+++ b/produce_min_max_log.py
@@ -0,0 +1,94 @@
+# Copyright 2019 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#!/usr/bin/env python
+# encoding: utf-8
+
+import time
+import os
+
+import tensorflow as tf
+from tensorflow.core.framework import graph_pb2
+from tensorflow.python.platform import gfile
+
+from absl import app, flags
+
+import preprocessing
+import dual_net
+
+
+flags.DEFINE_string('input_graph', None, 'The path of input graph.')
+flags.DEFINE_string('data_location', None, 'The path of input data.')
+flags.DEFINE_integer('num_steps', 20, 'Number of eval steps.')
+flags.DEFINE_integer('batch_size', 20, 'eval batch size.')
+flags.DEFINE_boolean('random_rotation', True, 'Do random rotation if true.')
+
+
+FLAGS = flags.FLAGS
+
+def run_graph(graph, tf_records):
+
+  data_graph = tf.Graph()
+  with data_graph.as_default():
+    features, labels = preprocessing.get_input_tensors(
+              FLAGS.batch_size,
+              tf_records,
+              shuffle_buffer_size=100000000,
+              random_rotation=FLAGS.random_rotation, seed=2,
+              dist_train=False, make_one_shot=True)
+
+  infer_graph = tf.Graph()
+  with infer_graph.as_default():
+    tf.import_graph_def(graph, name='')
+
+  input_tensor = dual_net.get_input_tensor(infer_graph)
+  output_tensor = dual_net.get_output_tensor(infer_graph)
+
+  config = tf.ConfigProto(
+                intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                inter_op_parallelism_threads=FLAGS.num_inter_threads)
+  data_sess = tf.Session(graph=data_graph, config=config)
+  infer_sess = tf.Session(graph=infer_graph, config=config)
+
+  elapsed = 0
+  #with tf.contrib.tfprof.ProfileContext('/home/letiank/skx-8180/train_dir/minigo', trace_steps=range(70, 80), dump_steps=[110]):
+  for it in range(FLAGS.num_steps):
+    features_np = data_sess.run(features)
+    start_time = time.time()
+    infer_sess.run(output_tensor, feed_dict={input_tensor: features_np})
+    elapsed += time.time() - start_time
+
+def read_graph(input_graph):
+  if not gfile.Exists(input_graph):
+    print("Input graph file '" + input_graph + "' does not exist!")
+    exit(-1)
+
+  input_graph_def = graph_pb2.GraphDef()
+  with gfile.Open(input_graph, "rb") as f:
+    data = f.read()
+    input_graph_def.ParseFromString(data)
+
+  return input_graph_def
+
+
+def main(unused_argv):
+  """Run the reinforcement learning loop."""
+
+  graph = read_graph(FLAGS.input_graph)
+  tf_records = sorted(tf.gfile.Glob(FLAGS.data_location), reverse=True)[:1]
+  print(tf_records)
+  run_graph(graph, tf_records)
+
+if __name__ == "__main__":
+    app.run(main)
diff --git a/requirements-colab.txt b/requirements-colab.txt
index febb463..f24b44d 100644
--- a/requirements-colab.txt
+++ b/requirements-colab.txt
@@ -9,7 +9,6 @@ google.cloud.bigtable
 #keras
 #numpy>=1.14.0
 #protobuf
-pylint
 sgf==0.5
 #six
 #tqdm>=4.17
diff --git a/requirements.txt b/requirements.txt
index dcb70ae..86cdf9c 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -7,7 +7,6 @@ grpcio-tools
 keras
 numpy>=1.14.0
 protobuf
-pylint
 sgf==0.5
 six
 tqdm>=4.17
diff --git a/rl_loop/example_buffer.py b/rl_loop/example_buffer.py
index 14c7cef..28c77fd 100644
--- a/rl_loop/example_buffer.py
+++ b/rl_loop/example_buffer.py
@@ -92,9 +92,15 @@ class ExampleBuffer():
         if len(games) > max_games:
             games = games[-max_games:]
 
-        with mp.Pool(threads) as pool:
-            res = tqdm(pool.imap(self.func, games), total=len(games))
-            self.examples.extend(itertools.chain.from_iterable(res))
+        if threads > 1:
+          with mp.Pool(threads) as pool:
+              res = tqdm(pool.imap(self.func, games), total=len(games))
+              self.examples.extend(itertools.chain.from_iterable(res))
+        else:
+          res = []
+          for game in games:
+            res += [self.func(game)]
+          self.examples.extend(itertools.chain.from_iterable(res))
         print("Got", len(self.examples), "examples")
 
     def update(self, new_games):
diff --git a/rl_loop/fsdb.py b/rl_loop/fsdb.py
index ab9d107..442692c 100644
--- a/rl_loop/fsdb.py
+++ b/rl_loop/fsdb.py
@@ -62,6 +62,7 @@ models_dir = _with_base('models')
 selfplay_dir = _with_base('data', 'selfplay')
 holdout_dir = _with_base('data', 'holdout')
 sgf_dir = _with_base('sgf')
+mpi_log_dir = _with_base('mpi')
 eval_dir = _with_base('sgf', 'eval')
 golden_chunk_dir = _with_base('data', 'golden_chunks')
 flags_path = _with_base('flags.txt')
diff --git a/run.sh b/run.sh
new file mode 100755
index 0000000..7cc74e7
--- /dev/null
+++ b/run.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+echo Physical cores = $PHY_CORES
+echo Virtual cores = $VIRT_CORES
+echo NUMA cores = $NUMA_CORES
+
+export KMP_HW_SUBSET=2T
+echo KMP_HW_SUBSET = $KMP_HW_SUBSET
+
+output_dir=${SCRATCH:-$(pwd)}
+echo Output to ${output_dir}
+
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY=compact,granularity=fine
+export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/cc/tensorflow
+ulimit -u 760000
+
+export PYTHONPATH=$(pwd)/ml_perf/tools/tensorflow_quantization/quantization:$PYTHONPATH
+
+./run_minigo.sh ${output_dir}/results/$(hostname) ml_perf/flags/9 $1
diff --git a/run_minigo.sh b/run_minigo.sh
new file mode 100755
index 0000000..d3cfab1
--- /dev/null
+++ b/run_minigo.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+BASE_DIR=$1
+FLAG_DIR=$2
+
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+# Run training loop
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$3 \
+  --train_node=localhost
+
+# Once the training loop has finished, run model evaluation to find the
+# first trained model that's better than the target
+BOARD_SIZE=9  python3  ml_perf/eval_models.py \
+  --base_dir=$BASE_DIR \
+  --flags_dir=$FLAG_DIR
diff --git a/run_minigo_mn.sh b/run_minigo_mn.sh
new file mode 100755
index 0000000..44df2ea
--- /dev/null
+++ b/run_minigo_mn.sh
@@ -0,0 +1,31 @@
+#!/bin/bash
+BASE_DIR=$1
+FLAG_DIR=$2
+
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+NUM_NODES=`ml_perf/hostlist.sh|wc -l`
+TRAIN_NODES=$3
+PLAY_NODES=$(expr $NUM_NODES - $TRAIN_NODES)
+EVAL_NODES=$PLAY_NODES
+
+# Run training loop
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$4 \
+  `ml_perf/hostlist.sh |head -n $PLAY_NODES |awk '/./{print "--selfplay_node="$0}'` \
+  `ml_perf/hostlist.sh |tail -n $TRAIN_NODES|awk '/./{print "--train_node="$0}'` \
+  `ml_perf/hostlist.sh |head -n $EVAL_NODES |awk '/./{print "--eval_node="$0}'`
+
+# Once the training loop has finished, run model evaluation to find the
+# first trained model that's better than the target
+BOARD_SIZE=9  python3  ml_perf/eval_models.py \
+  --base_dir=$BASE_DIR \
+  --flags_dir=$FLAG_DIR
diff --git a/run_mn.sh b/run_mn.sh
new file mode 100755
index 0000000..a72b116
--- /dev/null
+++ b/run_mn.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+echo Physical cores = $PHY_CORES
+echo Virtual cores = $VIRT_CORES
+echo NUMA cores = $NUMA_CORES
+
+export KMP_HW_SUBSET=2T
+echo KMP_HW_SUBSET = $KMP_HW_SUBSET
+
+output_dir=${SCRATCH:-$(pwd)}
+echo Output to ${output_dir}
+
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY=compact,granularity=fine
+export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/cc/tensorflow
+ulimit -u 760000
+
+export PYTHONPATH=./ml_perf/tools/tensorflow_quantization/quantization:$PYTHONPATH
+
+./run_minigo_mn.sh ${output_dir}/results/$(hostname) ml_perf/flags/9.mn $1 $2
diff --git a/set_avx2_build b/set_avx2_build
new file mode 100755
index 0000000..b60a540
--- /dev/null
+++ b/set_avx2_build
@@ -0,0 +1,61 @@
+#This file exports the bazel build opts for AVX2 platforms (broadwell and haswell). By setting -march=haswell and -mtune=broadwell, the binary will run on systems haswell and newer, but will be tuned for broadwell.
+
+MIN_GCC_MAJOR_VERSION=5
+MIN_GCC_MINOR_VERSION=3
+MIN_GCC_REVISION=0
+GCC_VERSION_STR=$(gcc -dumpversion)
+echo "GCC Version: ${GCC_VERSION_STR}"
+IFS='.' read -r -a GCC_VERSION <<< ${GCC_VERSION_STR}
+
+if [ "${GCC_VERSION[0]}" -lt "${MIN_GCC_MAJOR_VERSION}" ] ;
+then
+  echo "Your MAJOR version of GCC is too old: ${GCC_VERSION_STR}; it must be at least ${MIN_GCC_MAJOR_VERSION}.${MIN_GCC_MINOR_VERSION}.${MIN_GCC_REVISION}"
+  return 1
+
+elif [ "${GCC_VERSION[0]}" -eq "${MIN_GCC_MAJOR_VERSION}" ] ;
+then
+    if [ "${GCC_VERSION[1]}" -lt "${MIN_GCC_MINOR_VERSION}" ] ;
+    then
+      echo "Your MINOR version of GCC is too old: ${GCC_VERSION_STR}; it must be at least ${MIN_GCC_MAJOR_VERSION}.${MIN_GCC_MINOR_VERSION}."
+      return 1
+    fi
+fi
+
+echo "GCC ${GCC_VERSION_STR}: OK"
+
+#Don't use the C++11 ABI; use the old one 
+#These two options should be equivalent to all the options commented out below
+BAZEL_BUILD_OPTS_BASIC="--cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 \
+        --copt=-march=haswell \
+        --copt=-mtune=broadwell \
+                --copt=-O3"
+BAZEL_SECURE_BUILD_OPTS="--copt=-Wformat \
+                --copt=-Wformat-security \
+                --copt=-fstack-protector \
+                --copt=-fPIC \
+                --copt=-fpic \
+                --linkopt=-znoexecstack \
+                --linkopt=-zrelro \
+                --linkopt=-znow \
+                --linkopt=-fstack-protector \
+                --linkopt=-pie"
+
+#basic build flags
+echo "exporting BAZEL_BUILD_OPTS_BASIC=${BAZEL_BUILD_OPTS_BASIC}"
+export BAZEL_BUILD_OPTS_BASIC="${BAZEL_BUILD_OPTS_BASIC}"
+
+#secure build flags
+BAZEL_BUILD_OPTS="${BAZEL_BUILD_OPTS_BASIC} ${BAZEL_SECURE_BUILD_OPTS}"
+echo "exporting BAZEL_BUILD_OPTS=${BAZEL_BUILD_OPTS}"
+export BAZEL_BUILD_OPTS="${BAZEL_BUILD_OPTS}"
+
+#basic mkl flags
+BAZEL_MKL_BUILD_OPTS_BASIC="--config=mkl ${BAZEL_BUILD_OPTS_BASIC}"
+echo "exporting BAZEL_MKL_BUILD_OPTS_BASIC=${BAZEL_MKL_BUILD_OPTS_BASIC}"
+export BAZEL_MKL_BUILD_OPTS_BASIC="${BAZEL_MKL_BUILD_OPTS_BASIC}"
+
+#secure mkl flags
+BAZEL_SECURE_MKL_BUILD_OPTS="--config=mkl ${BAZEL_BUILD_OPTS}"
+echo "exporting BAZEL_SECURE_MKL_BUILD_OPTS=${BAZEL_SECURE_MKL_BUILD_OPTS}"
+export BAZEL_SECURE_MKL_BUILD_OPTS="${BAZEL_SECURE_MKL_BUILD_OPTS}"
+
diff --git a/test.sh b/test.sh
index 5a2d7fa..45d4672 100755
--- a/test.sh
+++ b/test.sh
@@ -25,11 +25,6 @@
 cd "$(dirname "$0")"
 
 lint_fail=0
-python3 -m pylint *.py || {
-  lint_fail=1
-  echo >&2 "--------------------------------------"
-  echo >&2 "Py linting did not pass successfully!"
-}
 
 PYTHONPATH= BOARD_SIZE=9 python3 tests/run_tests.py || {
   echo >&2 "--------------------------------------"
diff --git a/train.py b/train.py
index d6b24bc..2554826 100644
--- a/train.py
+++ b/train.py
@@ -29,6 +29,9 @@ import dual_net
 import preprocessing
 import utils
 
+import ml_perf.mlp_log as mll
+import horovod.tensorflow as hvd
+
 # See www.moderndescartes.com/essays/shuffle_viz for discussion on sizing
 flags.DEFINE_integer('shuffle_buffer_size', 2000,
                      'Size of buffer used to shuffle train examples.')
@@ -77,6 +80,8 @@ flags.declare_key_flag('work_dir')
 flags.declare_key_flag('train_batch_size')
 flags.declare_key_flag('num_tpu_cores')
 flags.declare_key_flag('use_tpu')
+flags.declare_key_flag('dist_train')
+flags.declare_key_flag('training_seed')
 
 FLAGS = flags.FLAGS
 
@@ -145,6 +150,8 @@ def train(*tf_records: "Records to train on"):
     estimator = dual_net.get_estimator()
 
     effective_batch_size = FLAGS.train_batch_size
+    if FLAGS.dist_train:
+        effective_batch_size = int(FLAGS.train_batch_size/hvd.size())
     if FLAGS.use_tpu:
         effective_batch_size *= FLAGS.num_tpu_cores
 
@@ -172,14 +179,17 @@ def train(*tf_records: "Records to train on"):
     else:
         def _input_fn():
             return preprocessing.get_input_tensors(
-                FLAGS.train_batch_size,
+                effective_batch_size,
                 tf_records,
                 filter_amount=FLAGS.filter_amount,
                 shuffle_buffer_size=FLAGS.shuffle_buffer_size,
-                random_rotation=True)
+                random_rotation=True, seed=FLAGS.training_seed,
+                dist_train=FLAGS.dist_train)
 
         hooks = [UpdateRatioSessionHook(FLAGS.work_dir),
                  EchoStepCounterHook(output_dir=FLAGS.work_dir)]
+        if FLAGS.dist_train:
+            hooks.append(hvd.BroadcastGlobalVariablesHook(0))
 
     steps = FLAGS.steps_to_train
     logging.info("Training, steps = %s, batch = %s -> %s examples",
@@ -209,18 +219,25 @@ def train(*tf_records: "Records to train on"):
 
 def main(argv):
     """Train on examples and export the updated model weights."""
+    if FLAGS.dist_train:
+        hvd.init()
+    mll.global_batch_size(FLAGS.train_batch_size)
+    mll.lr_rates(FLAGS.lr_rates)
+    mll.lr_boundaries(FLAGS.lr_boundaries)
     tf_records = argv[1:]
     logging.info("Training on %s records: %s to %s",
                  len(tf_records), tf_records[0], tf_records[-1])
     with utils.logged_timer("Training"):
         train(*tf_records)
-    if FLAGS.export_path:
-        dual_net.export_model(FLAGS.export_path)
-    if FLAGS.freeze:
-        if FLAGS.use_tpu:
-            dual_net.freeze_graph_tpu(FLAGS.export_path)
-        else:
-            dual_net.freeze_graph(FLAGS.export_path)
+
+    if(not FLAGS.dist_train) or hvd.rank()==0:
+        if FLAGS.export_path:
+            dual_net.export_model(FLAGS.export_path)
+        if FLAGS.freeze:
+            if FLAGS.use_tpu:
+                dual_net.freeze_graph_tpu(FLAGS.export_path)
+            else:
+                dual_net.freeze_graph(FLAGS.export_path)
 
 
 if __name__ == "__main__":
