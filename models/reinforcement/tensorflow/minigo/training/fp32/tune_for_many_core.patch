diff --git a/ml_perf/execute.py b/ml_perf/execute.py
index 00a6bed..37cf205 100644
--- a/ml_perf/execute.py
+++ b/ml_perf/execute.py
@@ -28,7 +28,9 @@ from ml_perf.utils import *
 
 from absl import app, flags
 
-flags.DEFINE_integer('num_instance', 1, 'Number of instances for selfplay')
+flags.DEFINE_integer('num_instance', 1, 'Number of instances')
+flags.DEFINE_integer('start_core', None, 'The first core number used to run instances')
+flags.DEFINE_integer('num_cores', None, 'The number of cores dedicated to these instances')
 
 FLAGS = flags.FLAGS
 
@@ -47,7 +49,7 @@ async def do_execute_mi():
 
   if num_instance > 1:
     result_list = checked_run_mi(
-      num_instance,
+      num_instance, FLAGS.start_core, FLAGS.num_cores,
       *arg_list
     )
     for result in result_list:
diff --git a/ml_perf/flags/9/rl_loop.flags b/ml_perf/flags/9/rl_loop.flags
index 75b18ff..0a3b95c 100644
--- a/ml_perf/flags/9/rl_loop.flags
+++ b/ml_perf/flags/9/rl_loop.flags
@@ -8,3 +8,4 @@
 --window_size=10
 --engine=tf
 --train_instance_per_numa=2
+--eval_cores=32
diff --git a/ml_perf/reference_implementation.py b/ml_perf/reference_implementation.py
index 0a8910f..26ecd1e 100644
--- a/ml_perf/reference_implementation.py
+++ b/ml_perf/reference_implementation.py
@@ -79,6 +79,7 @@ flags.DEFINE_integer('physical_cores', None, 'The number of cores for each node.
 flags.DEFINE_integer('virtual_cores', None, 'The number of SMT for each node.')
 flags.DEFINE_integer('numa_cores', None, 'The number of core for each numa node.')
 flags.DEFINE_integer('train_instance_per_numa', 2, 'The number of instance for each numa node.')
+flags.DEFINE_integer('eval_cores', None, 'The number of cores for evaluation on single node, the rest cores are used for selfplay')
 
 flags.DEFINE_multi_string('train_node', [], 'The node:core list for training')
 flags.DEFINE_multi_string('eval_node', [], 'The node list for evaluation')
@@ -361,12 +362,13 @@ async def selfplay(state, flagfile='selfplay'):
   else:
     if FLAGS.selfplay_node == []:
       # run selfplay locally
-      lines = await run(
+      cmd = [
           'python3', 'ml_perf/execute.py',
-          '--num_instance={}'.format(num_instance),
-          '--',
-          *sp_cmd,
-          '--seed={}'.format(state.seed))
+          '--num_instance={}'.format(num_instance)]
+      if (FLAGS.eval_cores != None):
+          cmd += ['--start_core={}'.format(FLAGS.eval_cores), '--num_cores={}'.format(FLAGS.physical_cores-FLAGS.eval_cores)]
+      cmd += ['--', *sp_cmd, '--seed={}'.format(state.seed)]
+      lines = await run(*cmd)
     else:
       with logged_timer('selfplay mn'):
         # run one selfplay instance per host
@@ -540,12 +542,15 @@ async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed, flag
   else:
     if FLAGS.eval_node == []:
       # run eval locally
-      lines = await run(
+      cmd = [
           'python3', 'ml_perf/execute.py',
-          '--num_instance={}'.format(num_instance),
-          '--',
-          *eval_cmd,
-          '--seed={}'.format(seed))
+          '--num_instance={}'.format(num_instance)]
+      if (FLAGS.eval_cores != None):
+          cmd += ['--start_core=0', '--num_cores={}'.format(FLAGS.eval_cores)]
+
+      cmd += ['--', *eval_cmd, '--seed={}'.format(seed)]
+
+      lines = await run(*cmd)
     else:
       # run one selfplay instance per host
       lines = await run_distributed(
@@ -653,10 +658,20 @@ def rl_loop():
       if FLAGS.parallel_post_train == 0:
         state.iter_num += 1
         wait(train(state, tf_records))
-        post_train(state)
-        # Run eval, validation & selfplay sequentially.
-        wait(selfplay(state))
-        model_win_rate = wait(evaluate_trained_model(state))
+        if (FLAGS.eval_cores == None):
+          post_train(state)
+          # Run eval, validation & selfplay sequentially.
+          wait(selfplay(state))
+          model_win_rate = wait(evaluate_trained_model(state))
+        else:
+          # when FLAGS.eval_cores != None, we spare some cores for evaluation, run evaluation
+          # and selfplay in parallel.  We run post_train in parallel with selfplay as well
+          sp_handle = asyncio.gather(selfplay(state), return_exceptions=True)
+          post_train(state)
+          # Run eval, validation & selfplay sequentially.
+          model_win_rate = wait(evaluate_trained_model(state))
+          asyncio.get_event_loop().run_until_complete(sp_handle)
+
         if model_win_rate >= FLAGS.gating_win_rate:
           # Promote the trained model to the best model and increment the generation
           # number.
diff --git a/ml_perf/utils.py b/ml_perf/utils.py
index bf30f75..c17cee7 100644
--- a/ml_perf/utils.py
+++ b/ml_perf/utils.py
@@ -144,10 +144,22 @@ async def checked_run_distributed(genvs, num_instance, hosts, proclists, numa_no
     outfile.close()
   return result
 
-def checked_run_mi(num_instance, *cmd):
+def checked_run_mi(num_instance, start_core, num_cores, *cmd):
   name = get_cmd_name(cmd)
   logging.debug('Running %s*%d: %s', name, num_instance, expand_cmd_str(cmd))
-  num_parallel_instance = int(multiprocessing.cpu_count())
+
+  if (start_core == None):
+    start_core = 0
+  if (num_cores == None):
+    num_parallel_instance = int(multiprocessing.cpu_count() // 2)  # find core count, assume hyper threading
+    # find the smallest number of instance that have same number of batches to default one (core count)
+    num_batch = ((num_instance-1) // num_parallel_instance) + 1
+    num_parallel_instance = ((num_instance-1) // num_batch) + 1
+    # make the number of instances more divisible
+    num_parallel_instance = (((num_parallel_instance - 1) // 8) + 1) * 8
+  else:
+    num_parallel_instance = num_cores
+
   procs=[None]*num_parallel_instance
   results = [""]*num_parallel_instance
   result_list = []
@@ -162,7 +174,7 @@ def checked_run_mi(num_instance, *cmd):
               'OMP_NUM_THREADS=1',
               'KMP_AFFINITY=granularity=fine,proclist=[{}],explicit'.format(
                   ','.join(str(i) for i in list(range(
-                      index, index+1)))),
+                      start_core+index, start_core+index+1)))),
               *cmd,
               '--instance_id={}'.format(cur_instance),
       ]
diff --git a/run.sh b/run.sh
index 7cc74e7..62494ff 100755
--- a/run.sh
+++ b/run.sh
@@ -8,7 +8,7 @@ echo Physical cores = $PHY_CORES
 echo Virtual cores = $VIRT_CORES
 echo NUMA cores = $NUMA_CORES
 
-export KMP_HW_SUBSET=2T
+export KMP_HW_SUBSET=1T
 echo KMP_HW_SUBSET = $KMP_HW_SUBSET
 
 output_dir=${SCRATCH:-$(pwd)}
