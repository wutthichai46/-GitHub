diff --git a/ml_perf/divide_golden_chunk.py b/ml_perf/divide_golden_chunk.py
new file mode 100644
index 0000000..364da8d
--- /dev/null
+++ b/ml_perf/divide_golden_chunk.py
@@ -0,0 +1,63 @@
+# Here need some words
+
+import os
+import shutil
+import random
+import functools
+
+import numpy as np
+import tensorflow as tf
+import threading
+
+from mpi4py import MPI
+from absl import app, flags
+from rl_loop import example_buffer
+
+flags.DEFINE_string('read_path', '/tmp/minigo',
+                    'Path to the read origin data.')
+
+flags.DEFINE_string('write_path', '/tmp/minigo/output',
+                    'Path to the read origin data.')
+
+flags.DEFINE_integer('out_files_number', 2,
+                     'Num of files to produce.')
+
+flags.DEFINE_integer('physical_cores', 56,
+                     'Num of cores.')
+
+flags.DEFINE_integer('seed', 0,
+                     'Random seed.')
+
+FLAGS = flags.FLAGS
+
+
+def main(unused_argv):
+  mpi_comm = MPI.COMM_WORLD
+  mpi_rank = mpi_comm.Get_rank()
+  mpi_size = mpi_comm.Get_size()
+  # avoid seed out of range
+  random.seed(FLAGS.seed % 1048576)
+  tf.set_random_seed(FLAGS.seed % 1048576)
+  np.random.seed(FLAGS.seed % 1048576)
+
+  pattern = os.path.join(FLAGS.read_path, '*.zz')
+  files = tf.gfile.Glob(pattern)
+
+  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+  example_num = buffer.parallel_fill(files, threads=FLAGS.physical_cores)
+
+  # make sure all nodes generate same number of examples
+  example_num = int(mpi_comm.allreduce(example_num, op=MPI.MIN))
+
+  buffer.flush_new(FLAGS.write_path+'_{}'.format(mpi_rank), example_num, FLAGS.out_files_number, threads=1)
+
+  shutil.rmtree('/tmp/minigo/home', ignore_errors=True)
+
+if __name__ == '__main__':
+  app.run(main)
+
+
+
+
+
+
diff --git a/ml_perf/reference_implementation.py b/ml_perf/reference_implementation.py
index e04d873..1649a0a 100644
--- a/ml_perf/reference_implementation.py
+++ b/ml_perf/reference_implementation.py
@@ -34,6 +34,8 @@ import multiprocessing as mp
 from ml_perf.utils import *
 import ml_perf.mlp_log as mll
 
+from fractions import gcd
+
 from absl import app, flags
 from rl_loop import example_buffer, fsdb
 import dual_net
@@ -64,7 +66,7 @@ flags.DEFINE_string('flags_dir', None,
 
 flags.DEFINE_integer('window_size', 10,
                      'Maximum number of recent selfplay rounds to train on.')
-flags.DEFINE_integer('golden_chunk_split', 16,
+flags.DEFINE_integer('golden_chunk_split', 2,
                      'Golden chunk of each selfplay is splited to accelerate write golden chunk')
 
 flags.DEFINE_integer('parallel_post_train', 0,
@@ -164,7 +166,7 @@ class WinStats:
     self.white_wins = ColorWinStats(*raw_stats[4:])
     self.total_wins = self.black_wins.total + self.white_wins.total
 
-def initialize_from_checkpoint(state):
+def initialize_from_checkpoint(state, out_files_number):
   """Initialize the reinforcement learning loop from a checkpoint."""
   # The checkpoint's work_dir should contain the most recently trained model.
   model_paths = glob.glob(os.path.join(FLAGS.checkpoint_dir,
@@ -174,18 +176,20 @@ def initialize_from_checkpoint(state):
                        'got [{}]'.format(', '.join(model_paths)))
   start_model_path = model_paths[0]
 
-  # Copy the training chunks.
   golden_chunks_dir = os.path.join(FLAGS.checkpoint_dir, 'golden_chunks')
   for basename in os.listdir(golden_chunks_dir):
     path = os.path.join(golden_chunks_dir, basename)
-    shutil.copy(path, fsdb.golden_chunk_dir())
+    out_path = os.path.join(fsdb.golden_chunk_dir(), basename)
+    buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+    example_num = buffer.parallel_fill(tf.gfile.Glob(path),FLAGS.physical_cores)
+    buffer.flush_new(out_path, example_num, out_files_number, 1)# FLAGS.physical_cores)
 
   # Copy the latest trained model into the models directory and use it on the
   # first round of selfplay.
   state.best_model_name = 'checkpoint'
   best_model_path = os.path.join(fsdb.models_dir(), state.best_model_name)
 
-  dual_net.optimize_graph(start_model_path, best_model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz', FLAGS.eval_min_max_every_epoch)
+  dual_net.optimize_graph(start_model_path, best_model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz*', FLAGS.eval_min_max_every_epoch)
 
   # Copy the training files.
   work_dir = os.path.join(FLAGS.checkpoint_dir, 'work_dir')
@@ -194,7 +198,6 @@ def initialize_from_checkpoint(state):
     shutil.copy(path, fsdb.working_dir())
 
 
-
 def parse_win_stats_table(stats_str, num_lines):
   result = []
   lines = stats_str.split('\n')
@@ -322,9 +325,7 @@ def get_golden_chunk_records(window_size):
     A list of golden chunks up to num_records in length, sorted by path.
   """
 
-  pattern = os.path.join(fsdb.golden_chunk_dir(), '*.zz')
-  if window_size > FLAGS.golden_chunk_split * FLAGS.window_size:
-    window_size = FLAGS.golden_chunk_split * FLAGS.window_size
+  pattern = os.path.join(fsdb.golden_chunk_dir(), '*.zz*')
   return sorted(tf.gfile.Glob(pattern), reverse=True)[:window_size]
 
 
@@ -343,9 +344,9 @@ async def selfplay(state, flagfile='selfplay'):
     flagfile: the name of the flagfile to use for selfplay, either 'selfplay'
         (the default) or 'boostrap'.
   """
-
   output_dir = os.path.join(fsdb.selfplay_dir(), state.output_model_name)
   holdout_dir = os.path.join(fsdb.holdout_dir(), state.output_model_name)
+  output_dir = '/tmp/minigo' + output_dir
 
   multi_instance, num_instance, flag_list = extract_multi_instance(
       ['--flagfile={}_mi.flags'.format(os.path.join(FLAGS.flags_dir, flagfile))])
@@ -391,43 +392,34 @@ async def selfplay(state, flagfile='selfplay'):
 
   with logged_timer('generate golden chunk'):
     # Write examples to a single record.
-    pattern = os.path.join(output_dir, '*', '*.zz')
-    files = tf.gfile.Glob(pattern)
-
-    random.seed(state.seed)
-    tf.set_random_seed(state.seed)
-    np.random.seed(state.seed)
-
-    # TODO(tommadams): This method of generating one golden chunk per generation
-    # is sub-optimal because each chunk gets reused multiple times for training,
-    # introducing bias. Instead, a fresh dataset should be uniformly sampled out
-    # of *all* games in the training window before the start of each training run.
-
-    # TODO(tommadams): parallel_fill is currently non-deterministic. Make it not
-    # so.
-    logging.info('Writing golden chunk from "{}"'.format(pattern))
-    threads = FLAGS.golden_chunk_split
-    file_list = []
-    files_number = len(files)
-    chunk_size = files_number // threads
-
-    # split files into N seperate parts
-    for i in range(threads):
-      if i == threads - 1:
-        file_list += [[i, files[chunk_size * i :]]]
-      else:
-        file_list += [[i, files[chunk_size * i : chunk_size * (i + 1)]]]
-    pool = mp.Pool(threads)
-    pool.map(functools.partial(gen_golden_chunk, state=state), file_list)
+    hosts = FLAGS.selfplay_node
+    if hosts == []:
+      hosts = ['localhost']
+    num_instance = len(hosts)
+    numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+    train_instance_num = FLAGS.train_instance_per_numa * len(FLAGS.train_node) * numa_per_node
+    selfplay_node_num = len(hosts)
+    selfplay_num = selfplay_node_num
+    #out_files_number = (train_instance_num*selfplay_num/gcd(train_instance_num, selfplay_num))/selfplay_num
+    out_files_number = int(train_instance_num/gcd(train_instance_num, selfplay_num))
+
+    cmd = ['python3', 'ml_perf/divide_golden_chunk.py',
+        '--read_path={}'.format(output_dir + "/*"),
+        '--write_path={}'.format(os.path.join(fsdb.golden_chunk_dir(), state.output_model_name + '.tfrecord.zz')),
+        '--out_files_number={}'.format(out_files_number),
+        '--physical_cores={}'.format(FLAGS.physical_cores),
+        '--base_dir={}'.format(FLAGS.base_dir)]
+    lines = await run_distributed([], 1, hosts, None, None, state.seed, *cmd)
+
+    print(lines)
 
   return bias
 
-async def train(state, tf_records):
+async def train(state, window_size):
   """Run training and write a new model to the fsdb models_dir.
 
   Args:
     state: the RL loop State instance.
-    tf_records: a list of paths to TensorFlow records to train on.
   """
   train_node = FLAGS.train_node
   num_node = len(train_node)
@@ -451,10 +443,12 @@ async def train(state, tf_records):
     intra_threads = FLAGS.physical_cores
 
   model_path = os.path.join(fsdb.models_dir(), state.train_model_name)
-  cmd = ['python3', 'train.py', *tf_records,
+  cmd = ['python3', 'train.py',
       '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags')),
       '--work_dir={}'.format(fsdb.working_dir()),
       '--export_path={}'.format(model_path),
+      '--window_size={}'.format(window_size),
+      '--data_path={}'.format(fsdb.golden_chunk_dir()),
       '--training_seed={}'.format(state.seed),
       '--freeze=True',
       '--num_inter_threads=1',
@@ -486,7 +480,7 @@ async def train(state, tf_records):
 
 def post_train(state):
   model_path = os.path.join(fsdb.models_dir(), state.train_model_name)
-  dual_net.optimize_graph(model_path + '.pb', model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz', FLAGS.eval_min_max_every_epoch)
+  dual_net.optimize_graph(model_path + '.pb', model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz*', FLAGS.eval_min_max_every_epoch)
   mll.save_model(state.iter_num-1)
 
   # Append the time elapsed from when the RL was started to when this model
@@ -603,27 +597,38 @@ def rl_loop():
   # chunk left.  Until it reach FLAGS.window_size * FLAGS.golden_chunk_split
 
   window_size = 0
-  big_chunk_remaining = 0
 
   state = State()
+  numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+  train_instance_num = FLAGS.train_instance_per_numa * len(FLAGS.train_node) * numa_per_node
+  selfplay_node_num = max(len(FLAGS.selfplay_node), 1)
+  selfplay_num = selfplay_node_num
+  out_files_number = int(train_instance_num/gcd(train_instance_num, selfplay_num)*selfplay_node_num)
+  FLAGS.golden_chunk_split = out_files_number
+
+  window_size = out_files_number * FLAGS.window_size
 
   if FLAGS.checkpoint_dir != None:
     # Start from a partially trained model.
-    initialize_from_checkpoint(state)
-    window_size = len(get_golden_chunk_records(FLAGS.window_size))
-    big_chunk_remaining = window_size
+    initialize_from_checkpoint(state, out_files_number)
+    window_size = len(get_golden_chunk_records(window_size))
+    mll.init_stop()
+    mll.run_start()
+    state.start_time = time.time()
   else:
     # Play the first round of selfplay games with a fake model that returns
     # random noise. We do this instead of playing multiple games using a single
     # model bootstrapped with random noise to avoid any initial bias.
+    mll.init_stop()
+    mll.run_start()
+    state.start_time = time.time()
     mll.epoch_start(state.iter_num)
     wait(selfplay(state, 'bootstrap'))
     window_size += FLAGS.golden_chunk_split
 
     # Train a real model from the random selfplay games.
-    tf_records = get_golden_chunk_records(window_size)
     state.iter_num += 1
-    wait(train(state, tf_records))
+    wait(train(state, window_size))
     post_train(state)
 
     # Select the newly trained model as the best.
@@ -647,12 +652,9 @@ def rl_loop():
       holdout_glob = os.path.join(fsdb.holdout_dir(), '%06d-*' % state.iter_num,
                                   '*')
 
-      # Train on shuffled game data from recent selfplay rounds.
-      tf_records = get_golden_chunk_records(window_size)
-
       if FLAGS.parallel_post_train == 0:
         state.iter_num += 1
-        wait(train(state, tf_records))
+        wait(train(state, window_size))
         post_train(state)
         # Run eval, validation & selfplay sequentially.
         wait(selfplay(state))
@@ -667,7 +669,7 @@ def rl_loop():
 
       if FLAGS.parallel_post_train == 1:
         state.iter_num += 1
-        wait([train(state, tf_records),
+        wait([train(state, window_size),
             selfplay(state)])
         post_train(state)
         # Run eval, validation & selfplay in parallel.
@@ -694,7 +696,7 @@ def rl_loop():
         # |   start selfplay[iter]
         # |   wait selfplay
         # wait train
-        train_handle = asyncio.gather(train(state, tf_records), return_exceptions=True)
+        train_handle = asyncio.gather(train(state, window_size), return_exceptions=True)
         if not first_iter:
           post_train(state_copy)
           model_win_rate = wait(evaluate_trained_model(state_copy))
@@ -730,12 +732,6 @@ def rl_loop():
                 train_model_name_after.join(model.rsplit(train_model_name_before, 1))))
               shutil.copy(model, train_model_name_after.join(model.rsplit(train_model_name_before, 1)))
 
-    if big_chunk_remaining > 0:
-      window_size += FLAGS.golden_chunk_split - 1
-      big_chunk_remaining -= 1
-    else:
-      window_size += FLAGS.golden_chunk_split
-
   # after the main loop, if parallel_post_train = 2
   # needs to print epoch_stop for last epoch
   if FLAGS.parallel_post_train == 2:
@@ -775,8 +771,6 @@ def main(unused_argv):
 
   with logged_timer('Total time'):
     try:
-      mll.init_stop()
-      mll.run_start()
       rl_loop()
     finally:
       asyncio.get_event_loop().close()
diff --git a/preprocessing.py b/preprocessing.py
index d5a99a6..af35b00 100644
--- a/preprocessing.py
+++ b/preprocessing.py
@@ -122,26 +122,26 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
 
     random.seed(seed)
 
-    if shuffle_records:
-        random.shuffle(tf_records)
+    #if shuffle_records:
+    #    random.shuffle(tf_records)
+
     record_list = tf.data.Dataset.from_tensor_slices(tf_records)
 
+    if dist_train:
+        record_list = record_list.shard(hvd.size(), hvd.rank())
+
     # compression_type here must agree with write_tf_examples
     map_func = functools.partial(
         tf.data.TFRecordDataset,
         buffer_size=8 * 1024 * 1024,
         compression_type='ZLIB')
 
-    if dist_train:
-        # no need to interleave in data parallelism
-        interleave = False
-
     if interleave:
         # cycle_length = how many tfrecord files are read in parallel
         # The idea is to shuffle both the order of the files being read,
         # and the examples being read from the files.
         dataset = record_list.apply(tf.data.experimental.parallel_interleave(
-            map_func, cycle_length=64, sloppy=True))
+            map_func, cycle_length=1000, sloppy=True))
     else:
         dataset = record_list.flat_map(map_func)
 
@@ -150,15 +150,15 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
             lambda _: tf.random.uniform([], seed=seed) < filter_amount)
         dataset = dataset.apply(optimization.optimize(["filter_with_random_uniform_fusion"]))
 
-    if dist_train:
-        dataset = dataset.shard(hvd.size(), hvd.rank())
+    #if dist_train:
+    #    dataset = dataset.shard(hvd.size(), hvd.rank())
 
     dataset = dataset.repeat(num_repeats)
 
     if shuffle_examples:
         dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)
 
-    dataset = dataset.batch(batch_size)
+    dataset = dataset.batch(batch_size, drop_remainder=True)
     return dataset
 
 
diff --git a/rl_loop/example_buffer.py b/rl_loop/example_buffer.py
index 28c77fd..40b467f 100644
--- a/rl_loop/example_buffer.py
+++ b/rl_loop/example_buffer.py
@@ -75,6 +75,10 @@ def file_timestamp(filename):
 def _ts_to_str(timestamp):
     return dt.datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
 
+def parallel_shuffle(i, example_list):
+    # random.shuffle on deque is O(n^2) convert to list for O(n)
+    random.shuffle(example_list)
+
 
 class ExampleBuffer():
     def __init__(self, max_size=2**21, sampling_frac=0.02):
@@ -92,16 +96,11 @@ class ExampleBuffer():
         if len(games) > max_games:
             games = games[-max_games:]
 
-        if threads > 1:
-          with mp.Pool(threads) as pool:
-              res = tqdm(pool.imap(self.func, games), total=len(games))
-              self.examples.extend(itertools.chain.from_iterable(res))
-        else:
-          res = []
-          for game in games:
-            res += [self.func(game)]
-          self.examples.extend(itertools.chain.from_iterable(res))
+        with mp.pool.ThreadPool(threads) as pool:
+            res = tqdm(pool.imap(self.func, games), total=len(games))
+            self.examples.extend(itertools.chain.from_iterable(res))
         print("Got", len(self.examples), "examples")
+        return len(self.examples)
 
     def update(self, new_games):
         """ new_games is a list of .tfrecord.zz new game records. """
@@ -132,6 +131,32 @@ class ExampleBuffer():
         self.examples.clear()
         self.examples = deque(maxlen=self.max_size)
 
+    def flush_new(self, path, example_num, num_out = 1, threads = 8):
+        # random.shuffle on deque is O(n^2) convert to list for O(n)
+        self.examples = list(self.examples)
+        example_list = [ex[1] for ex in self.examples]
+        length = example_num // num_out
+        example_list = example_list[:length*num_out]
+
+        i_list = []
+        for i in range(num_out):
+            i_list.append((i, example_list[i*length:(i+1)*length]))
+
+        with timer("Writing examples to " + path):
+            with mp.pool.ThreadPool(threads) as pool:
+                pool.starmap(parallel_shuffle, i_list)
+
+        i_list = []
+        for i in range(num_out):
+            i_list.append((path+'_'+str(i), example_list[i*length:(i+1)*length], False))
+
+        with timer("Writing examples to " + path):
+            with mp.pool.ThreadPool(num_out) as pool:
+                pool.starmap(preprocessing.write_tf_examples, i_list)
+
+        self.examples.clear()
+        self.examples = deque(maxlen=self.max_size)
+
     @property
     def count(self):
         return len(self.examples)
diff --git a/train.py b/train.py
index 2554826..cf2c8c9 100644
--- a/train.py
+++ b/train.py
@@ -19,6 +19,7 @@ Usage:
 """
 
 import logging
+import os
 
 from absl import app, flags
 import numpy as np
@@ -50,6 +51,9 @@ flags.DEFINE_float('filter_amount', 1.0,
 flags.DEFINE_string('export_path', None,
                     'Where to export the model after training.')
 
+flags.DEFINE_string('data_path', None,
+                    'Where to get the data for training.')
+
 flags.DEFINE_bool('use_bt', False,
                   'Whether to use Bigtable as input.  '
                   '(Only supported with --use_tpu, currently.)')
@@ -216,6 +220,11 @@ def train(*tf_records: "Records to train on"):
             games.require_fresh_games(0)
         raise
 
+def get_golden_chunk_records(base):
+  pattern = os.path.join(base, '*.zz*')
+  window_size = FLAGS.window_size
+  return sorted(tf.gfile.Glob(pattern), reverse=True)[:window_size]
+
 
 def main(argv):
     """Train on examples and export the updated model weights."""
@@ -224,7 +233,7 @@ def main(argv):
     mll.global_batch_size(FLAGS.train_batch_size)
     mll.lr_rates(FLAGS.lr_rates)
     mll.lr_boundaries(FLAGS.lr_boundaries)
-    tf_records = argv[1:]
+    tf_records = get_golden_chunk_records(FLAGS.data_path)
     logging.info("Training on %s records: %s to %s",
                  len(tf_records), tf_records[0], tf_records[-1])
     with utils.logged_timer("Training"):
