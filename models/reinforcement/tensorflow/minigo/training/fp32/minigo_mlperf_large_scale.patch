diff --git a/WORKSPACE b/WORKSPACE
index cabf233..1977a70 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -12,11 +12,8 @@ http_archive(
 
 http_archive(
     name = "com_google_absl",
-    strip_prefix = "abseil-cpp-666fc1266bccfd8e6eaaa084e7b42580bb8eb199",
-    urls = [
-        "http://mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/666fc1266bccfd8e6eaaa084e7b42580bb8eb199.tar.gz",
-        "https://github.com/abseil/abseil-cpp/archive/666fc1266bccfd8e6eaaa084e7b42580bb8eb199.tar.gz",
-    ],
+    strip_prefix = "abseil-cpp-93dfcf74cb5fccae3da07897d8613ae6cab958a0",
+    urls = ["https://github.com/abseil/abseil-cpp/archive/93dfcf74cb5fccae3da07897d8613ae6cab958a0.tar.gz"],
 )
 
 http_archive(
diff --git a/build.sh b/build.sh
new file mode 100755
index 0000000..1f682cc
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,4 @@
+#!/bin/bash
+. ./set_avx2_build
+bazel build  --incompatible_remove_native_http_archive=false -c opt  --verbose_failures --define=tf=1  --define=board_size=9 $BAZEL_BUILD_OPTS cc:selfplay cc:eval
+
diff --git a/cc/configure_tensorflow.sh b/cc/configure_tensorflow.sh
index 8b39178..2ea9789 100755
--- a/cc/configure_tensorflow.sh
+++ b/cc/configure_tensorflow.sh
@@ -4,109 +4,100 @@ set -e
 
 script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
 dst_dir="${script_dir}/tensorflow"
-tmp_dir="/tmp/minigo_tf"
-tmp_pkg_dir="/tmp/tensorflow_pkg"
+tmp_dir="${script_dir}/minigo_tf"
+tmp_pkg_dir="${script_dir}/tensorflow_pkg"
 
-rm -rfd ${tmp_dir}
 rm -rfd ${tmp_pkg_dir}
-mkdir -p ${tmp_dir}
 
 rm -rf ${dst_dir}/*
 mkdir -p ${dst_dir}
 
+if [ -d "${script_dir}/ml_perf/tools" ]; then
+  echo "Intel AI tools exist."
+else
+  git clone https://github.com/IntelAI/tools.git ${script_dir}/ml_perf/tools/
+fi
+
 # TODO(tommadams): we should probably switch to Clang at some point.
-commit_tag="v1.11.0"
 
-echo "Cloning tensorflow to ${tmp_dir}"
-git clone https://github.com/tensorflow/tensorflow "${tmp_dir}"
+if [ -d "${tmp_dir}" ]; then
+  pushd "${tmp_dir}"
+else
+  echo "Cloning tensorflow to ${tmp_dir}"
+  git clone https://github.com/tensorflow/tensorflow "${tmp_dir}"
+  cp cc/tf_int8_fusion.patch "${tmp_dir}"
+
+  pushd "${tmp_dir}"
 
-pushd "${tmp_dir}"
+  cherry_pick_tag="02c111ab4269ab73a506164e4b54ba996d28a8cf"
+  prev_tag="8be9158c7a701d933bbe532f5d54df17f47a4284"
 
-echo "Checking out ${commit_tag}"
-git checkout "${commit_tag}"
+  git diff "${prev_tag}" "${cherry_pick_tag}" > sample.patch
+
+  commit_tag="961bb02b882a8bb921e5be1c09c34b51fffd25dc"
+  echo "Checking out ${commit_tag}"
+  git checkout "${commit_tag}"
+  git apply sample.patch
+  git apply tf_int8_fusion.patch
+fi
 
 # Run the TensorFlow configuration script, setting reasonable values for most
 # of the options.
 echo "Configuring tensorflow"
 cc_opt_flags="${CC_OPT_FLAGS:--march=native}"
 
+PYTHON_BIN_PATH=`which python`
+
 CC_OPT_FLAGS="${cc_opt_flags}" \
-TF_NEED_JEMALLOC=${TF_NEED_JEMALLOC:-1} \
-TF_NEED_GCP=${TF_NEED_GCP:-1} \
+PYTHON_BIN_PATH=${PYTHON_BIN_PATH} \
+USE_DEFAULT_PYTHON_LIB_PATH="${USE_DEFAULT_PYTHON_LIB_PATH:-1}" \
+TF_NEED_JEMALLOC=${TF_NEED_JEMALLOC:-0} \
+TF_NEED_GCP=${TF_NEED_GCP:-0} \
 TF_NEED_HDFS=${TF_NEED_HDFS:-0} \
 TF_NEED_S3=${TF_NEED_S3:-0} \
 TF_NEED_KAFKA=${TF_NEED_KAFKA:-0} \
-TF_NEED_CUDA=${TF_NEED_CUDA:-1} \
+TF_NEED_CUDA=${TF_NEED_CUDA:-0} \
 TF_NEED_GDR=${TF_NEED_GDR:-0} \
 TF_NEED_VERBS=${TF_NEED_VERBS:-0} \
 TF_NEED_OPENCL_SYCL=${TF_NEED_OPENCL_SYCL:-0} \
+TF_NEED_ROCM=${TF_NEED_ROCM:-0} \
 TF_CUDA_CLANG=${TF_CUDA_CLANG:-0} \
+TF_DOWNLOAD_CLANG=${TF_DOWNLOAD_CLANG:-0} \
 TF_NEED_TENSORRT=${TF_NEED_TENSORRT:-0} \
 TF_NEED_MPI=${TF_NEED_MPI:-0} \
 TF_SET_ANDROID_WORKSPACE=${TF_SET_ANDROID_WORKSPACE:-0} \
 TF_NCCL_VERSION=${TF_NCCL_VERSION:-1.3} \
+TF_ENABLE_XLA=${TF_ENABLE_XLA:-0} \
 ./configure
 
+. ${script_dir}/../set_avx2_build
+BAZEL_OPTS="-c opt --config=mkl \
+            --action_env=PATH \
+            --action_env=LD_LIBRARY_PATH \
+            $BAZEL_BUILD_OPTS \
+            --copt=-DINTEL_MKLDNN"
 echo "Building tensorflow package"
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" //tensorflow/tools/pip_package:build_pip_package
+bazel build -s $BAZEL_OPTS //tensorflow/tools/pip_package:build_pip_package
 bazel-bin/tensorflow/tools/pip_package/build_pip_package ${tmp_pkg_dir}
 
 echo "Tensorflow built-ish"
 echo "Unpacking tensorflow package..."
 unzip -q ${tmp_pkg_dir}/tensorflow-*.whl -d ${tmp_dir}
 
+
 echo "Copying tensor flow headers to ${dst_dir}"
 cp -r ${tmp_dir}/tensorflow-*.data/purelib/tensorflow/include/* "${dst_dir}"
-
 echo "Building tensorflow libraries"
 
-# Add a custom BUILD target for the gRPC runtime.
-# TODO(tommadams): Remove this once the gRPC runtime is linked in to TensorFlow.
-cat <<EOF >> tensorflow/BUILD
-
-tf_cc_shared_object(
-    name = "libgrpc_runtime.so",
-    linkopts = select({
-        "//tensorflow:darwin": [
-            "-Wl,-exported_symbols_list",  # This line must be directly followed by the exported_symbols.lds file
-            "\$(location //tensorflow:tf_exported_symbols.lds)",
-        ],
-        "//tensorflow:windows": [],
-        "//conditions:default": [
-            "-z defs",
-            "-Wl,--version-script",  #  This line must be directly followed by the version_script.lds file
-            "\$(location //tensorflow:tf_version_script.lds)",
-        ],
-    }),
-    deps = [
-        "//tensorflow:tf_exported_symbols.lds",
-        "//tensorflow:tf_version_script.lds",
-       "//tensorflow/core/distributed_runtime/rpc:grpc_runtime",
-    ]
-)
-EOF
-
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" \
-    //tensorflow:libgrpc_runtime.so \
+bazel build -s $BAZEL_OPTS \
     //tensorflow:libtensorflow_cc.so \
     //tensorflow:libtensorflow_framework.so
 
 echo "Copying tensorflow libraries to ${dst_dir}"
-cp bazel-bin/tensorflow/{libgrpc_runtime,libtensorflow_*}.so "${dst_dir}"
-
-echo "Building toco"
-bazel build -c opt --config=opt --copt="${cc_opt_flags}" //tensorflow/contrib/lite/toco:toco
-cp bazel-bin/tensorflow/contrib/lite/toco/toco "${dst_dir}"
-
-echo "Building TF Lite"
-
-./tensorflow/contrib/lite/tools/make/download_dependencies.sh
-make -j $(nproc) -f tensorflow/contrib/lite/tools/make/Makefile
-cp tensorflow/contrib/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a $dst_dir/libtensorflow_lite.a
-for dir in contrib/lite contrib/lite/kernels contrib/lite/profiling contrib/lite/schema; do
-  mkdir -p $dst_dir/tensorflow/$dir
-  cp tensorflow/$dir/*.h $dst_dir/tensorflow/$dir/
-done
-cp -r tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include/flatbuffers $dst_dir/
+cp bazel-bin/tensorflow/libtensorflow_*.so "${dst_dir}"
+cp bazel-bin/tensorflow/libtensorflow_*.so.1 "${dst_dir}"
+
+cp `find ${tmp_dir} |grep libiomp5.so` ${dst_dir}
+cp `find ${tmp_dir} |grep libmklml_intel.so` ${dst_dir}
 
 popd
diff --git a/cc/dual_net/tf_dual_net.cc b/cc/dual_net/tf_dual_net.cc
index a400cc2..3bee107 100644
--- a/cc/dual_net/tf_dual_net.cc
+++ b/cc/dual_net/tf_dual_net.cc
@@ -58,6 +58,9 @@ class TfDualNet : public DualNet {
    public:
     TfWorker(const GraphDef& graph_def) : batch_capacity_(0) {
       SessionOptions options;
+      options.config.set_intra_op_parallelism_threads(1);
+      options.config.set_inter_op_parallelism_threads(0);
+      options.config.set_use_per_session_threads(false);
       options.config.mutable_gpu_options()->set_allow_growth(true);
       session_.reset(NewSession(options));
       TF_CHECK_OK(session_->Create(graph_def));
diff --git a/cc/eval.cc b/cc/eval.cc
index bde9011..525c840 100644
--- a/cc/eval.cc
+++ b/cc/eval.cc
@@ -68,6 +68,7 @@ DEFINE_string(model, "",
               "engine=lite, the model should be .tflite flatbuffer.");
 DEFINE_string(model_two, "", "Descriptor for the second model");
 DEFINE_int32(parallel_games, 32, "Number of games to play in parallel.");
+DEFINE_int32(instance_id, 0, "Unique id with multi-instance.");
 
 // Output flags.
 DEFINE_string(output_bigtable, "",
@@ -170,7 +171,10 @@ class Evaluator {
     ParseOptionsFromFlags(&game_options_, &player_options_);
 
     int num_games = FLAGS_parallel_games;
-    for (int thread_id = 0; thread_id < num_games; ++thread_id) {
+    int instance_id = FLAGS_instance_id;
+    int thread_id_begin = instance_id*num_games;
+    for (int thread_id = thread_id_begin;
+             thread_id < thread_id_begin+num_games; ++thread_id) {
       bool swap_models = (thread_id & 1) != 0;
       threads_.emplace_back(std::bind(&Evaluator::ThreadRun, this, thread_id,
                                       swap_models ? &model_b : &model_a,
diff --git a/cc/selfplay.cc b/cc/selfplay.cc
index a3d4d9e..9d3cfc0 100644
--- a/cc/selfplay.cc
+++ b/cc/selfplay.cc
@@ -119,6 +119,7 @@ DEFINE_int32(parallel_games, 32, "Number of games to play in parallel.");
 DEFINE_int32(num_games, 0,
              "Total number of games to play. Defaults to parallel_games. "
              "Only one of num_games and run_forever must be set.");
+DEFINE_int32(instance_id, 0, "Unique id with multi-instance.");
 
 // Output flags.
 DEFINE_string(output_dir, "",
@@ -244,7 +245,10 @@ class SelfPlayer {
       batcher_ =
           absl::make_unique<BatchingDualNetFactory>(std::move(model_factory));
     }
-    for (int i = 0; i < FLAGS_parallel_games; ++i) {
+    int instance_id = FLAGS_instance_id;
+    int thread_id_begin = instance_id * FLAGS_parallel_games;
+    for (int i = thread_id_begin;
+             i < thread_id_begin+FLAGS_parallel_games; ++i) {
       threads_.emplace_back(std::bind(&SelfPlayer::ThreadRun, this, i));
     }
     for (auto& t : threads_) {
diff --git a/cc/tf_int8_fusion.patch b/cc/tf_int8_fusion.patch
new file mode 100644
index 0000000..4d1b8e3
--- /dev/null
+++ b/cc/tf_int8_fusion.patch
@@ -0,0 +1,422 @@
+diff --git a/tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc b/tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc
+index 5aa2dd4..c860473 100644
+--- a/tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc
++++ b/tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc
+@@ -1,11 +1,8 @@
+ /* Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+-
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+-
+     http://www.apache.org/licenses/LICENSE-2.0
+-
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+@@ -36,23 +33,28 @@ Status FuseQuantizedConvolutionAndRequantize(
+     GraphDef* output_graph_def) {
+   std::map<string, const NodeDef*> node_map;
+   MapNamesToNodes(input_graph_def, &node_map);
++  bool is_perchannel = false;
+   GraphDef replaced_graph_def;
+   TF_RETURN_IF_ERROR(ReplaceMatchingOpTypes(
+       input_graph_def,  // clang-format off
+ 
+-      {"Requantize",
++      {"Requantize|RequantizePerChannel",
+         {
+           {"QuantizedConv2D|QuantizedConv2DWithBias|QuantizedConv2DWithRelu|"
+-            "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu"},
++            "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu|"
++            "QuantizedDepthwiseConv2DWithBiasAndRelu"},
+           {"QuantizedConv2D|QuantizedConv2DWithBias|QuantizedConv2DWithRelu|"
+-           "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu"},
++           "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu|"
++           "QuantizedDepthwiseConv2DWithBiasAndRelu"},
+           {"QuantizedConv2D|QuantizedConv2DWithBias|QuantizedConv2DWithRelu|"
+-           "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu"},
++           "QuantizedConv2DWithBiasAndRelu|QuantizedConv2DWithBiasSumAndRelu|"
++           "QuantizedDepthwiseConv2DWithBiasAndRelu"},
+           {"Const"},
+           {"Const"}
+         }
+       },  // clang-format on */
+-      [&node_map](const NodeMatch& match, const std::set<string>& input_nodes,
++      [&node_map, &is_perchannel](const NodeMatch& match,
++         const std::set<string>& input_nodes,
+          const std::set<string>& output_nodes,
+          std::vector<NodeDef>* new_nodes) {
+         // TODO(mdfaijul/sheng): Current implementation assumed all
+@@ -61,13 +63,14 @@ Status FuseQuantizedConvolutionAndRequantize(
+ 
+         // Find all the nodes we expect in the subgraph.
+         const NodeDef& requantize_node = match.node;
+-        CHECK_EQ("Requantize", requantize_node.op());
+         const NodeDef& quantized_conv2D_node = match.inputs[0].node;
+         const NodeDef& const_requantize_range_min_node = match.inputs[3].node;
+         CHECK_EQ("Const", const_requantize_range_min_node.op());
+         const NodeDef& const_requantize_range_max_node = match.inputs[4].node;
+         CHECK_EQ("Const", const_requantize_range_max_node.op());
+ 
++        is_perchannel = ("RequantizePerChannel" == requantize_node.op());
++
+         string quantized_conv2D_op_name = quantized_conv2D_node.op();
+         // Set up the new fused version of the convolution op.
+         NodeDef fused_conv;
+@@ -78,47 +81,177 @@ Status FuseQuantizedConvolutionAndRequantize(
+                 "QuantizedConv2DWithBiasSumAndRelu") == 0)
+           n_input -= 1;  // -1 since summand is moved after frozen min-max
+ 
+-        for (int i=0; i < n_input; i++)
+-          AddNodeInput(quantized_conv2D_node.input(i), &fused_conv);
+-
++        string control_input;
++        string current_input;
++        for (int i=0; i < n_input; i++) {
++          current_input = quantized_conv2D_node.input(i);
++          if (current_input.length() > 0 && current_input[0] == '^') {
++            control_input = current_input;
++          } else {
++            AddNodeInput(current_input, &fused_conv);
++          }
++        }
+         AddNodeInput(const_requantize_range_min_node.name(), &fused_conv);
+         AddNodeInput(const_requantize_range_max_node.name(), &fused_conv);
+ 
+-        // Add additional inputs to
+-        // QuantizedConv2DWithBiasSumAndReluAndRequantize
++        // Ensure QuantizedConv2DWithBiasSumAndReluAndRequantize receives
++        // integer summand. Because requantization fusion is registered
++        // for integer summand only.
+         if (quantized_conv2D_op_name.compare(
+               "QuantizedConv2DWithBiasSumAndRelu") == 0) {
+-          const NodeDef *in_requantize = node_map[node_map[
+-              quantized_conv2D_node.input(n_input)]->input(0)];
+-          string summand(in_requantize->name());
+-          string min_summand(in_requantize->name() + ":1");
+-          string max_summand(in_requantize->name() + ":2");
++          const NodeDef *summand_node = node_map[quantized_conv2D_node.input(
++            n_input)];
++          NodeDef* new_summand_node = nullptr;
++          NodeDef quantize_node;
++          if (summand_node->op() != "Dequantize") {
++            // Quantizing the summand.
++            // Add some common constants we need for reshaping inputs.
++            NodeDef reshape_dims;
++            reshape_dims.set_op("Const");
++            reshape_dims.set_name(summand_node->name() + "/reshape_dims");
++            SetNodeAttr("dtype", DT_INT32, &reshape_dims);
++            Tensor reshape_dims_tensor(DT_INT32, {1});
++            reshape_dims_tensor.flat<int32>()(0) = -1;
++            SetNodeTensorAttr<int32>(
++              "value", reshape_dims_tensor, &reshape_dims);
++            AddNodeInput("^" + summand_node->name(), &reshape_dims);
++
++            NodeDef reduction_dims;
++            reduction_dims.set_op("Const");
++            reduction_dims.set_name(summand_node->name() + "/reduction_dims");
++            SetNodeAttr("dtype", DT_INT32, &reduction_dims);
++            Tensor reduction_dims_tensor(DT_INT32, {1});
++            reduction_dims_tensor.flat<int32>()(0) = 0;
++            SetNodeTensorAttr<int32>("value", reduction_dims_tensor,
++                                    &reduction_dims);
++            AddNodeInput("^" + summand_node->name(), &reduction_dims);
++
++            NodeDef reshape_node;
++            reshape_node.set_op("Reshape");
++            reshape_node.set_name(summand_node->name() + "/reshape");
++            SetNodeAttr("T", DT_FLOAT, &reshape_node);
++
++            NodeDef min_node;
++            min_node.set_op("Min");
++            min_node.set_name(summand_node->name() + "/min");
++            SetNodeAttr("T", DT_FLOAT, &min_node);
++            SetNodeAttr("keep_dims", false, &min_node);
++            AddNodeInput(reshape_node.name(), &min_node);
++            AddNodeInput(reduction_dims.name(), &min_node);
++
++            NodeDef max_node;
++            max_node.set_op("Max");
++            max_node.set_name(summand_node->name() + "/max");
++            SetNodeAttr("T", DT_FLOAT, &max_node);
++            SetNodeAttr("keep_dims", false, &max_node);
++            AddNodeInput(reshape_node.name(), &max_node);
++            AddNodeInput(reduction_dims.name(), &max_node);
++
++            // NodeDef quantize_node;
++            quantize_node.set_op("QuantizeV2");
++            quantize_node.set_name(summand_node->name() + "/quantize");
++            // Decide data type of quantize op
++            std::vector<string> relu_ops = {
++                "Relu",
++                "Relu6"
++                };
++            bool is_relu = std::find(relu_ops.begin(), relu_ops.end(),
++                          summand_node->op()) != relu_ops.end();
++            if (is_relu)
++              SetNodeAttr("T", DT_QUINT8, &quantize_node);
++            else
++              SetNodeAttr("T", DT_QINT8, &quantize_node);
++            SetNodeAttr("mode", "SCALED", &quantize_node);
++
++            AddNodeInput(summand_node->name(), &reshape_node);
++            AddNodeInput(reshape_dims.name(), &reshape_node);
++
++            AddNodeInput(summand_node->name(), &quantize_node);
++            AddNodeInput(min_node.name(), &quantize_node);
++            AddNodeInput(max_node.name(), &quantize_node);
++
++            new_nodes->push_back(reshape_dims);
++            new_nodes->push_back(reduction_dims);
++            new_nodes->push_back(reshape_node);
++            new_nodes->push_back(min_node);
++            new_nodes->push_back(max_node);
++            new_nodes->push_back(quantize_node);
++            // Set the new summand node for fused_conv
++            new_summand_node = &quantize_node;
++          } else {
++            // If summand node is "Dequantize" then either "QuantizeV2" or
++            // "Requantize{PerChannel}" is feeding Dequantize op. Set new_summand_node
++            // as the input of summand node.
++            new_summand_node = const_cast<NodeDef*>(node_map[
++                  summand_node->input(0)]);
++          }
++          string summand(new_summand_node->name());
++          string min_summand(new_summand_node->name() + ":1");
++          string max_summand(new_summand_node->name() + ":2");
+           AddNodeInput(summand, &fused_conv);
+           AddNodeInput(min_summand, &fused_conv);
+           AddNodeInput(max_summand, &fused_conv);
+ 
+-          // Signed version QuantizedConv2DWithBiasSumAndReluAndRequantize
+-          // if Relu does not follow the convolution operation
+-          std::vector<string> signed_ops = {
+-              "QuantizedConv2DWithBias",
+-              "QuantizedConv2D"
+-              };
+-          bool is_signed_summand =
++          DataType summand_type;
++          // New summand node should be QuantizeV2 or
++          // Requantize{PerChannel}
++          if (new_summand_node->op() == "QuantizeV2") {
++            TF_RETURN_IF_ERROR(GetNodeAttr(*new_summand_node,
++                                           "T", &summand_type));
++          } else if (new_summand_node->op() == "RequantizePerChannel") {
++            TF_RETURN_IF_ERROR(GetNodeAttr(*new_summand_node,
++                                           "out_type", &summand_type));
++          } else if (new_summand_node->op() == "Requantize") {
++            // Requantize op is Eigen kernel that does non-SCALED quantization
++            // and always maps into quint8. However, for MKLDNN fusion, which is
++            // SCALED quantization, the summand fused requantize op may have
++            // qint8 or quint8 as its output type. Therefore, it is needed to set
++            // the summand_type correctly.
++            std::vector<string> signed_ops = {
++                "QuantizedConv2DWithBias",
++                "QuantizedConv2D"
++                };
++            bool is_signed_summand =
+               std::find(signed_ops.begin(), signed_ops.end(),
+-              node_map[in_requantize->input(0)]->op()) != signed_ops.end();
+-          if (is_signed_summand) {
+-            fused_conv.set_op(
+-                "QuantizedConv2DWithBiasSignedSumAndReluAndRequantize");
+-            SetNodeAttr("Tsummand", DT_QINT8, &fused_conv);
++                  node_map[new_summand_node->input(0)]->op()) != signed_ops.end();
++            summand_type = is_signed_summand ? DT_QINT8 : DT_QUINT8;
+           } else {
+-            SetNodeAttr("Tsummand", DT_QUINT8, &fused_conv);
++            return Status(error::Code::FAILED_PRECONDITION,
++                               "Fusion is not supported, a fix is required.");
+           }
++          SetNodeAttr("Tsummand", summand_type, &fused_conv);
++          // Decide whether signed version of
++          // QuantizedConv2DWithBiasSumAndReluAndRequantize or not
++          if (summand_type == DT_QINT8)
++            fused_conv.set_op(
++                "QuantizedConv2DWithBiasSignedSumAndReluAndRequantize");
+         }
+-        CopyNodeAttr(quantized_conv2D_node, "Tinput", "Tinput", &fused_conv);
++
++        // Add control input to the very end of the input list
++        // of the newly fused op
++        if (control_input.length() > 0)
++          AddNodeInput(control_input, &fused_conv);
++
++        CopyNodeAttr(quantized_conv2D_node, "Tinput", "Tinput",   &fused_conv);
+         CopyNodeAttr(quantized_conv2D_node, "Tfilter", "Tfilter", &fused_conv);
+         CopyNodeAttr(quantized_conv2D_node, "strides", "strides", &fused_conv);
+         CopyNodeAttr(quantized_conv2D_node, "padding", "padding", &fused_conv);
+ 
++        std::vector<std::string> fused_quantized_bias_ops = {
++          "QuantizedConv2DWithBias",
++          "QuantizedConv2DWithBiasAndRelu",
++          "QuantizedDepthwiseConv2DWithBias",
++          "QuantizedDepthwiseConv2DWithBiasAndRelu",
++          "QuantizedConv2DWithBiasSumAndRelu",
++        };
++        if (std::find(fused_quantized_bias_ops.begin(),
++            fused_quantized_bias_ops.end(),
++            quantized_conv2D_node.op()) != fused_quantized_bias_ops.end()) {
++          SetNodeAttr("Tbias", DT_FLOAT, &fused_conv);
++        }
++        if (HasNodeAttr(quantized_conv2D_node, "padding_list"))
++          CopyNodeAttr(quantized_conv2D_node, "padding_list",
++                       "padding_list",     &fused_conv);
+         // Copy dilation attribute if exsit in the orginal node
+         if (HasNodeAttr(quantized_conv2D_node, "dilations"))
+           CopyNodeAttr(quantized_conv2D_node, "dilations",
+@@ -135,30 +268,53 @@ Status FuseQuantizedConvolutionAndRequantize(
+         return Status::OK();
+       },
+       {}, &replaced_graph_def));
+-
+-  // Convert bias float -> int32 on replaced_graph_def
+-  std::vector<string> fused_requantized_bias_ops = {
++  
++  // After Requantize op fusion, fix attributes for nodes in the graph, if threre is
++  // some discrepency. And also quantize the bias (float -> int32)
++  // List of requantize fused ops that have biases. 
++  std::vector<std::string> fused_requantized_bias_ops = {
+       "QuantizedConv2DWithBiasAndRequantize",
+       "QuantizedConv2DWithBiasAndReluAndRequantize",
+       "QuantizedConv2DWithBiasSumAndReluAndRequantize",
+-      "QuantizedConv2DWithBiasSignedSumAndReluAndRequantize"
+-      };
++      "QuantizedConv2DWithBiasSignedSumAndReluAndRequantize",
++      "QuantizedDepthwiseConv2DWithBiasAndRequantize",
++      "QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize"
++  };
++
+   node_map.clear();
+   MapNamesToNodes(replaced_graph_def, &node_map);
+   for (auto& node_pair : node_map) {
+     const NodeDef *node = node_pair.second;
++    // An workaround to fix attributes of "Dequantize" op with non-perchannel
++    // quantization. "Dequantize" node should accept DT_QINT8 if the input node
++    // is "QuantizedConv2DAndRequantize" or 
++    // "QuantizedConv2DWithBiasAndRequantize".
++    if (str_util::StartsWith(node->op(), "Dequantize")) {
++      std::string input_node_op =
++          node_map[NodeNameFromInput(node->input(0))]->op();
++      if (str_util::StartsWith(input_node_op,
++             "QuantizedConv2DAndRequantize") ||
++          str_util::StartsWith(input_node_op,
++             "QuantizedConv2DWithBiasAndRequantize")) {
++        SetNodeAttr("T", DT_QINT8, const_cast<NodeDef*>(node));
++        SetNodeAttr("mode", "SCALED", const_cast<NodeDef*>(node));
++      }
++      continue;
++    }
++    // Quantize bias to int32 if input min-max values are constants.
++    // This is guaranteed if the preceeding op is a fused requantize op.
+     bool is_fused_requantized_conv_op =
+-        std::find(fused_requantized_bias_ops.begin(),
+-                  fused_requantized_bias_ops.end(),
+-                  node->op()) != fused_requantized_bias_ops.end();
++    std::find(fused_requantized_bias_ops.begin(),
++              fused_requantized_bias_ops.end(), node->op())
++        != fused_requantized_bias_ops.end();
+     if (is_fused_requantized_conv_op) {
+-      // If the op is not fed by Another Requantize op,
+-      // then we coonvert bias as Int32
+-      string input_op = node_map[NodeNameFromInput(node->input(0))]->op();
+-      if (str_util::StartsWith(input_op, "QuantizedConv2D") &&
+-          str_util::EndsWith(input_op, "AndRequantize")) {
++      std::string preceeding_op = node_map[NodeNameFromInput(
++                  node->input(0))]->op();
++      if (str_util::StartsWith(preceeding_op, "Quantized") &&
++          str_util::StrContains(preceeding_op, "Conv2D") &&
++          str_util::EndsWith(preceeding_op, "AndRequantize")) {
+         NodeDef *bias_node = const_cast<NodeDef*>(node_map[NodeNameFromInput(
+-            node->input(2))]);
++          node->input(2))]);
+         const NodeDef *min_input_node = node_map[NodeNameFromInput(
+             node_map[node->input(0)]->input(7))];
+         const NodeDef *max_input_node = node_map[NodeNameFromInput(
+@@ -171,41 +327,50 @@ Status FuseQuantizedConvolutionAndRequantize(
+             GetNodeTensorAttr(*min_input_node, "value").flat<float>()(0);
+         const float max_input =
+             GetNodeTensorAttr(*max_input_node, "value").flat<float>()(0);
+-        const float min_filter =
+-            GetNodeTensorAttr(*min_filter_node, "value").flat<float>()(0);
+-        const float max_filter =
+-            GetNodeTensorAttr(*max_filter_node, "value").flat<float>()(0);
+-
+-        TensorProto float_tensor_proto = bias_node->attr().at("value").tensor();
+-        Tensor float_tensor;
+-        if(!float_tensor.FromProto(float_tensor_proto)) {
+-          TF_RETURN_IF_ERROR(::tensorflow::errors::InvalidArgument(
+-              "TensorProto object is not valid."));
+-        }
+-        if (float_tensor.dtype() != DT_FLOAT) {
+-          TF_RETURN_IF_ERROR(::tensorflow::errors::Unimplemented(
+-              "Expected float tensor."));
+-        }
+-        float *p_bias_float = float_tensor.flat<float>().data();
++        const Tensor& min_filter_tensor = 
++            GetNodeTensorAttr(*min_filter_node, "value");
++        const Tensor& max_filter_tensor =
++            GetNodeTensorAttr(*max_filter_node, "value");
++        const float* min_filter = min_filter_tensor.flat<float>().data();
++        const float* max_filter = max_filter_tensor.flat<float>().data();
++        size_t num_scale_factors = min_filter_tensor.NumElements();
+ 
+-        Tensor int32_tensor = Tensor(DT_QINT32, float_tensor.shape());
+-        qint32 *p_bias_int32 = int32_tensor.flat<qint32>().data();
+-
+-        float bias_scale = 255.0 * 127.0 /
+-            (std::max(std::abs(max_input), std::abs(min_input)) *
+-            std::max(std::abs(max_filter), std::abs(min_filter)));
+-        int64 nelems = float_tensor.NumElements();
+-        for (int64 n = 0; n < nelems; n++)
+-          p_bias_int32[n] = (int32_t) (p_bias_float[n] * bias_scale);
++        TensorProto float_tensor_proto =
++            bias_node->attr().at("value").tensor();
++        Tensor float_bias_tensor;
++        CHECK(float_bias_tensor.FromProto(float_tensor_proto));
++        CHECK_EQ(float_bias_tensor.dtype(), DT_FLOAT);
++        float *float_bias = float_bias_tensor.flat<float>().data();
+ 
++        Tensor int32_bias_tensor = Tensor(DT_QINT32, float_bias_tensor.shape());
++        qint32 *int32_bias = int32_bias_tensor.flat<qint32>().data();
++        std::vector<float> scales(num_scale_factors);
++        for (size_t i = 0; i < num_scale_factors; ++i) {
++          scales[i] = 255.0 * 127.0 /
++              (std::max(std::abs(max_input), std::abs(min_input)) *
++              std::max(std::abs(max_filter[i]), std::abs(min_filter[i])));
++        } 
++        int64 bias_length = float_bias_tensor.NumElements();
++        if (num_scale_factors > 1) {
++          if (bias_length != num_scale_factors)
++            return Status(error::Code::FAILED_PRECONDITION,
++                          "Number of filter output channels is not"
++                          "equal to bias size");
++          else {
++            for (int64 i = 0; i < bias_length; i++)
++              int32_bias[i] = (int32_t) (float_bias[i] * scales[i]);
++          }
++        } else {
++          for (int64 i = 0; i < bias_length; i++)
++            int32_bias[i] = (int32_t) (float_bias[i] * scales[0]);
++        }
+         bias_node->clear_attr();
+         AttrValue attr_type;
+-        attr_type.set_type(int32_tensor.dtype());
++        attr_type.set_type(int32_bias_tensor.dtype());
+         bias_node->mutable_attr()->insert({"dtype", attr_type});
+-
+         AttrValue attr_tensor;
+         TensorProto* t = attr_tensor.mutable_tensor();
+-        int32_tensor.AsProtoTensorContent(t);
++        int32_bias_tensor.AsProtoTensorContent(t);
+         bias_node->mutable_attr()->insert({"value", attr_tensor});
+         SetNodeAttr("Tbias", DT_QINT32, const_cast<NodeDef*>(node));
+       } else {
+@@ -222,4 +387,4 @@ REGISTER_GRAPH_TRANSFORM("fuse_quantized_conv_and_requantize",
+ 
+ }  // namespace graph_transforms
+ }  // namespace tensorflow
+-#endif // INTEL_MKL
++#endif  // INTEL_MKL
+\ No newline at end of file
diff --git a/cluster/common.sh b/cluster/common.sh
old mode 100644
new mode 100755
diff --git a/cluster/minigui/run-local.sh b/cluster/minigui/run-local.sh
old mode 100755
new mode 100644
diff --git a/cluster/unset-common.sh b/cluster/unset-common.sh
old mode 100644
new mode 100755
diff --git a/cluster/utils.sh b/cluster/utils.sh
old mode 100644
new mode 100755
diff --git a/common.py b/common.py
new file mode 100644
index 0000000..37516cf
--- /dev/null
+++ b/common.py
@@ -0,0 +1,31 @@
+import os
+
+class Config():
+  def __init__(self, tf_root):
+    self.demo_dir = os.path.join(tf_root, 'demo')
+    self.demo_tmp_dir = os.path.join(tf_root, '../demo_tmp')
+
+    self.pb_dir = os.path.join(self.demo_dir, 'pb')
+    if not os.path.exists(self.pb_dir):
+      os.makedirs(self.pb_dir)
+    self.fp32_optimized_graph = os.path.join(self.pb_dir, 'freezed_resnet50_opt.pb')
+    self.int8_graph = os.path.join(self.pb_dir, 'int8_resnet50.pb')
+    self.int8_graph_logged = os.path.join(self.pb_dir, 'int8_resnet50_logged.pb')
+    self.int8_graph_freese = os.path.join(self.pb_dir, 'int8_resnet50_freese.pb')
+    self.int8_graph_final = os.path.join(self.pb_dir, 'int8_resnet50_final.pb')
+
+    self.accuracy_script = os.path.join(self.demo_dir, 'accuracy.py')
+    self.benchmark_script = os.path.join(self.demo_dir, 'benchmark.py')
+    self.quantize_script = os.path.join(self.demo_dir, 'quantize_graph.py')
+
+    self.min_max_log = os.path.join(self.demo_dir, 'min_max.log')
+
+
+  input_names = 'input'
+  output_names = 'predict'
+
+  def set_fp32_graph(self, pb):
+    self.fp32_original_graph = pb
+
+  def set_dataset(self, ds):
+    self.imagenet_data = ds
diff --git a/dual_net.py b/dual_net.py
index edf946d..36dd3b8 100644
--- a/dual_net.py
+++ b/dual_net.py
@@ -22,6 +22,7 @@ from absl import flags
 import functools
 import logging
 import os.path
+import shutil
 import time
 import numpy as np
 import random
@@ -36,6 +37,15 @@ import features as features_lib
 import go
 import symmetries
 
+import horovod.tensorflow as hvd
+
+from tensorflow.python.framework import dtypes
+from tensorflow.core.framework import graph_pb2
+from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference
+from tensorflow.tools.graph_transforms import TransformGraph
+from ml_perf.utils import *
+
+import quantize_graph
 
 flags.DEFINE_integer('train_batch_size', 256,
                      'Batch size to use for train/eval evaluation. For GPU '
@@ -120,6 +130,18 @@ flags.DEFINE_integer(
 flags.DEFINE_integer(
     'keep_checkpoint_max', default=5, help='Number of checkpoints to keep.')
 
+flags.DEFINE_integer(
+    'num_inter_threads', default=0,
+    help=('Number of inter threads.'))
+
+flags.DEFINE_integer(
+    'num_intra_threads', default=0,
+    help=('Number of intra threads.'))
+
+flags.DEFINE_bool(
+    'dist_train', default=False,
+    help=('Using distributed training or not.'))
+
 flags.DEFINE_bool(
     'use_random_symmetry', True,
     help='If true random symmetries be used when doing inference.')
@@ -157,7 +179,9 @@ class DualNetwork():
         self.save_file = save_file
         self.inference_input = None
         self.inference_output = None
-        config = tf.ConfigProto()
+        config = tf.ConfigProto(
+                      intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                      inter_op_parallelism_threads=FLAGS.num_inter_threads)
         config.gpu_options.allow_growth = True
         self.sess = tf.Session(graph=tf.Graph(), config=config)
         self.initialize_graph()
@@ -273,6 +297,8 @@ def model_fn(features, labels, mode, params):
 
     optimizer = tf.train.MomentumOptimizer(
         learning_rate, params['sgd_momentum'])
+    if(params['dist_train']):
+        optimizer = hvd.DistributedOptimizer(optimizer)
     if params['use_tpu']:
         optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)
     with tf.control_dependencies(update_ops):
@@ -376,6 +402,164 @@ def model_fn(features, labels, mode, params):
         return tpu_estimator_spec.as_estimator_spec()
 
 
+def model_fn_new(features, labels, mode, params):
+    """
+    Create the model for estimator api
+    Args:
+        features: tensor with shape
+            [BATCH_SIZE, go.N, go.N, features_lib.NEW_FEATURES_PLANES]
+        labels: dict from string to tensor with shape
+            'pi_tensor': [BATCH_SIZE, go.N * go.N + 1]
+            'value_tensor': [BATCH_SIZE]
+        mode: a tf.estimator.ModeKeys (batchnorm params update for TRAIN only)
+        params: A dictionary (Typically derived from the FLAGS object.)
+    Returns: tf.estimator.EstimatorSpec with props
+        mode: same as mode arg
+        predictions: dict of tensors
+            'policy': [BATCH_SIZE, go.N * go.N + 1]
+            'value': [BATCH_SIZE]
+        loss: a single value tensor
+        train_op: train op
+        eval_metric_ops
+    return dict of tensors
+        logits: [BATCH_SIZE, go.N * go.N + 1]
+    """
+
+    policy_output, value_output, logits = model_inference_fn(
+        features, mode == tf.estimator.ModeKeys.TRAIN, params)
+
+    # train ops
+    policy_cost = tf.reduce_mean(
+        tf.nn.softmax_cross_entropy_with_logits_v2(
+            logits=logits, labels=tf.stop_gradient(labels['pi_tensor'])))
+
+    value_cost = params['value_cost_weight'] * tf.reduce_mean(
+        tf.square(value_output - labels['value_tensor']))
+
+    reg_vars = [v for v in tf.trainable_variables()
+                if 'bias' not in v.name and 'beta' not in v.name]
+    l2_cost = params['l2_strength'] * \
+        tf.add_n([tf.nn.l2_loss(v) for v in reg_vars])
+
+    combined_cost = policy_cost + value_cost + l2_cost
+
+    global_step = tf.train.get_or_create_global_step()
+    learning_rate = tf.train.piecewise_constant(
+        global_step, params['lr_boundaries'], params['lr_rates'])
+    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+
+    # Insert quantization ops if requested
+    if params['quantize']:
+        if mode == tf.estimator.ModeKeys.TRAIN:
+            tf.contrib.quantize.create_training_graph(
+                quant_delay=params['quant_delay'])
+        else:
+            tf.contrib.quantize.create_eval_graph()
+
+    optimizer = tf.train.MomentumOptimizer(
+        learning_rate, params['sgd_momentum'])
+    if(params['dist_train']):
+        optimizer = hvd.DistributedOptimizer(optimizer)
+    if params['use_tpu']:
+        optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)
+    with tf.control_dependencies(update_ops):
+        train_op = optimizer.minimize(combined_cost, global_step=global_step)
+
+    # Computations to be executed on CPU, outside of the main TPU queues.
+    def eval_metrics_host_call_fn(policy_output, value_output, pi_tensor, policy_cost,
+                                  value_cost, l2_cost, combined_cost, step,
+                                  est_mode=tf.estimator.ModeKeys.TRAIN):
+        policy_entropy = -tf.reduce_mean(tf.reduce_sum(
+            policy_output * tf.log(policy_output), axis=1))
+        # pi_tensor is one_hot when generated from sgfs (for supervised learning)
+        # and soft-max when using self-play records. argmax normalizes the two.
+        policy_target_top_1 = tf.argmax(pi_tensor, axis=1)
+
+        policy_output_in_top1 = tf.to_float(
+            tf.nn.in_top_k(policy_output, policy_target_top_1, k=1))
+        policy_output_in_top3 = tf.to_float(
+            tf.nn.in_top_k(policy_output, policy_target_top_1, k=3))
+
+        policy_top_1_confidence = tf.reduce_max(policy_output, axis=1)
+        policy_target_top_1_confidence = tf.boolean_mask(
+            policy_output,
+            tf.one_hot(policy_target_top_1, tf.shape(policy_output)[1]))
+
+        value_cost_normalized = value_cost / params['value_cost_weight']
+
+        with tf.variable_scope("metrics"):
+            metric_ops = {
+                'policy_cost': tf.metrics.mean(policy_cost),
+                'value_cost': tf.metrics.mean(value_cost),
+                'value_cost_normalized': tf.metrics.mean(value_cost_normalized),
+                'l2_cost': tf.metrics.mean(l2_cost),
+                'policy_entropy': tf.metrics.mean(policy_entropy),
+                'combined_cost': tf.metrics.mean(combined_cost),
+
+                'policy_accuracy_top_1': tf.metrics.mean(policy_output_in_top1),
+                'policy_accuracy_top_3': tf.metrics.mean(policy_output_in_top3),
+                'policy_top_1_confidence': tf.metrics.mean(policy_top_1_confidence),
+                'policy_target_top_1_confidence': tf.metrics.mean(
+                    policy_target_top_1_confidence),
+                'value_confidence': tf.metrics.mean(tf.abs(value_output)),
+            }
+
+        if est_mode == tf.estimator.ModeKeys.EVAL:
+            return metric_ops
+
+        # NOTE: global_step is rounded to a multiple of FLAGS.summary_steps.
+        eval_step = tf.reduce_min(step)
+
+        # Create summary ops so that they show up in SUMMARIES collection
+        # That way, they get logged automatically during training
+        summary_writer = summary.create_file_writer(FLAGS.work_dir)
+        with summary_writer.as_default(), \
+                summary.record_summaries_every_n_global_steps(
+                    params['summary_steps'], eval_step):
+            for metric_name, metric_op in metric_ops.items():
+                summary.scalar(metric_name, metric_op[1], step=eval_step)
+
+        # Reset metrics occasionally so that they are mean of recent batches.
+        reset_op = tf.variables_initializer(tf.local_variables("metrics"))
+        cond_reset_op = tf.cond(
+            tf.equal(eval_step % params['summary_steps'], tf.to_int64(1)),
+            lambda: reset_op,
+            lambda: tf.no_op())
+
+        return summary.all_summary_ops() + [cond_reset_op]
+
+    metric_args = [
+        policy_output,
+        value_output,
+        labels['pi_tensor'],
+        tf.reshape(policy_cost, [1]),
+        tf.reshape(value_cost, [1]),
+        tf.reshape(l2_cost, [1]),
+        tf.reshape(combined_cost, [1]),
+        tf.reshape(global_step, [1]),
+    ]
+
+    predictions = {
+        'policy_output': policy_output,
+        'value_output': value_output,
+    }
+
+    eval_metrics_only_fn = functools.partial(
+        eval_metrics_host_call_fn, est_mode=tf.estimator.ModeKeys.EVAL)
+    host_call_fn = functools.partial(
+        eval_metrics_host_call_fn, est_mode=tf.estimator.ModeKeys.TRAIN)
+
+    tpu_estimator_spec = tpu_estimator.TPUEstimatorSpec(
+        mode=mode,
+        predictions=predictions,
+        loss=combined_cost,
+        train_op=train_op,
+        eval_metrics=(eval_metrics_only_fn, metric_args),
+        host_call=(host_call_fn, metric_args)
+    )
+    return train_op
+
+
 def model_inference_fn(features, training, params):
     """Builds just the inference part of the model graph.
 
@@ -428,7 +612,8 @@ def model_inference_fn(features, training, params):
 
     def mg_res_layer(inputs):
         residual = residual_inner(inputs)
-        output = mg_activation(inputs + residual)
+        fixed = tf.math.add_n([inputs, residual])
+        output = mg_activation(fixed)
         return output
 
     def mg_squeeze_excitation_layer(inputs):
@@ -538,15 +723,26 @@ def get_estimator():
 
 
 def _get_nontpu_estimator():
-    session_config = tf.ConfigProto()
+    session_config = tf.ConfigProto(
+                        intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                        inter_op_parallelism_threads=FLAGS.num_inter_threads)
     session_config.gpu_options.allow_growth = True
+    model_dir = None
+    if(not FLAGS.dist_train) or (hvd.rank()==0):
+        model_dir = FLAGS.work_dir
+        step_count_steps = 50
+        summary_steps = FLAGS.summary_steps
+    else:
+        step_count_steps = 1000000
+        summary_steps = 1000000
     run_config = tf.estimator.RunConfig(
-        save_summary_steps=FLAGS.summary_steps,
+        log_step_count_steps = step_count_steps,
+        save_summary_steps=summary_steps,
         keep_checkpoint_max=FLAGS.keep_checkpoint_max,
         session_config=session_config)
     return tf.estimator.Estimator(
         model_fn,
-        model_dir=FLAGS.work_dir,
+        model_dir=model_dir,
         config=run_config,
         params=FLAGS.flag_values_dict())
 
@@ -618,14 +814,97 @@ def export_model(model_path):
         print("Copying {} to {}".format(filename, destination_path))
         tf.gfile.Copy(filename, destination_path)
 
+def generate_min_max_log(log_graph_file, tf_records, log_file):
+  cmd = 'numactl -N 0 -l python3 produce_min_max_log.py'
+  cmd += ' --input_graph={0}'.format(log_graph_file)
+  cmd += ' --data_location={0}'.format(tf_records)
+  cmd += ' --num_steps={0}'.format(FLAGS.quantize_test_steps)
+  cmd += ' --batch_size={0}'.format(FLAGS.quantize_test_batch_size)
+  cmd += ' --random_rotation={0}'.format(FLAGS.random_rotation)
+  cmd += ' 2> {0}'.format(log_file)
+  print(cmd)
+  subprocess.call(cmd, shell=True)
+
+def quantization(opt_graph, model_path, tf_records, eval_min_max_every_epoch):
+  # first_quantize
+  #rewriter = quantize_graph.GraphRewriter(opt_graph, 'eightbit', None, None, True, [], [])
+  rewriter = quantize_graph.GraphRewriter(opt_graph, 'eightbit', None, None, True)
+  first_quantize_graph = rewriter.rewrite(["policy_output", "value_output"])
+
+  if eval_min_max_every_epoch:
+    # insert_min_max_log
+    transform = 'insert_logging(op=RequantizationRange, show_name=true, message="__requant_min_max:")'
+    log_graph = TransformGraph(first_quantize_graph, ["pos_tensor"],
+                                      ["policy_output", "value_output"], [transform])
+    with tf.gfile.FastGFile(model_path + '_for_min_max.pb', 'wb') as f:
+        f.write(log_graph.SerializeToString())
+
+    # generate_min_max_log
+    with logged_timer('minmax time'):
+      generate_min_max_log(model_path + '_for_min_max.pb', tf_records, model_path + 'log.txt')
+    shutil.copy(model_path + 'log.txt', os.path.join(os.path.dirname(model_path), 'lastlog.txt'))
+  else:
+    print('min max skipped')
+
+  # apply_calibration
+  if eval_min_max_every_epoch:
+    transform = 'freeze_requantization_ranges(min_max_log_file="{0}")'.format(model_path + 'log.txt')
+  else:
+    transform = 'freeze_requantization_ranges(min_max_log_file="{0}")'.format(os.path.join(os.path.dirname(model_path), 'lastlog.txt'))
+  calibration_graph = TransformGraph(first_quantize_graph, ["pos_tensor"],
+                                    ["policy_output", "value_output"], [transform])
+
+  # fuse_requantize
+  transform = 'fuse_quantized_conv_and_requantize strip_unused_nodes'
+  output_graph = TransformGraph(calibration_graph, ["pos_tensor"],
+                                    ["policy_output", "value_output"], [transform])
+  return output_graph
+
+def optimize_graph(input_graph, model_path, quantizing_graph, tf_records, eval_min_max_every_epoch, freeze=False):
+  if freeze:
+    n = DualNetwork(model_path)
+    fp32_graph = tf.graph_util.convert_variables_to_constants(
+        n.sess, n.sess.graph.as_graph_def(), ["policy_output", "value_output"])
+  else:
+    fp32_graph = graph_pb2.GraphDef()
+    with tf.gfile.Open(input_graph, "rb") as read_f:
+      weight = read_f.read()
+      fp32_graph.ParseFromString(weight)
+
+  opt_graph = optimize_for_inference(
+      fp32_graph,
+      ["pos_tensor"],
+      ["policy_output", "value_output"],
+      dtypes.float32.as_datatype_enum,
+      False)
+
+  if(quantizing_graph):
+    output_graph = quantization(opt_graph, model_path, tf_records, eval_min_max_every_epoch)
+  else:
+    output_graph = opt_graph
+
+  with tf.gfile.GFile(model_path + '.pb', 'wb') as write_f:
+      write_f.write(output_graph.SerializeToString())
+
+def get_input_tensor(graph):
+  return graph.get_tensor_by_name('pos_tensor:0')
+def get_output_tensor(graph):
+  policy_output = graph.get_tensor_by_name('policy_output:0')
+  value_output = graph.get_tensor_by_name('value_output:0')
+  return policy_output, value_output
 
 def freeze_graph(model_path):
     n = DualNetwork(model_path)
     out_graph = tf.graph_util.convert_variables_to_constants(
         n.sess, n.sess.graph.as_graph_def(), ["policy_output", "value_output"])
+    output_graph_def = optimize_for_inference(
+        out_graph,
+        ["pos_tensor"],
+        ["policy_output", "value_output"],
+        dtypes.float32.as_datatype_enum,
+        False)
     with tf.gfile.GFile(model_path + '.pb', 'wb') as f:
-        f.write(out_graph.SerializeToString())
-
+        f.write(output_graph_def.SerializeToString())
 
 def freeze_graph_tpu(model_path):
     """Custom freeze_graph implementation for Cloud TPU."""
diff --git a/minigui/minigui-common.sh b/minigui/minigui-common.sh
old mode 100644
new mode 100755
diff --git a/minigui/unset-minigui-common.sh b/minigui/unset-minigui-common.sh
old mode 100644
new mode 100755
diff --git a/ml_perf/divide_golden_chunk.py b/ml_perf/divide_golden_chunk.py
new file mode 100644
index 0000000..02e6e33
--- /dev/null
+++ b/ml_perf/divide_golden_chunk.py
@@ -0,0 +1,73 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import random
+import functools
+import shutil
+
+import numpy as np
+import tensorflow as tf
+import threading
+
+from mpi4py import MPI
+from absl import app, flags
+from rl_loop import example_buffer
+
+flags.DEFINE_string('read_path', '/tmp/minigo',
+                    'Path to the read origin data.')
+
+flags.DEFINE_string('write_path', '/tmp/minigo/output',
+                    'Path to the read origin data.')
+
+flags.DEFINE_integer('out_files_number', 2,
+                     'Num of files to produce.')
+
+flags.DEFINE_integer('physical_cores', 56,
+                     'Num of cores.')
+
+flags.DEFINE_integer('seed', 0,
+                     'Random seed.')
+
+FLAGS = flags.FLAGS
+
+
+def main(unused_argv):
+  mpi_comm = MPI.COMM_WORLD
+  mpi_rank = mpi_comm.Get_rank()
+  mpi_size = mpi_comm.Get_size()
+  # avoid seed out of range
+  random.seed(FLAGS.seed % 1048576)
+  tf.set_random_seed(FLAGS.seed % 1048576)
+  np.random.seed(FLAGS.seed % 1048576)
+
+  pattern = os.path.join(FLAGS.read_path, '*.zz')
+  files = tf.gfile.Glob(pattern)
+
+  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+  example_num = buffer.parallel_fill(files, threads=FLAGS.physical_cores)
+  # make sure all nodes generate same number of examples
+  example_num = int(mpi_comm.allreduce(example_num, op=MPI.MIN))
+  buffer.flush_new(FLAGS.write_path+'_{}'.format(mpi_rank), example_num, FLAGS.out_files_number, threads=1)
+
+  shutil.rmtree('/tmp/minigo/home', ignore_errors=True)
+
+if __name__ == '__main__':
+  app.run(main)
+
+
+
+
+
+
diff --git a/ml_perf/eval_models.py b/ml_perf/eval_models.py
index 74702e4..b552f42 100644
--- a/ml_perf/eval_models.py
+++ b/ml_perf/eval_models.py
@@ -23,7 +23,7 @@ import os
 from absl import app
 from reference_implementation import evaluate_model, wait
 from rl_loop import fsdb
-
+import ml_perf.mlp_log as mll
 
 def load_train_times():
   models = []
@@ -43,10 +43,17 @@ def main(unused_argv):
   target = 'tf,' + os.path.join(fsdb.models_dir(), 'target.pb')
   models = load_train_times()
   for i, (timestamp, name, path) in enumerate(models):
+    mll.eval_start(i)
     winrate = wait(evaluate_model(path, target, sgf_dir, i + 1))
+    mll.eval_stop(i)
+    mll.eval_accuracy(i, winrate)
     if winrate >= 0.50:
       print('Model {} beat target after {}s'.format(name, timestamp))
-      break
+      mll.eval_result(i, timestamp)
+      mll.run_stop('success')
+      return
+  mll.eval_result(i, 0)
+  mll.run_stop('aborted')
 
 
 if __name__ == '__main__':
diff --git a/ml_perf/execute.py b/ml_perf/execute.py
new file mode 100644
index 0000000..00a6bed
--- /dev/null
+++ b/ml_perf/execute.py
@@ -0,0 +1,69 @@
+# Copyright 2019 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Run the command in multi-instance mode
+
+If there is a --seed parameter from input, change seed to generate randomness among instances
+
+Args:
+  num_instance: the number of instance needed to start
+"""
+
+import sys
+sys.path.insert(0, '.')  # nopep8
+
+import asyncio
+from ml_perf.utils import *
+
+from absl import app, flags
+
+flags.DEFINE_integer('num_instance', 1, 'Number of instances for selfplay')
+
+FLAGS = flags.FLAGS
+
+# Self-play a number of games.
+async def do_execute_mi():
+
+  num_instance = FLAGS.num_instance
+
+  start_copy = False
+  arg_list = []
+  for arg in sys.argv:
+    if start_copy:
+      arg_list.append(arg)
+    if arg == '--':
+      start_copy = True
+
+  if num_instance > 1:
+    result_list = checked_run_mi(
+      num_instance,
+      *arg_list
+    )
+    for result in result_list:
+      # TODO needs to be more generic
+      print ('\n'.join(result.split('\n')[-7:]))
+  else:
+    result = checked_run(
+      *arg_list
+    )
+    print (result)
+
+def main(unused_argv):
+  try:
+    wait(do_execute_mi())
+  finally:
+    asyncio.get_event_loop().close()
+
+if __name__ == '__main__':
+  app.run(main)
diff --git a/ml_perf/flags/9.mn/architecture.flags b/ml_perf/flags/9.mn/architecture.flags
new file mode 100644
index 0000000..ec2abf4
--- /dev/null
+++ b/ml_perf/flags/9.mn/architecture.flags
@@ -0,0 +1,7 @@
+# architecture.flags: Flags that control the model architecture.
+
+--conv_width=32
+--fc_width=64
+--trunk_layers=9
+--value_cost_weight=0.25
+--summary_steps=64
diff --git a/ml_perf/flags/9.mn/bootstrap.flags b/ml_perf/flags/9.mn/bootstrap.flags
new file mode 100644
index 0000000..0283a92
--- /dev/null
+++ b/ml_perf/flags/9.mn/bootstrap.flags
@@ -0,0 +1,9 @@
+# bootstrap.flags
+# Flags for the first bootstrap round of selfplay.
+
+--flagfile=ml_perf/flags/9.mn/selfplay.flags
+
+# Don't perform holdout for the first bootstrap round.
+--holdout_pct=0
+
+--num_readouts=20
diff --git a/ml_perf/flags/9.mn/bootstrap_mi.flags b/ml_perf/flags/9.mn/bootstrap_mi.flags
new file mode 100644
index 0000000..f4da7c1
--- /dev/null
+++ b/ml_perf/flags/9.mn/bootstrap_mi.flags
@@ -0,0 +1,3 @@
+--num_games=8192
+--parallel_games=4
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/eval.flags b/ml_perf/flags/9.mn/eval.flags
new file mode 100644
index 0000000..f07d715
--- /dev/null
+++ b/ml_perf/flags/9.mn/eval.flags
@@ -0,0 +1,6 @@
+# eval.flags: Flags for playing eval games.
+
+--flagfile=ml_perf/flags/9.mn/selfplay.flags
+
+# Play fewer games for eval than selfplay.
+--parallel_games=1
diff --git a/ml_perf/flags/9.mn/eval_mi.flags b/ml_perf/flags/9.mn/eval_mi.flags
new file mode 100644
index 0000000..00e960c
--- /dev/null
+++ b/ml_perf/flags/9.mn/eval_mi.flags
@@ -0,0 +1,3 @@
+--num_games=100
+--parallel_games=1
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/rl_loop.flags b/ml_perf/flags/9.mn/rl_loop.flags
new file mode 100644
index 0000000..c5f2b23
--- /dev/null
+++ b/ml_perf/flags/9.mn/rl_loop.flags
@@ -0,0 +1,15 @@
+# rl_loop.flags: Flags for the reinforcement learning loop.
+
+--flags_dir=ml_perf/flags/9.mn/
+--checkpoint_dir=ml_perf/checkpoint/9/
+
+--iterations=30
+--gating_win_rate=0.49
+--window_size=10
+--engine=tf
+--parallel_post_train=3
+--train_instance_per_numa=2
+--eval_min_max_every_epoch=True
+--quantize_test_steps=1
+--quantize_test_batch_size=80
+
diff --git a/ml_perf/flags/9.mn/selfplay.flags b/ml_perf/flags/9.mn/selfplay.flags
new file mode 100644
index 0000000..a1d5815
--- /dev/null
+++ b/ml_perf/flags/9.mn/selfplay.flags
@@ -0,0 +1,14 @@
+# selfplay.flags: Flags for selfplay.
+
+# This flagfile also serves as the base for the boostrap & eval stages of
+# the RL loop.
+
+--num_readouts=240
+--value_init_penalty=0.2
+--holdout_pct=0.03
+--disable_resign_pct=0.1
+--resign_threshold=-0.99
+
+# Device-specific selfplay flags.
+--parallel_games=1
+--virtual_losses=8
diff --git a/ml_perf/flags/9.mn/selfplay_mi.flags b/ml_perf/flags/9.mn/selfplay_mi.flags
new file mode 100644
index 0000000..4c63d58
--- /dev/null
+++ b/ml_perf/flags/9.mn/selfplay_mi.flags
@@ -0,0 +1,3 @@
+--num_games=4096
+--parallel_games=1
+--multi_instance=True
diff --git a/ml_perf/flags/9.mn/train.flags b/ml_perf/flags/9.mn/train.flags
new file mode 100644
index 0000000..19f8b9d
--- /dev/null
+++ b/ml_perf/flags/9.mn/train.flags
@@ -0,0 +1,15 @@
+# train.flags: Flags for training.
+
+--flagfile=ml_perf/flags/9.mn/architecture.flags
+
+--shuffle_buffer_size=10000
+--filter_amount=0.5
+
+# Device specific hyperparameters re: batch size and LR schedules.
+--train_batch_size=8192
+--lr_rates=0.32
+--lr_rates=0.032
+--lr_rates=0.0032
+--lr_boundaries=12500
+--lr_boundaries=18750
+--l2_strength=0.0001
diff --git a/ml_perf/flags/9.mn/validate.flags b/ml_perf/flags/9.mn/validate.flags
new file mode 100644
index 0000000..de4f22d
--- /dev/null
+++ b/ml_perf/flags/9.mn/validate.flags
@@ -0,0 +1,7 @@
+# validate.flags Flags for validation.
+
+--flagfile=ml_perf/flags/9.mn/architecture.flags
+
+--examples_to_validate=256
+--train_batch_size=64
+--summary_steps=2
diff --git a/ml_perf/flags/9/bootstrap.flags b/ml_perf/flags/9/bootstrap.flags
index 4e7341e..29c66d6 100644
--- a/ml_perf/flags/9/bootstrap.flags
+++ b/ml_perf/flags/9/bootstrap.flags
@@ -6,5 +6,4 @@
 # Don't perform holdout for the first bootstrap round.
 --holdout_pct=0
 
---num_games=8192
 --num_readouts=20
diff --git a/ml_perf/flags/9/bootstrap_mi.flags b/ml_perf/flags/9/bootstrap_mi.flags
new file mode 100644
index 0000000..f4da7c1
--- /dev/null
+++ b/ml_perf/flags/9/bootstrap_mi.flags
@@ -0,0 +1,3 @@
+--num_games=8192
+--parallel_games=4
+--multi_instance=True
diff --git a/ml_perf/flags/9/eval.flags b/ml_perf/flags/9/eval.flags
index aecf855..9f8759e 100644
--- a/ml_perf/flags/9/eval.flags
+++ b/ml_perf/flags/9/eval.flags
@@ -3,5 +3,4 @@
 --flagfile=ml_perf/flags/9/selfplay.flags
 
 # Play fewer games for eval than selfplay.
---num_games=100
---parallel_games=100
+--parallel_games=1
diff --git a/ml_perf/flags/9/eval_mi.flags b/ml_perf/flags/9/eval_mi.flags
new file mode 100644
index 0000000..00e960c
--- /dev/null
+++ b/ml_perf/flags/9/eval_mi.flags
@@ -0,0 +1,3 @@
+--num_games=100
+--parallel_games=1
+--multi_instance=True
diff --git a/ml_perf/flags/9/rl_loop.flags b/ml_perf/flags/9/rl_loop.flags
index c6b6dc2..4b8dc29 100644
--- a/ml_perf/flags/9/rl_loop.flags
+++ b/ml_perf/flags/9/rl_loop.flags
@@ -3,8 +3,8 @@
 --flags_dir=ml_perf/flags/9/
 --checkpoint_dir=ml_perf/checkpoint/9/
 
---iterations=50
+--iterations=30
 --gating_win_rate=0.49
 --window_size=10
 --engine=tf
---parallel_post_train=true
+--train_instance_per_numa=2
diff --git a/ml_perf/flags/9/selfplay.flags b/ml_perf/flags/9/selfplay.flags
index 3d8d64c..5164768 100644
--- a/ml_perf/flags/9/selfplay.flags
+++ b/ml_perf/flags/9/selfplay.flags
@@ -3,7 +3,6 @@
 # This flagfile also serves as the base for the boostrap & eval stages of
 # the RL loop.
 
---num_games=4096
 --num_readouts=240
 --value_init_penalty=0.2
 --holdout_pct=0.03
@@ -11,5 +10,5 @@
 --resign_threshold=-0.99
 
 # Device-specific selfplay flags.
---parallel_games=2048
+--parallel_games=16
 --virtual_losses=8
diff --git a/ml_perf/flags/9/selfplay_mi.flags b/ml_perf/flags/9/selfplay_mi.flags
new file mode 100644
index 0000000..7b30dc8
--- /dev/null
+++ b/ml_perf/flags/9/selfplay_mi.flags
@@ -0,0 +1,3 @@
+--num_games=4096
+--parallel_games=16
+--multi_instance=True
diff --git a/ml_perf/flags/9/train.flags b/ml_perf/flags/9/train.flags
index aa1a3cf..a65044d 100644
--- a/ml_perf/flags/9/train.flags
+++ b/ml_perf/flags/9/train.flags
@@ -6,10 +6,10 @@
 --filter_amount=0.5
 
 # Device specific hyperparameters re: batch size and LR schedules.
---train_batch_size=4096
---lr_rates=0.16
---lr_rates=0.016
---lr_rates=0.0016
---lr_boundaries=25000
---lr_boundaries=37500
+--train_batch_size=8192
+--lr_rates=0.32
+--lr_rates=0.032
+--lr_rates=0.0032
+--lr_boundaries=12500
+--lr_boundaries=18750
 --l2_strength=0.0001
diff --git a/ml_perf/hostlist.sh b/ml_perf/hostlist.sh
new file mode 100755
index 0000000..94465f2
--- /dev/null
+++ b/ml_perf/hostlist.sh
@@ -0,0 +1,3 @@
+# generate a list of host ip or hostname
+# one ip/hostname per line
+cat $HOSTLIST.txt
diff --git a/ml_perf/mlp_log.py b/ml_perf/mlp_log.py
new file mode 100644
index 0000000..501baf1
--- /dev/null
+++ b/ml_perf/mlp_log.py
@@ -0,0 +1,118 @@
+# Copyright 2019 MLBenchmark Group. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Utilities for compliance logging."""
+
+import logging
+import time
+import inspect
+import sys
+
+def init_start():
+  log('init_start', caller_depth=3)
+
+def init_stop():
+  log('init_stop', caller_depth=3)
+
+def run_start():
+  log('run_start', caller_depth=3)
+
+def run_stop(status):
+  assert status == 'success' or status == 'aborted'
+  log('run_stop',
+      meta_data = {'status':status},
+      caller_depth=3)
+
+def block_start(epoch, count):
+  log('block_start',
+      meta_data = {'first_epoch_num':epoch,
+                   'epoch_count':count},
+      caller_depth=3)
+
+def block_stop(epoch):
+  log('block_stop',
+      meta_data = {'first_epoch_num':epoch},
+      caller_depth=3)
+
+def epoch_start(epoch):
+  log('epoch_start',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def epoch_stop(epoch):
+  log('epoch_stop',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_start(epoch):
+  log('eval_start',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_stop(epoch):
+  log('eval_stop',
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def eval_accuracy(epoch, accuracy):
+  log('eval_accuracy',
+      val = '{}'.format(accuracy),
+      meta_data = {'epoch_num':epoch},
+      caller_depth=3)
+
+def global_batch_size(batch_size):
+  log('global_batch_size',
+      val = '{}'.format(batch_size),
+      caller_depth=3)
+
+def lr_rates(rates):
+  log('opt_base_learning_rate',
+      val = '{}'.format(rates),
+      caller_depth=3)
+
+def lr_boundaries(boundaries):
+  log('opt_learning_rate_decay_boundary_steps',
+      val = '{}'.format(boundaries),
+      caller_depth=3)
+
+def save_model(iteration):
+  log('save_model',
+      meta_data = {'iteration':iteration},
+      caller_depth=3)
+
+def eval_result(iteration, timestamp):
+  log('eval_result',
+      meta_data = {'iteration':iteration, 'timestamp':timestamp},
+      caller_depth=3)
+
+def log(key, val='null', meta_data = None, caller_depth=2):
+  filename, lineno = get_caller(caller_depth)
+  meta_dict = {'lineno': lineno, 'file': filename}
+  if meta_data != None:
+    meta_dict.update(meta_data)
+  meta_string = '{}'.format(meta_dict)
+  print(':::MLL %f %s: {"value": %s, "metadata": %s}'%(time.time(), key, val, meta_string), file=sys.stderr)
+
+def get_caller(stack_index=2, root_dir=None):
+  ''' Returns file.py:lineno of your caller. A stack_index of 2 will provide
+      the caller of the function calling this function. Notice that stack_index
+      of 2 or more will fail if called from global scope. '''
+  caller = inspect.getframeinfo(inspect.stack()[stack_index][0])
+
+  # Trim the filenames for readability.
+  filename = caller.filename
+  if root_dir is not None:
+    filename = re.sub("^" + root_dir + "/", "", filename)
+  return (filename, caller.lineno)
diff --git a/ml_perf/reference_implementation.py b/ml_perf/reference_implementation.py
index 1ca724e..c3c9489 100644
--- a/ml_perf/reference_implementation.py
+++ b/ml_perf/reference_implementation.py
@@ -26,17 +26,27 @@ import random
 import re
 import shutil
 import subprocess
+import functools
 import tensorflow as tf
 import time
+import copy
+import multiprocessing as mp
 from ml_perf.utils import *
+import ml_perf.mlp_log as mll
+
+from fractions import gcd
 
 from absl import app, flags
 from rl_loop import example_buffer, fsdb
-from tensorflow import gfile
+import dual_net
+
+from tensorflow.python.platform import gfile
+
+import socket
 
 N = int(os.environ.get('BOARD_SIZE', 19))
 
-flags.DEFINE_string('checkpoint_dir', 'ml_perf/checkpoint/{}'.format(N),
+flags.DEFINE_string('checkpoint_dir', None,
                     'The checkpoint directory specify a start model and a set '
                     'of golden chunks used to start training.  If not '
                     'specified, will start from scratch.')
@@ -58,15 +68,35 @@ flags.DEFINE_string('flags_dir', None,
 
 flags.DEFINE_integer('window_size', 10,
                      'Maximum number of recent selfplay rounds to train on.')
+flags.DEFINE_integer('golden_chunk_split', 2,
+                     'Golden chunk of each selfplay is splited to accelerate write golden chunk')
 
-flags.DEFINE_boolean('parallel_post_train', False,
-                     'If true, run the post-training stages (eval, validation '
-                     '& selfplay) in parallel.')
+flags.DEFINE_integer('parallel_post_train', 0,
+                     '0: run the post-training stages in serial mode'
+                     '1: run the post-training stages (eval, validation '
+                     '& selfplay) in parallel.'
+                     '2: run the post-train stage in pipeline mode.')
 
 flags.DEFINE_string('engine', 'tf', 'The engine to use for selfplay.')
 
-FLAGS = flags.FLAGS
+flags.DEFINE_integer('physical_cores', None, 'The number of cores for each node.')
+flags.DEFINE_integer('virtual_cores', None, 'The number of SMT for each node.')
+flags.DEFINE_integer('numa_cores', None, 'The number of core for each numa node.')
+flags.DEFINE_integer('train_instance_per_numa', 2, 'The number of instance for each numa node.')
+
+flags.DEFINE_bool('setup_train_workers', False, 'True if setting up train workers.')
+
+flags.DEFINE_multi_string('train_node', [], 'The node:core list for training')
+flags.DEFINE_multi_string('eval_node', [], 'The node list for evaluation')
+flags.DEFINE_multi_string('selfplay_node', [], 'The node list for selfplay.')
 
+flags.DEFINE_bool('quantization', True, 'Using Int8 if true.')
+flags.DEFINE_bool('eval_min_max_every_epoch', True, 'Genereting min max log every epoch if true.')
+flags.DEFINE_boolean('random_rotation', True, 'Do random rotation when running for min&max log.')
+flags.DEFINE_integer('quantize_test_steps', 5, 'The steps to run for min&max log.')
+flags.DEFINE_integer('quantize_test_batch_size', 16, 'The batch size for running inference for min&max log.')
+
+FLAGS = flags.FLAGS
 
 class State:
   """State data used in each iteration of the RL loop.
@@ -133,17 +163,15 @@ class WinStats:
     pattern = '\s*(\S+)' + '\s+(\d+)' * 8
     match = re.search(pattern, line)
     if match is None:
-      raise ValueError('Can\t parse line "{}"'.format(line))
+      raise ValueError('Can\'t parse line "{}"'.format(line))
     self.model_name = match.group(1)
     raw_stats = [float(x) for x in match.groups()[1:]]
     self.black_wins = ColorWinStats(*raw_stats[:4])
     self.white_wins = ColorWinStats(*raw_stats[4:])
     self.total_wins = self.black_wins.total + self.white_wins.total
 
-
-def initialize_from_checkpoint(state):
+def initialize_from_checkpoint(state, out_files_number):
   """Initialize the reinforcement learning loop from a checkpoint."""
-
   # The checkpoint's work_dir should contain the most recently trained model.
   model_paths = glob.glob(os.path.join(FLAGS.checkpoint_dir,
                                        'work_dir/model.ckpt-*.pb'))
@@ -152,17 +180,20 @@ def initialize_from_checkpoint(state):
                        'got [{}]'.format(', '.join(model_paths)))
   start_model_path = model_paths[0]
 
+  golden_chunks_dir = os.path.join(FLAGS.checkpoint_dir, 'golden_chunks')
+  for basename in os.listdir(golden_chunks_dir):
+    path = os.path.join(golden_chunks_dir, basename)
+    out_path = os.path.join(fsdb.golden_chunk_dir(), basename)
+    buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+    example_num = buffer.parallel_fill(tf.gfile.Glob(path),FLAGS.physical_cores)
+    buffer.flush_new(out_path, example_num, out_files_number, 1)# FLAGS.physical_cores)
+
   # Copy the latest trained model into the models directory and use it on the
   # first round of selfplay.
   state.best_model_name = 'checkpoint'
-  shutil.copy(start_model_path,
-              os.path.join(fsdb.models_dir(), state.best_model_name + '.pb'))
+  best_model_path = os.path.join(fsdb.models_dir(), state.best_model_name)
 
-  # Copy the training chunks.
-  golden_chunks_dir = os.path.join(FLAGS.checkpoint_dir, 'golden_chunks')
-  for basename in os.listdir(golden_chunks_dir):
-    path = os.path.join(golden_chunks_dir, basename)
-    shutil.copy(path, fsdb.golden_chunk_dir())
+  dual_net.optimize_graph(start_model_path, best_model_path, FLAGS.quantization, fsdb.golden_chunk_dir()+'/*.zz*', FLAGS.eval_min_max_every_epoch)
 
   # Copy the training files.
   work_dir = os.path.join(FLAGS.checkpoint_dir, 'work_dir')
@@ -174,19 +205,68 @@ def initialize_from_checkpoint(state):
 def parse_win_stats_table(stats_str, num_lines):
   result = []
   lines = stats_str.split('\n')
-  while True:
-    # Find the start of the win stats table.
-    assert len(lines) > 1
-    if 'Black' in lines[0] and 'White' in lines[0] and 'm.lmt.' in lines[1]:
-        break
-    lines = lines[1:]
 
-  # Parse the expected number of lines from the table.
-  for line in lines[2:2 + num_lines]:
-    result.append(WinStats(line))
-
-  return result
+  while True:
+    while True:
+      # Find the start of the win stats table.
+      if len(lines) == 0:
+        return result
+      if 'Black' in lines[0] and 'White' in lines[0] and 'm.lmt.' in lines[1]:
+          break
+      lines = lines[1:]
+
+    # Parse the expected number of lines from the table.
+    for line in lines[2:2 + num_lines]:
+      stat = WinStats(line)
+      for s in result:
+        if s.model_name == stat.model_name:
+          s.black_wins.total += stat.black_wins.total
+          s.white_wins.total += stat.white_wins.total
+          s.total_wins += stat.total_wins
+          stat = None
+          break
+      if stat != None:
+        result.append(stat)
+    lines = lines[2 + num_lines:]
+
+def extract_multi_instance(cmd):
+  cmd_list = flags.FlagValues().read_flags_from_files(cmd)
+  new_cmd_list = []
+  multi_instance = False
+  num_instance = 0
+  num_games = 0
+  parallel_games = 0
+
+  for arg in cmd_list:
+    argsplit = arg.split('=', 1)
+    flag = argsplit[0]
+    if flag == '--multi_instance':
+      if argsplit[1] == 'True':
+        multi_instance = True
+      else:
+        multi_instance = False
+    elif flag == '--num_games':
+      num_games = int(argsplit[1])
+    elif flag == '--parallel_games':
+      parallel_games = int(argsplit[1])
+
+  if multi_instance:
+    if num_games % parallel_games != 0:
+      logging.error('Error num_games must be multiply of %d', parallel_games)
+      raise RuntimeError('incompatible num_games/parallel_games combination')
+    num_instance = num_games//parallel_games
+
+  for arg in cmd_list:
+    argsplit = arg.split('=', 1)
+    flag = argsplit[0]
+    if flag == '--multi_instance':
+      pass
+    elif multi_instance and flag == '--num_games':
+      pass
+    else:
+      new_cmd_list.append(arg)
 
+  return multi_instance, num_instance, new_cmd_list
 
 async def run(*cmd):
   """Run the given subprocess command in a coroutine.
@@ -214,20 +294,55 @@ async def run(*cmd):
   # Split stdout into lines.
   return stdout.split('\n')
 
+async def run_distributed(genvs, num_instance, hosts, proclists, numa_nodes,
+                          seed, *cmd):
+  """Run the given subprocess command in a coroutine.
 
-def get_golden_chunk_records():
+  Args:
+    *cmd: the command to run and its arguments.
+
+  Returns:
+    The output that the command wrote to stdout as a list of strings, one line
+    per element (stderr output is piped to stdout).
+
+  Raises:
+    RuntimeError: if the command returns a non-zero result.
+  """
+
+  stdout = await checked_run_distributed(genvs, num_instance, hosts, proclists,
+                                         numa_nodes, seed, fsdb.mpi_log_dir(), *cmd)
+
+  log_path = os.path.join(FLAGS.base_dir, get_cmd_name(cmd) + '.log')
+  with gfile.Open(log_path, 'a') as f:
+    f.write(expand_cmd_str(cmd))
+    f.write('\n')
+    f.write(stdout)
+    f.write('\n')
+
+  # Split stdout into lines.
+  return stdout.split('\n')
+
+def get_golden_chunk_records(window_size):
   """Return up to num_records of golden chunks to train on.
 
   Returns:
     A list of golden chunks up to num_records in length, sorted by path.
   """
 
-  pattern = os.path.join(fsdb.golden_chunk_dir(), '*.zz')
-  return sorted(tf.gfile.Glob(pattern), reverse=True)[:FLAGS.window_size]
+  pattern = os.path.join(fsdb.golden_chunk_dir(), '*.zz*')
+  #if window_size > FLAGS.golden_chunk_split * FLAGS.window_size:
+  #  window_size = FLAGS.golden_chunk_split * FLAGS.window_size
+  return sorted(tf.gfile.Glob(pattern), reverse=True)[:window_size]
+
 
+def gen_golden_chunk(files, state):
+  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
+  buffer.parallel_fill(files[1], threads=1)
+  buffer.flush(os.path.join(fsdb.golden_chunk_dir(),
+                            state.output_model_name + '-{}.tfrecord.zz'.format(files[0])))
 
 # Self-play a number of games.
-async def selfplay(state, flagfile='selfplay'):
+async def selfplay(state, flagfile='selfplay', post=True):
   """Run selfplay and write a training chunk to the fsdb golden_chunk_dir.
 
   Args:
@@ -235,43 +350,92 @@ async def selfplay(state, flagfile='selfplay'):
     flagfile: the name of the flagfile to use for selfplay, either 'selfplay'
         (the default) or 'boostrap'.
   """
-
   output_dir = os.path.join(fsdb.selfplay_dir(), state.output_model_name)
   holdout_dir = os.path.join(fsdb.holdout_dir(), state.output_model_name)
+  output_dir = '/tmp/minigo' + output_dir
+
+  multi_instance, num_instance, flag_list = extract_multi_instance(
+      ['--flagfile={}_mi.flags'.format(os.path.join(FLAGS.flags_dir, flagfile))])
+  sp_cmd = ['bazel-bin/cc/selfplay',
+            '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
+            '--model={}'.format(state.best_model_path),
+            '--output_dir={}'.format(output_dir),
+            '--holdout_dir={}'.format(holdout_dir)]
+  if not multi_instance:
+    lines = await run(
+        *sp_cmd,
+        '--seed={}'.format(state.seed))
+  else:
+    if FLAGS.selfplay_node == []:
+      # run selfplay locally
+      lines = await run(
+          'python3', 'ml_perf/execute.py',
+          '--num_instance={}'.format(num_instance),
+          '--',
+          *sp_cmd,
+          '--seed={}'.format(state.seed))
+    else:
+      with logged_timer('selfplay mn'):
+        # run one selfplay instance per host
+        lines = await run_distributed(
+            ['LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow'],
+            num_instance, FLAGS.selfplay_node, None, None, state.seed,
+            *sp_cmd)
+
+  result = '\n'.join(lines)
+  bias = 0.0
+  #with logged_timer('parse win stats'):
+  #  stats = parse_win_stats_table(result, 1)[0]
+  #  num_games = stats.total_wins
+  #  black_total = stats.black_wins.total
+  #  white_total = stats.white_wins.total
+
+  #  logging.info('Black won %0.3f, white won %0.3f',
+  #               black_total / num_games,
+  #               white_total / num_games)
+  #  bias = abs(white_total - black_total)/num_games
+  #  logging.info('Black total %d, white total %d, total games %d, bias %0.3f.',
+  #               black_total, white_total, num_games, bias)
+
+  if post:
+    await post_selfplay(state)
+
+  return bias
+
+# pack records into golden chunks
+async def post_selfplay(state, clean=False):
+  output_dir = os.path.join(fsdb.selfplay_dir(), state.output_model_name)
+  output_dir = '/tmp/minigo' + output_dir
 
-  lines = await run(
-      'bazel-bin/cc/selfplay',
-      '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
-      '--model={}'.format(state.best_model_path),
-      '--output_dir={}'.format(output_dir),
-      '--holdout_dir={}'.format(holdout_dir),
-      '--seed={}'.format(state.seed))
-  result = '\n'.join(lines[-6:])
-  logging.info(result)
-  stats = parse_win_stats_table(result, 1)[0]
-  num_games = stats.total_wins
-  logging.info('Black won %0.3f, white won %0.3f',
-               stats.black_wins.total / num_games,
-               stats.white_wins.total / num_games)
-
-  # Write examples to a single record.
-  pattern = os.path.join(output_dir, '*', '*.zz')
-  random.seed(state.seed)
-  tf.set_random_seed(state.seed)
-  np.random.seed(state.seed)
-  # TODO(tommadams): This method of generating one golden chunk per generation
-  # is sub-optimal because each chunk gets reused multiple times for training,
-  # introducing bias. Instead, a fresh dataset should be uniformly sampled out
-  # of *all* games in the training window before the start of each training run.
-  buffer = example_buffer.ExampleBuffer(sampling_frac=1.0)
-
-  # TODO(tommadams): parallel_fill is currently non-deterministic. Make it not
-  # so.
-  logging.info('Writing golden chunk from "{}"'.format(pattern))
-  buffer.parallel_fill(tf.gfile.Glob(pattern))
-  buffer.flush(os.path.join(fsdb.golden_chunk_dir(),
-                            state.output_model_name + '.tfrecord.zz'))
+  if clean:
+    hosts = FLAGS.selfplay_node
+    if hosts == []:
+      hosts = ['localhost']
+    cmd = ['rm', '-rf', '/tmp/minigo']
+    lines = await run_distributed([], 1, hosts, None, None, 0, *cmd)
 
+  else:
+    with logged_timer('generate golden chunk'):
+      # Write examples to a single record.
+      hosts = FLAGS.selfplay_node
+      if hosts == []:
+        hosts = ['localhost']
+      num_instance = len(hosts)
+      numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+      train_instance_num = FLAGS.train_instance_per_numa * len(FLAGS.train_node) * numa_per_node
+      selfplay_node_num = len(hosts)
+      selfplay_num = selfplay_node_num
+      out_files_number = int(train_instance_num/gcd(train_instance_num, selfplay_num))
+
+      cmd = ['python3', 'ml_perf/divide_golden_chunk.py',
+          '--read_path={}'.format(output_dir + "/*"),
+          '--write_path={}'.format(os.path.join(fsdb.golden_chunk_dir(), state.output_model_name + '.tfrecord.zz')),
+          '--out_files_number={}'.format(out_files_number),
+          '--physical_cores={}'.format(FLAGS.physical_cores),
+          '--base_dir={}'.format(FLAGS.base_dir)]
+      lines = await run_distributed([], 1, hosts, None, None, state.seed, *cmd)
+
+  print(lines)
 
 async def train(state, tf_records):
   """Run training and write a new model to the fsdb models_dir.
@@ -280,15 +444,66 @@ async def train(state, tf_records):
     state: the RL loop State instance.
     tf_records: a list of paths to TensorFlow records to train on.
   """
+  train_node = FLAGS.train_node
+  num_node = len(train_node)
+  if num_node == 0:
+    dist_train = False
+  else:
+    dist_train = True
+
+  if dist_train:
+    intra_threads = FLAGS.numa_cores // FLAGS.train_instance_per_numa - 1
+    numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+    instance_per_node = numa_per_node * FLAGS.train_instance_per_numa
+
+    mpi_async_progress = ''
+    for i in range(numa_per_node):
+      for j in range(FLAGS.train_instance_per_numa):
+        if (not i==0) or (not j==0):
+          mpi_async_progress += ','
+        mpi_async_progress += '{}'.format(i * FLAGS.numa_cores + j)
+  else:
+    intra_threads = FLAGS.physical_cores
 
   model_path = os.path.join(fsdb.models_dir(), state.train_model_name)
-  await run(
-      'python3', 'train.py', *tf_records,
+  cmd = ['python3', 'train.py',
       '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags')),
       '--work_dir={}'.format(fsdb.working_dir()),
       '--export_path={}'.format(model_path),
+      '--window_size={}'.format(FLAGS.window_size),
+      '--data_path={}'.format(fsdb.golden_chunk_dir()),
       '--training_seed={}'.format(state.seed),
-      '--freeze=true')
+      '--freeze=True',
+      '--num_inter_threads=1',
+      '--num_intra_threads={}'.format(intra_threads)]
+
+  if(dist_train):
+    genvs = ['HOROVOD_FUSION_THRESHOLD=134217728',
+             'KMP_BLOCKTIME=0',
+             'KMP_HW_SUBSET=1T',
+             'OMP_BIND_PROC=true',
+             'I_MPI_ASYNC_PROGRESS_PIN=' + mpi_async_progress,
+             'OMP_NUM_THREADS={}'.format(intra_threads)]
+    hosts = []
+    proclists = []
+    numa_nodes = []
+    for node in range(num_node):
+      # add all instance to the list
+      for numa in range(numa_per_node):
+        for instance in range(FLAGS.train_instance_per_numa):
+          hosts += [train_node[node]]
+          proclist = numa * FLAGS.numa_cores + FLAGS.train_instance_per_numa + instance * intra_threads
+          proclists += ['{}'.format(proclist)]
+          numa_nodes += ['{}'.format(numa)]
+
+    lines = await run_distributed(genvs, 1, hosts, proclists, numa_nodes, None, *cmd, '--dist_train=True')
+  else:
+    lines = await run(*cmd)
+  print('\n'.join(lines), file=sys.stderr)
+
+def post_train(state):
+  mll.save_model(state.iter_num-1)
+
   # Append the time elapsed from when the RL was started to when this model
   # was trained.
   elapsed = time.time() - state.start_time
@@ -315,7 +530,7 @@ async def validate(state, holdout_glob):
         '--work_dir={}'.format(fsdb.working_dir()))
 
 
-async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed):
+async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed, flagfile='eval', gating_win_rate=None):
   """Evaluate one model against a target.
 
   Args:
@@ -328,24 +543,53 @@ async def evaluate_model(eval_model_path, target_model_path, sgf_dir, seed):
     The win-rate of eval_model against target_model in the range [0, 1].
   """
 
-  lines = await run(
-      'bazel-bin/cc/eval',
-      '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'eval.flags')),
-      '--model={}'.format(eval_model_path),
-      '--model_two={}'.format(target_model_path),
-      '--sgf_dir={}'.format(sgf_dir),
-      '--seed={}'.format(seed))
-  result = '\n'.join(lines[-7:])
+  multi_instance, num_instance, flag_list = extract_multi_instance(
+      ['--flagfile={}_mi.flags'.format(os.path.join(FLAGS.flags_dir, flagfile))])
+  eval_cmd = ['bazel-bin/cc/eval',
+              '--flagfile={}.flags'.format(os.path.join(FLAGS.flags_dir, flagfile)),
+              '--model={}'.format(eval_model_path),
+              '--model_two={}'.format(target_model_path),
+              '--sgf_dir={}'.format(sgf_dir)]
+  if not multi_instance:
+    lines = await run(*eval_cmd, '--seed={}'.format(seed))
+  else:
+    if FLAGS.eval_node == []:
+      # run eval locally
+      lines = await run(
+          'python3', 'ml_perf/execute.py',
+          '--num_instance={}'.format(num_instance),
+          '--',
+          *eval_cmd,
+          '--seed={}'.format(seed))
+    else:
+      # run one selfplay instance per host
+      lines = await run_distributed(
+          ['LD_LIBRARY_PATH=$LD_LIBRARY_PATH:cc/tensorflow'],
+          num_instance, FLAGS.eval_node, None, None, seed,
+          *eval_cmd)
+  result = '\n'.join(lines)
   logging.info(result)
   eval_stats, target_stats = parse_win_stats_table(result, 2)
   num_games = eval_stats.total_wins + target_stats.total_wins
   win_rate = eval_stats.total_wins / num_games
+  eval_total = eval_stats.total_wins
+  black_total = eval_stats.black_wins.total
+  white_total = eval_stats.white_wins.total
+
+  if eval_total != 0:
+    bias = abs(white_total - black_total) / eval_total
+  else:
+    # by definition bias = 0.0 if eval model win zero games
+    bias = 0.0
   logging.info('Win rate %s vs %s: %.3f', eval_stats.model_name,
                target_stats.model_name, win_rate)
+  logging.info('Black total %d, white total %d, eval total %d, bias %0.3f.',
+               black_total, white_total, eval_total, bias)
+
   return win_rate
 
 
-async def evaluate_trained_model(state):
+async def evaluate_trained_model(state, gating_win_rate=None):
   """Evaluate the most recently trained model against the current best model.
 
   Args:
@@ -354,27 +598,151 @@ async def evaluate_trained_model(state):
 
   return await evaluate_model(
       state.train_model_path, state.best_model_path,
-      os.path.join(fsdb.eval_dir(), state.train_model_name), state.seed)
+      os.path.join(fsdb.eval_dir(), state.train_model_name), state.seed, gating_win_rate=gating_win_rate)
+
+
+async def evaluate_target_model(state, gating_win_rate=None):
+  sgf_dir = os.path.join(fsdb.eval_dir(), 'target')
+  target = 'tf,' + os.path.join(fsdb.models_dir(), 'target.pb')
+  return await evaluate_model(
+      state.train_model_path, target, sgf_dir, state.iter_num, gating_win_rate=gating_win_rate)
 
+async def set_up_train():
+  train_node = FLAGS.train_node
+  num_node = len(train_node)
+  if num_node == 0:
+    dist_train = False
+  else:
+    dist_train = True
+
+  if dist_train:
+    numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+    instance_per_node = numa_per_node * FLAGS.train_instance_per_numa
+    intra_threads = FLAGS.numa_cores // FLAGS.train_instance_per_numa - 1
+
+    mpi_async_progress = ''
+    for i in range(numa_per_node):
+      for j in range(FLAGS.train_instance_per_numa):
+        if (not i==0) or (not j==0):
+          mpi_async_progress += ','
+        mpi_async_progress += '{}'.format(i * FLAGS.numa_cores + j)
+  else:
+    intra_threads = FLAGS.physical_cores
 
-def rl_loop():
+  cmd = ['python3', 'train.py',
+      '--flagfile={}'.format(os.path.join(FLAGS.flags_dir, 'train.flags')),
+      '--work_dir={}'.format(fsdb.working_dir()),
+      '--window_size={}'.format(FLAGS.window_size),
+      '--data_path={}'.format(fsdb.golden_chunk_dir()),
+      '--training_seed={}'.format(0),
+      '--freeze=True',
+      '--quantization={}'.format(FLAGS.quantization),
+      '--quantize_test_steps={}'.format(FLAGS.quantize_test_steps),
+      '--quantize_test_batch_size={}'.format(FLAGS.quantize_test_batch_size),
+      '--random_rotation={}'.format(FLAGS.random_rotation),
+      '--eval_min_max_every_epoch={}'.format(FLAGS.eval_min_max_every_epoch),
+      '--host_addr={}'.format(FLAGS.train_node[0]),
+      '--num_inter_threads=1',
+      '--num_intra_threads={}'.format(intra_threads)]
+
+  if(dist_train):
+    genvs = ['HOROVOD_FUSION_THRESHOLD=134217728',
+             'KMP_BLOCKTIME=0',
+             'KMP_HW_SUBSET=1T',
+             'OMP_BIND_PROC=true',
+             'I_MPI_ASYNC_PROGRESS_PIN=' + mpi_async_progress,
+             'OMP_NUM_THREADS={}'.format(intra_threads)]
+    hosts = []
+    proclists = []
+    numa_nodes = []
+    for node in range(num_node):
+      # add all instance to the list
+      for numa in range(numa_per_node):
+        for instance in range(FLAGS.train_instance_per_numa):
+          hosts += [train_node[node]]
+          proclist = numa * FLAGS.numa_cores + FLAGS.train_instance_per_numa + instance * intra_threads
+          proclists += ['{}'.format(proclist)]
+          numa_nodes += ['{}'.format(numa)]
+
+    lines = await run_distributed(genvs, 1, hosts, proclists, numa_nodes, None, *cmd, '--dist_train=True')
+  else:
+    lines = run(*cmd)
+  print('\n'.join(lines), file=sys.stderr)
+
+def init_socket(host):
+  count = 0
+  while(True):
+    try:
+      if count == 0:
+        print("init_socket():")
+      reception = socket.socket()
+      addr = (host, 52175)
+      if count == 0:
+        print('connecting to server...')
+      reception.connect(addr)
+      print('worker: connection established.')
+    except:
+      count += 1
+      if count % 10 == 0:
+        print("Error: server {} not found {} times ...\r".format(host, count))
+      time.sleep(0.2)
+    else:
+      return reception
+
+def train_begin(export_path):
+  reception = init_socket(FLAGS.train_node[0])
+  reception.send(export_path.encode())
+  return reception
+
+def train_finished(reception):
+  reception.recv(1024)
+  reception.close()
+
+def train_one_step(export_path):
+  reception = train_begin(export_path)
+  train_finished(reception)
+
+def train_stop():
+  reception = train_begin('stop training')
+  train_finished(reception)
+
+def rl_loop(out_files_number):
   """The main reinforcement learning (RL) loop."""
 
+  # The 'window_size' reflect the split of golden chunk after selfplay
+  # basically each selfplay generate N golden chunks instead of one to
+  # accelerate write golden chunks (N determined by FLAGS.golden_chunk_slit).
+  # Yet this make effective_window_size dynamic.   It should increase by N-1
+  # to keep the effective window size not change.  Then increase by N if no big
+  # chunk left.  Until it reach FLAGS.window_size * FLAGS.golden_chunk_split
+
+  window_size = FLAGS.window_size
+
   state = State()
 
-  if FLAGS.checkpoint_dir:
+  if FLAGS.checkpoint_dir != None:
     # Start from a partially trained model.
-    initialize_from_checkpoint(state)
+    initialize_from_checkpoint(state, out_files_number)
+    window_size = len(get_golden_chunk_records(window_size))
+    mll.init_stop()
+    mll.run_start()
+    state.start_time = time.time()
   else:
     # Play the first round of selfplay games with a fake model that returns
     # random noise. We do this instead of playing multiple games using a single
     # model bootstrapped with random noise to avoid any initial bias.
+    mll.init_stop()
+    mll.run_start()
+    state.start_time = time.time()
+    mll.epoch_start(state.iter_num)
     wait(selfplay(state, 'bootstrap'))
+    window_size += FLAGS.golden_chunk_split
 
     # Train a real model from the random selfplay games.
-    tf_records = get_golden_chunk_records()
+    tf_records = get_golden_chunk_records(window_size)
     state.iter_num += 1
     wait(train(state, tf_records))
+    post_train(state)
 
     # Select the newly trained model as the best.
     state.best_model_name = state.train_model_name
@@ -382,45 +750,198 @@ def rl_loop():
 
     # Run selfplay using the new model.
     wait(selfplay(state))
+    window_size += FLAGS.golden_chunk_split
+    mll.epoch_stop(state.iter_num - 1)
+
+  first_iter = True
+  state_copy = None
+  model_win_rate = -1.0
 
+  socket.setdefaulttimeout(99999999)
   # Now start the full training loop.
   while state.iter_num <= FLAGS.iterations:
-    # Build holdout glob before incrementing the iteration number because we
-    # want to run validation on the previous generation.
-    holdout_glob = os.path.join(fsdb.holdout_dir(), '%06d-*' % state.iter_num,
-                                '*')
-
-    # Train on shuffled game data from recent selfplay rounds.
-    tf_records = get_golden_chunk_records()
-    state.iter_num += 1
-    wait(train(state, tf_records))
+    with logged_timer('iteration time {}'.format(state.iter_num)):
+      mll.epoch_start(state.iter_num)
+      # Build holdout glob before incrementing the iteration number because we
+      # want to run validation on the previous generation.
+      holdout_glob = os.path.join(fsdb.holdout_dir(), '%06d-*' % state.iter_num,
+                                  '*')
+
+      # Train on shuffled game data from recent selfplay rounds.
+      #tf_records = get_golden_chunk_records(window_size)
+
+      if FLAGS.parallel_post_train == 0:
+        state.iter_num += 1
+        train_one_step(os.path.join(fsdb.models_dir(), state.train_model_name))
+        post_train(state)
+        # Run eval, validation & selfplay sequentially.
+        wait(selfplay(state))
+        model_win_rate = wait(evaluate_trained_model(state))
+        if model_win_rate >= FLAGS.gating_win_rate:
+          # Promote the trained model to the best model and increment the generation
+          # number.
+          state.best_model_name = state.train_model_name
+          state.gen_num += 1
+        mll.epoch_stop(state.iter_num - 1)
+        #                               ^ compensate iter_num += 1 above
+
+      if FLAGS.parallel_post_train == 1:
+        state.iter_num += 1
+        reception = train_begin(os.path.join(fsdb.models_dir(), state.train_model_name))
+        wait(selfplay(state))
+        train_finished(reception)
+        post_train(state)
+        # Run eval, validation & selfplay in parallel.
+        model_win_rate = wait(evaluate_trained_model(state))
+        if model_win_rate >= FLAGS.gating_win_rate:
+          # Promote the trained model to the best model and increment the generation
+          # number.
+          state.best_model_name = state.train_model_name
+          state.gen_num += 1
+        mll.epoch_stop(state.iter_num - 1)
+        #                               ^ compensate iter_num += 1 above
+
+      if FLAGS.parallel_post_train == 2:
+        state_copy = copy.copy(state)
+        state.iter_num += 1
+        # run training and evaluation/validation/selfplay in parallel
+        # this is software pipeline-ish parallelism
+        # start train[iter]
+        # |   start valiation[iter-1]
+        # |   wait for validation
+        # |   if not first time start evaluation[iter-1]
+        # |   if not first time wait for evaluation
+        # |   if not first time check for promotion
+        # |   start selfplay[iter]
+        # |   wait selfplay
+        # wait train
+        reception = train_begin(os.path.join(fsdb.models_dir(), state.train_model_name))
+        if not first_iter:
+          post_train(state_copy)
+          model_win_rate = wait(evaluate_trained_model(state_copy))
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # Promote the trained model to the best model
+            state.best_model_name = state_copy.train_model_name
+          mll.epoch_stop(state.iter_num - 1 - 1)
+          #                               ^---^-- compensate iter_num += 1 above
+          #                                   +-- it is actually last iteration
+        else:
+          first_iter = False
+        wait(selfplay(state))
+        train_finished(reception)
+        if not first_iter:
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # Increment the generation number.
+            train_model_name_before = state.train_model_name
+            state.gen_num += 1
+
+            # Output dependency:
+            # In parallel post train mode 1, there is output dependence between
+            # evaluation of iteration i (gen_num++)  and train of iteration i+1
+            # (use gen_num for export model path).  In parallel post train mode
+            # 2 (this mode), the evluation of iteration i is postponed to
+            # iteration i+1 after the training started, thus train of iteration
+            # i+1 won't generate correct model name when promotion needs to
+            # happen.  This part fix up the model name when evaluation decides
+            # there's a promotion
+            train_model_name_after = state.train_model_name
+            model_paths = glob.glob(os.path.join(fsdb.models_dir(), '{}.*'.format(train_model_name_before)))
+            for model in model_paths:
+              logging.info('moving {} --> {}'.format(model,
+                train_model_name_after.join(model.rsplit(train_model_name_before, 1))))
+              shutil.copy(model, train_model_name_after.join(model.rsplit(train_model_name_before, 1)))
+
+      if FLAGS.parallel_post_train == 3:
+        state_copy = copy.copy(state)
+        state.iter_num += 1
+        # run training and evaluation/validation/selfplay in parallel
+        # this is software pipeline-ish parallelism
+        # start train[iter]
+        # |   start valiation[iter-1]
+        # |   wait for validation
+        # |   if not first time start evaluation[iter-1]
+        # |   if not first time wait for evaluation
+        # |   if not first time check for promotion
+        # |   start selfplay[iter]
+        # |   wait selfplay
+        # wait train
+        reception = train_begin(os.path.join(fsdb.models_dir(), state.train_model_name))
+        #train_handle = asyncio.gather(train(state, tf_records), return_exceptions=True)
+        if not first_iter:
+          post_train(state_copy)
+          # predict the play model as promoted model
+          state_play = copy.copy(state_copy)
+          state_play.best_model_name = state_copy.train_model_name
+          model_win_rate, _ = wait([evaluate_trained_model(state_copy, FLAGS.gating_win_rate),
+                                    selfplay(state_play, post=False)])
+          if model_win_rate >= FLAGS.gating_win_rate:
+            logging.info('promote model')
+            # Promote the trained model to the best model
+            state.best_model_name = state_copy.train_model_name
+          else:
+            logging.info('no promote model')
+          mll.epoch_stop(state.iter_num - 1 - 1)
+          #                               ^---^-- compensate iter_num += 1 above
+          #                                   +-- it is actually last iteration
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # prediction hit
+            wait(post_selfplay(state_play))
+          else:
+            # prediction not hit
+            wait(selfplay(state))
+        else:
+          first_iter = False
+          wait(selfplay(state))
+        train_finished(reception)
+        if not first_iter:
+          if model_win_rate >= FLAGS.gating_win_rate:
+            # Increment the generation number.
+            train_model_name_before = state.train_model_name
+            state.gen_num += 1
+
+            # Output dependency:
+            # In parallel post train mode 1, there is output dependence between
+            # evaluation of iteration i (gen_num++)  and train of iteration i+1
+            # (use gen_num for export model path).  In parallel post train mode
+            # 2 (this mode), the evluation of iteration i is postponed to
+            # iteration i+1 after the training started, thus train of iteration
+            # i+1 won't generate correct model name when promotion needs to
+            # happen.  This part fix up the model name when evaluation decides
+            # there's a promotion
+            train_model_name_after = state.train_model_name
+            model_paths = glob.glob(os.path.join(fsdb.models_dir(), '{}.*'.format(train_model_name_before)))
+            for model in model_paths:
+              logging.info('moving {} --> {}'.format(model,
+                train_model_name_after.join(model.rsplit(train_model_name_before, 1))))
+              shutil.copy(model, train_model_name_after.join(model.rsplit(train_model_name_before, 1)))
+
+  # after the main loop, if parallel_post_train = 2
+  # needs to print epoch_stop for last epoch
+  if FLAGS.parallel_post_train == 2 or FLAGS.parallel_post_train == 3:
+    mll.epoch_stop(state.iter_num - 1)
+  train_stop()
 
-    if FLAGS.parallel_post_train:
-      # Run eval, validation & selfplay in parallel.
-      model_win_rate, _, _ = wait([
-          evaluate_trained_model(state),
-          validate(state, holdout_glob),
-          selfplay(state)])
-    else:
-      # Run eval, validation & selfplay sequentially.
-      model_win_rate = wait(evaluate_trained_model(state))
-      wait(validate(state, holdout_glob))
-      wait(selfplay(state))
+def main(unused_argv):
+  """Run the reinforcement learning loop."""
 
-    if model_win_rate >= FLAGS.gating_win_rate:
-      # Promote the trained model to the best model and increment the generation
-      # number.
-      state.best_model_name = state.train_model_name
-      state.gen_num += 1
+  numa_per_node = FLAGS.physical_cores // FLAGS.numa_cores
+  train_instance_num = FLAGS.train_instance_per_numa * len(FLAGS.train_node) * numa_per_node
+  selfplay_node_num = max(len(FLAGS.selfplay_node), 1)
+  selfplay_num = selfplay_node_num
+  out_files_number = int(train_instance_num/gcd(train_instance_num, selfplay_num)*selfplay_node_num)
 
+  FLAGS.window_size = out_files_number * FLAGS.window_size
 
-def main(unused_argv):
-  """Run the reinforcement learning loop."""
+  if(FLAGS.setup_train_workers):
+    wait(set_up_train())
+    return
 
+  mll.init_start()
   print('Wiping dir %s' % FLAGS.base_dir, flush=True)
   shutil.rmtree(FLAGS.base_dir, ignore_errors=True)
   dirs = [fsdb.models_dir(), fsdb.selfplay_dir(), fsdb.holdout_dir(),
-          fsdb.eval_dir(), fsdb.golden_chunk_dir(), fsdb.working_dir()]
+          fsdb.eval_dir(), fsdb.golden_chunk_dir(), fsdb.working_dir(),
+          fsdb.mpi_log_dir()]
   for d in dirs:
     ensure_dir_exists(d);
 
@@ -440,9 +961,13 @@ def main(unused_argv):
   for handler in logging.getLogger().handlers:
     handler.setFormatter(formatter)
 
+  logging.info('Selfplay nodes = {}'.format(FLAGS.selfplay_node))
+  logging.info('Train nodes = {}'.format(FLAGS.train_node))
+  logging.info('Eval nodes = {}'.format(FLAGS.eval_node))
+
   with logged_timer('Total time'):
     try:
-      rl_loop()
+      rl_loop(out_files_number)
     finally:
       asyncio.get_event_loop().close()
 
diff --git a/ml_perf/repeat_run.sh b/ml_perf/repeat_run.sh
old mode 100644
new mode 100755
diff --git a/ml_perf/utils.py b/ml_perf/utils.py
index 8e6b7c6..a26be49 100644
--- a/ml_perf/utils.py
+++ b/ml_perf/utils.py
@@ -20,18 +20,46 @@ sys.path.insert(0, '.')  # nopep8
 import asyncio
 import logging
 import os
+import os.path
+import multiprocessing
+import subprocess
+import fcntl
 
 from absl import flags
 from utils import *
 
 
 def expand_cmd_str(cmd):
-  return '  '.join(flags.FlagValues().read_flags_from_files(cmd))
+  result = ' '.join(flags.FlagValues().read_flags_from_files(cmd))
+  if cmd[0] == 'mpiexec' or cmd[0] == 'mpirun':
+    result = ' \\\n-host '.join(result.split(' -host '))
+  # avoid buffer too big to block I/O
+  return result[:8192]
 
 
 def get_cmd_name(cmd):
   if cmd[0] == 'python' or cmd[0] == 'python3':
     path = cmd[1]
+    for index in range(len(cmd)):
+      if cmd[index] == 'bazel-bin/cc/selfplay':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/eval':
+        path = cmd[index]
+        break
+  elif cmd[0] == 'mpirun' or cmd[0] == 'mpiexec':
+    for index in range(len(cmd)):
+      if cmd[index] == 'train.py':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/selfplay':
+        path = cmd[index]
+        break
+      if cmd[index] == 'bazel-bin/cc/eval':
+        path = cmd[index]
+        break
+      if cmd[index] == 'python' or cmd[index] == 'python3':
+        path = cmd[index+1]
   else:
     path = cmd[0]
   return os.path.splitext(os.path.basename(path))[0]
@@ -73,6 +101,127 @@ async def checked_run(*cmd):
 
     return stdout
 
+async def checked_run_distributed(genvs, num_instance, hosts, proclists, numa_nodes, seed, log_path, *cmd):
+  mpi_cmd = ['mpiexec',
+             '-outfile-pattern',
+             '{}/out-{}-{}-%r.txt'.format(log_path, get_cmd_name(cmd), seed)]
+  for genv in genvs:
+    mpi_cmd = mpi_cmd + ['-genv', genv]
+  num_nodes = len(hosts)
+  instance_per_node = num_instance // num_nodes
+  instance_remaining = num_instance - num_nodes * instance_per_node
+  for index in range(num_nodes):
+    if index < instance_remaining:
+      instance_to_launch = instance_per_node + 1
+    else:
+      instance_to_launch = instance_per_node
+
+    if index > 0:
+      mpi_cmd = mpi_cmd + [':']
+    mpi_cmd = mpi_cmd + ['-host', hosts[index]]
+
+    if proclists != None:
+      mpi_cmd = mpi_cmd + ['-env', 'KMP_AFFINITY=granularity=fine,compact,1,{}'.format(proclists[index])]
+
+    if numa_nodes != None:
+      mpi_cmd = mpi_cmd + ['numactl', '-l', '-N', numa_nodes[index]]
+
+    if num_instance > 1:
+      mpi_cmd = mpi_cmd + ['python3', 'ml_perf/execute.py',
+                           '--num_instance={}'.format(instance_to_launch),
+                           '--']
+    mpi_cmd = mpi_cmd + [*cmd]
+
+    if seed != None:
+      # ensure different seed for different node
+      mpi_cmd = mpi_cmd + ['--seed={}'.format(seed + index*1023779831)]
+
+  result = await checked_run(*mpi_cmd)
+  for index in range(num_nodes):
+    filename = '{}/out-{}-{}-{}.txt'.format(log_path, get_cmd_name(cmd), seed,
+                                            index)
+    outfile = open(filename, 'r')
+    result += outfile.read()
+    outfile.close()
+  return result
+
+def checked_run_mi(num_instance, *cmd):
+  name = get_cmd_name(cmd)
+  logging.debug('Running %s*%d: %s', name, num_instance, expand_cmd_str(cmd))
+  num_parallel_instance = int(multiprocessing.cpu_count())
+  procs=[None]*num_parallel_instance
+  results = [""]*num_parallel_instance
+  result_list = []
+
+  cur_instance = 0
+  # add new proc into procs
+  while cur_instance < num_instance or not all (
+      proc is None for proc in procs):
+    if None in procs and cur_instance < num_instance:
+      index = procs.index(None)
+      subproc_cmd = [
+              'OMP_NUM_THREADS=1',
+              'KMP_AFFINITY=granularity=fine,proclist=[{}],explicit'.format(
+                  ','.join(str(i) for i in list(range(
+                      index, index+1)))),
+              *cmd,
+              '--instance_id={}'.format(cur_instance),
+      ]
+      subproc_cmd = ' '.join(subproc_cmd)
+      if (cur_instance == 0):
+        logging.debug("subproc_cmd = {}".format(subproc_cmd))
+      procs[index] = subprocess.Popen(subproc_cmd, shell=True,
+                                      stdout=subprocess.PIPE,
+                                      stderr=subprocess.STDOUT)
+
+      proc_count = 0
+      for i in range(num_parallel_instance):
+        if procs[i] != None:
+          proc_count += 1
+      logging.debug('started instance {} in proc {}. proc count = {}'.format(
+          cur_instance, index, proc_count))
+
+      # change stdout of the process to non-blocking
+      # this is for collect output in a single thread
+      flags = fcntl.fcntl(procs[index].stdout, fcntl.F_GETFL)
+      fcntl.fcntl(procs[index].stdout, fcntl.F_SETFL, flags | os.O_NONBLOCK)
+
+      cur_instance += 1
+    for index in range(num_parallel_instance):
+      if procs[index] != None:
+        # collect proc output
+        while True:
+          try:
+            line = procs[index].stdout.readline()
+            if line == b'':
+              break
+            results[index] = results[index] + line.decode()
+          except IOError:
+            break
+
+        ret_val = procs[index].poll()
+        if ret_val == None:
+          continue
+        elif ret_val != 0:
+          logging.info(results[index])
+          raise RuntimeError(
+            'Non-zero return code (%d) executing %s' % (
+                ret_val, subproc_cmd))
+
+        if index == 0:
+          logging.debug(results[index])
+        result_list.append(results[index])
+        results[index] = ""
+        procs[index] = None
+
+        proc_count = 0
+        for i in range(num_parallel_instance):
+          if procs[i] != None:
+            proc_count += 1
+        logging.debug('proc {} finished. proc count = {}'.format(
+            index, proc_count))
+    time.sleep(0.001)  # avoid busy loop
+  return result_list
 
 def wait(aws):
   """Waits for all of the awaitable objects (e.g. coroutines) in aws to finish.
diff --git a/oneoffs/distillation.py b/oneoffs/distillation.py
old mode 100755
new mode 100644
diff --git a/oneoffs/embeddings.py b/oneoffs/embeddings.py
old mode 100755
new mode 100644
diff --git a/oneoffs/embeddings_graphs.py b/oneoffs/embeddings_graphs.py
old mode 100755
new mode 100644
diff --git a/oneoffs/l2_cost_by_var.py b/oneoffs/l2_cost_by_var.py
old mode 100755
new mode 100644
diff --git a/oneoffs/modelstats.sh b/oneoffs/modelstats.sh
old mode 100755
new mode 100644
diff --git a/oneoffs/training_curve.py b/oneoffs/training_curve.py
old mode 100755
new mode 100644
diff --git a/preprocessing.py b/preprocessing.py
index 595db38..bc134e2 100644
--- a/preprocessing.py
+++ b/preprocessing.py
@@ -26,6 +26,9 @@ import symmetries
 import numpy as np
 import tensorflow as tf
 
+import horovod.tensorflow as hvd
+from tensorflow.python.data.experimental.ops import optimization
+
 TF_RECORD_CONFIG = tf.python_io.TFRecordOptions(
     tf.python_io.TFRecordCompressionType.ZLIB)
 
@@ -84,11 +87,11 @@ def batch_parse_tf_example(batch_size, example_batch):
         'outcome': tf.FixedLenFeature([], tf.float32),
     }
     parsed = tf.parse_example(example_batch, features)
-    x = tf.decode_raw(parsed['x'], tf.uint8)
+    x = tf.io.decode_raw(parsed['x'], tf.uint8)
     x = tf.cast(x, tf.float32)
     x = tf.reshape(x, [batch_size, go.N, go.N,
                        features_lib.NEW_FEATURES_PLANES])
-    pi = tf.decode_raw(parsed['pi'], tf.float32)
+    pi = tf.io.decode_raw(parsed['pi'], tf.float32)
     pi = tf.reshape(pi, [batch_size, go.N * go.N + 1])
     outcome = parsed['outcome']
     outcome.set_shape([batch_size])
@@ -98,7 +101,7 @@ def batch_parse_tf_example(batch_size, example_batch):
 def read_tf_records(batch_size, tf_records, num_repeats=1,
                     shuffle_records=True, shuffle_examples=True,
                     shuffle_buffer_size=None, interleave=True,
-                    filter_amount=1.0):
+                    filter_amount=1.0, dist_train=False, seed = 0):
     """
     Args:
         batch_size: batch size to return
@@ -116,10 +119,17 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
         raise ValueError("Must set shuffle buffer size if shuffling examples")
 
     tf_records = list(tf_records)
-    if shuffle_records:
-        random.shuffle(tf_records)
+
+    random.seed(seed)
+
+    #if shuffle_records:
+    #    random.shuffle(tf_records)
+
     record_list = tf.data.Dataset.from_tensor_slices(tf_records)
 
+    if dist_train:
+        record_list = record_list.shard(hvd.size(), hvd.rank())
+
     # compression_type here must agree with write_tf_examples
     map_func = functools.partial(
         tf.data.TFRecordDataset,
@@ -130,20 +140,85 @@ def read_tf_records(batch_size, tf_records, num_repeats=1,
         # cycle_length = how many tfrecord files are read in parallel
         # The idea is to shuffle both the order of the files being read,
         # and the examples being read from the files.
-        dataset = record_list.apply(tf.contrib.data.parallel_interleave(
-            map_func, cycle_length=64, sloppy=True))
+        dataset = record_list.apply(tf.data.experimental.parallel_interleave(
+            map_func, cycle_length=1000, sloppy=True))
     else:
         dataset = record_list.flat_map(map_func)
 
     if filter_amount < 1.0:
         dataset = dataset.filter(
-            lambda _: tf.random_uniform([]) < filter_amount)
+            lambda _: tf.random.uniform([], seed=seed) < filter_amount)
+        dataset = dataset.apply(optimization.optimize(["filter_with_random_uniform_fusion"]))
+
+    #if dist_train:
+    #    dataset = dataset.shard(hvd.size(), hvd.rank())
 
     dataset = dataset.repeat(num_repeats)
+
     if shuffle_examples:
         dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)
 
-    dataset = dataset.batch(batch_size)
+    dataset = dataset.batch(batch_size, drop_remainder=True)
+    return dataset
+
+def read_tf_records_new(batch_size, tf_records, num_repeats=1,
+                    shuffle_records=True, shuffle_examples=True,
+                    shuffle_buffer_size=None, interleave=True,
+                    filter_amount=1.0, dist_train=False, seed = 0):
+    """
+    Args:
+        batch_size: batch size to return
+        tf_records: a list of tf_record filenames
+        num_repeats: how many times the data should be read (default: One)
+        shuffle_records: whether to shuffle the order of files read
+        shuffle_examples: whether to shuffle the tf.Examples
+        shuffle_buffer_size: how big of a buffer to fill before shuffling.
+        interleave: iwhether to interleave examples from multiple tf_records
+        filter_amount: what fraction of records to keep
+    Returns:
+        a tf dataset of batched tensors
+    """
+    if shuffle_examples and not shuffle_buffer_size:
+        raise ValueError("Must set shuffle buffer size if shuffling examples")
+
+    random.seed(seed)
+
+    #if shuffle_records:
+    #    random.shuffle(tf_records)
+    record_list = tf.data.Dataset.from_tensor_slices(tf_records)
+
+    if dist_train:
+        record_list = record_list.shard(hvd.size(), hvd.rank())
+
+    # compression_type here must agree with write_tf_examples
+    map_func = functools.partial(
+        tf.data.TFRecordDataset,
+        buffer_size=8 * 1024 * 1024,
+        compression_type='ZLIB')
+
+    if interleave:
+        # cycle_length = how many tfrecord files are read in parallel
+        # The idea is to shuffle both the order of the files being read,
+        # and the examples being read from the files.
+        dataset = record_list.apply(tf.data.experimental.parallel_interleave(
+            map_func, cycle_length=1000, sloppy=True))
+    else:
+        dataset = record_list.flat_map(map_func)
+
+    if filter_amount < 1.0:
+        dataset = dataset.filter(
+            lambda _: tf.random.uniform([], seed=seed) < filter_amount)
+        dataset = dataset.apply(optimization.optimize(["filter_with_random_uniform_fusion"]))
+
+    #if dist_train:
+    #    dataset = dataset.shard(hvd.size(), hvd.rank())
+
+    dataset = dataset.repeat(num_repeats)
+
+    if shuffle_examples:
+        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)
+
+    dataset = dataset.batch(batch_size, drop_remainder=True)
     return dataset
 
 
@@ -181,7 +256,8 @@ def _random_rotation_pure_tf(x_tensor, outcome_tensor):
 def get_input_tensors(batch_size, tf_records, num_repeats=1,
                       shuffle_records=True, shuffle_examples=True,
                       shuffle_buffer_size=None,
-                      filter_amount=0.05, random_rotation=True):
+                      filter_amount=0.05, random_rotation=True,
+                      dist_train=False, seed = 0, make_one_shot = False):
     """Read tf.Records and prepare them for ingestion by dual_net.
 
     See `read_tf_records` for parameter documentation.
@@ -197,18 +273,51 @@ def get_input_tensors(batch_size, tf_records, num_repeats=1,
         shuffle_examples=shuffle_examples,
         shuffle_buffer_size=shuffle_buffer_size,
         filter_amount=filter_amount,
-        interleave=True)
+        interleave=True,
+        dist_train=dist_train, seed=seed)
     dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
     dataset = dataset.map(
         functools.partial(batch_parse_tf_example, batch_size))
     if random_rotation:
         dataset = dataset.map(_random_rotation_pyfunc)
 
-    return dataset.make_one_shot_iterator().get_next()
+    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
+    if make_one_shot:
+      return dataset.make_one_shot_iterator().get_next()
+    else:
+      return dataset
+
+def get_input_tensors_new(batch_size, tf_records, num_repeats=1,
+                      shuffle_records=True, shuffle_examples=True,
+                      shuffle_buffer_size=None,
+                      filter_amount=0.05, random_rotation=True,
+                      dist_train=False, seed = 0, make_one_shot = False):
+    """Read tf.Records and prepare them for ingestion by dual_net.
+    See `read_tf_records` for parameter documentation.
+    Returns a dict of tensors (see return value of batch_parse_tf_example)
+    """
+    #print("Reading tf_records from {} inputs".format(len(tf_records)))
+    dataset = read_tf_records_new(
+        batch_size,
+        tf_records,
+        num_repeats=num_repeats,
+        shuffle_records=shuffle_records,
+        shuffle_examples=shuffle_examples,
+        shuffle_buffer_size=shuffle_buffer_size,
+        filter_amount=filter_amount,
+        interleave=True,
+        dist_train=dist_train, seed=seed)
+    dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
+    dataset = dataset.map(
+        functools.partial(batch_parse_tf_example, batch_size))
+    if random_rotation:
+        dataset = dataset.map(_random_rotation_pyfunc)
 
+    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
+    return dataset.make_initializable_iterator()
 
 def get_tpu_input_tensors(batch_size, tf_records, num_repeats=1,
-                          filter_amount=1, random_rotation=True):
+                          filter_amount=1, random_rotation=True, seed=0):
     # TPUs trains on sequential golden chunks to simplify preprocessing and
     # reproducibility.
     assert len(tf_records) < 101, "Use example_buffer to build a golden_chunk"
@@ -221,7 +330,7 @@ def get_tpu_input_tensors(batch_size, tf_records, num_repeats=1,
         shuffle_examples=False,
         shuffle_buffer_size=None,
         filter_amount=filter_amount,
-        interleave=False)
+        interleave=False, seed=seed)
     dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
     dataset = dataset.map(
         functools.partial(batch_parse_tf_example, batch_size))
diff --git a/produce_min_max_log.py b/produce_min_max_log.py
new file mode 100644
index 0000000..493ce38
--- /dev/null
+++ b/produce_min_max_log.py
@@ -0,0 +1,94 @@
+# Copyright 2019 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#!/usr/bin/env python
+# encoding: utf-8
+
+import time
+import os
+
+import tensorflow as tf
+from tensorflow.core.framework import graph_pb2
+from tensorflow.python.platform import gfile
+
+from absl import app, flags
+
+import preprocessing
+import dual_net
+
+
+flags.DEFINE_string('input_graph', None, 'The path of input graph.')
+flags.DEFINE_string('data_location', None, 'The path of input data.')
+flags.DEFINE_integer('num_steps', 20, 'Number of eval steps.')
+flags.DEFINE_integer('batch_size', 20, 'eval batch size.')
+flags.DEFINE_boolean('random_rotation', True, 'Do random rotation if true.')
+
+
+FLAGS = flags.FLAGS
+
+def run_graph(graph, tf_records):
+
+  data_graph = tf.Graph()
+  with data_graph.as_default():
+    features, labels = preprocessing.get_input_tensors(
+              FLAGS.batch_size,
+              tf_records,
+              shuffle_buffer_size=100000000,
+              random_rotation=FLAGS.random_rotation, seed=2,
+              dist_train=False, make_one_shot=True)
+
+  infer_graph = tf.Graph()
+  with infer_graph.as_default():
+    tf.import_graph_def(graph, name='')
+
+  input_tensor = dual_net.get_input_tensor(infer_graph)
+  output_tensor = dual_net.get_output_tensor(infer_graph)
+
+  config = tf.ConfigProto(
+                intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                inter_op_parallelism_threads=FLAGS.num_inter_threads)
+  data_sess = tf.Session(graph=data_graph, config=config)
+  infer_sess = tf.Session(graph=infer_graph, config=config)
+
+  elapsed = 0
+  #with tf.contrib.tfprof.ProfileContext('/home/letiank/skx-8180/train_dir/minigo', trace_steps=range(70, 80), dump_steps=[110]):
+  for it in range(FLAGS.num_steps):
+    features_np = data_sess.run(features)
+    start_time = time.time()
+    infer_sess.run(output_tensor, feed_dict={input_tensor: features_np})
+    elapsed += time.time() - start_time
+
+def read_graph(input_graph):
+  if not gfile.Exists(input_graph):
+    print("Input graph file '" + input_graph + "' does not exist!")
+    exit(-1)
+
+  input_graph_def = graph_pb2.GraphDef()
+  with gfile.Open(input_graph, "rb") as f:
+    data = f.read()
+    input_graph_def.ParseFromString(data)
+
+  return input_graph_def
+
+
+def main(unused_argv):
+  """Run the reinforcement learning loop."""
+
+  graph = read_graph(FLAGS.input_graph)
+  tf_records = sorted(tf.gfile.Glob(FLAGS.data_location), reverse=True)[:1]
+  print(tf_records)
+  run_graph(graph, tf_records)
+
+if __name__ == "__main__":
+    app.run(main)
diff --git a/quantize_graph.py b/quantize_graph.py
new file mode 100644
index 0000000..4789825
--- /dev/null
+++ b/quantize_graph.py
@@ -0,0 +1,1636 @@
+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Transforms a float-trained graph into an equivalent quantized version.
+An example of command-line usage is:
+bazel build tensorflow/tools/quantization:quantize_graph \
+&& bazel-bin/tensorflow/tools/quantization/quantize_graph \
+--input=tensorflow_inception_graph.pb
+--output_node_names="softmax2" --print_nodes --output=/tmp/quantized_graph.pb \
+--mode=eightbit --logtostderr
+To quantize for Intel CPU, add --intel_cpu_eightbitize=True.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import re
+import numpy as np
+
+from tensorflow.core.framework import attr_value_pb2
+from tensorflow.core.framework import graph_pb2
+from tensorflow.core.framework import node_def_pb2
+from tensorflow.python.client import session
+from tensorflow.python.framework import constant_op
+from tensorflow.python.framework import dtypes
+from tensorflow.python.framework import graph_util
+from tensorflow.python.framework import importer
+from tensorflow.python.framework import ops
+from tensorflow.python.framework import tensor_shape
+from tensorflow.python.framework import tensor_util
+from tensorflow.python.ops import array_ops
+from tensorflow.python.platform import app
+from tensorflow.python.platform import flags as flags_lib
+from tensorflow.python.platform import gfile
+from google.protobuf import text_format
+
+flags = flags_lib
+FLAGS = flags.FLAGS
+
+flags.DEFINE_boolean("print_nodes", False, """Lists all nodes in the model.""")
+flags.DEFINE_string("input", "", """TensorFlow 'GraphDef' file to load.""")
+flags.DEFINE_string("output_node_names", "",
+                    """Output node names, comma separated.""")
+flags.DEFINE_string("output", "", """File to save the output graph to.""")
+flags.DEFINE_integer("bitdepth", 8,
+                     """How many bits to quantize the graph to.""")
+flags.DEFINE_string("mode", "round",
+                    """What transformation to apply (round, quantize,"""
+                    """ eightbit, weights, or weights_rounded).""")
+flags.DEFINE_string("test_input_dims", "1,224,224,3",
+                    """The size of the input tensor to use when testing a"""
+                    """ graph loaded from a file.""")
+flags.DEFINE_boolean("strip_redundant_quantization", True,
+                     """Removes redundant dequantize/quantize pairs.""")
+flags.DEFINE_boolean("quantized_input", False,
+                     "If true, assume Placeholders are quantized with values "
+                     "covering [--quantized_input_min,--quantized_input_max]. "
+                     "Only supported when --mode=eightbit")
+flags.DEFINE_float("quantized_input_min", 0,
+                   "The minimum of the actual input range when "
+                   "--quantized_input")
+flags.DEFINE_float("quantized_input_max", 1,
+                   "The maximum of the actual input range when "
+                   "--quantized_input")
+flags.DEFINE_float(
+    "quantized_fallback_min", None,
+    "The fallback 'min' value to use for layers which lack min-max "
+    "information. Note: this should be considered a coarse tool just good "
+    "enough for experimentation purposes, since graphs quantized in this way "
+    "would be very inaccurate.")
+flags.DEFINE_float(
+    "quantized_fallback_max", None,
+    "The fallback 'max' value to use for layers which lack min-max "
+    "information. Note: this should be considered a coarse tool just good "
+    "enough for experimentation purposes, since graphs quantized in this way "
+    "would be very inaccurate.")
+flags.DEFINE_boolean("input_binary", True,
+                     """Input graph binary or text.""")
+flags.DEFINE_boolean("output_binary", True,
+                     """Output graph binary or text.""")
+flags.DEFINE_boolean(
+    "intel_cpu_eightbitize", False,
+    "If true eightbitized graph will include fused quantized"
+    "nodes in the output_graph for Intel CPU.")
+
+def print_input_nodes(current_node, nodes_map, indent, already_visited):
+  print(" " * indent + current_node.op + ":" + current_node.name)
+  already_visited[current_node.name] = True
+  for input_node_name in current_node.input:
+    if input_node_name in already_visited:
+      continue
+    input_node = nodes_map[input_node_name]
+    print_input_nodes(input_node, nodes_map, indent + 1, already_visited)
+
+
+def create_node(op, name, inputs):
+  new_node = node_def_pb2.NodeDef()
+  new_node.op = op
+  new_node.name = name
+  for input_name in inputs:
+    new_node.input.extend([input_name])
+  return new_node
+
+
+def create_constant_node(name, value, dtype, shape=None):
+  node = create_node("Const", name, [])
+  set_attr_dtype(node, "dtype", dtype)
+  set_attr_tensor(node, "value", value, dtype, shape)
+  return node
+
+
+def copy_attr(node, key, attr_value):
+  try:
+    node.attr[key].CopyFrom(attr_value)
+  except KeyError:
+    pass
+
+
+def set_attr_dtype(node, key, value):
+  try:
+    node.attr[key].CopyFrom(
+        attr_value_pb2.AttrValue(type=value.as_datatype_enum))
+  except KeyError:
+    pass
+
+
+def set_attr_shape(node, key, value):
+  try:
+    node.attr[key].CopyFrom(
+        attr_value_pb2.AttrValue(shape=tensor_shape.as_shape(value).as_proto()))
+  except KeyError:
+    pass
+
+
+def set_attr_tensor(node, key, value, dtype, shape=None):
+  try:
+    node.attr[key].CopyFrom(
+        attr_value_pb2.AttrValue(tensor=tensor_util.make_tensor_proto(
+            value, dtype=dtype, shape=shape)))
+  except KeyError:
+    pass
+
+
+def set_attr_string(node, key, value):
+  try:
+    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(s=value))
+  except KeyError:
+    pass
+
+
+def set_attr_int_list(node, key, value):
+  list_value = attr_value_pb2.AttrValue.ListValue(i=value)
+  try:
+    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(list=list_value))
+  except KeyError:
+    pass
+
+
+def set_attr_bool(node, key, value):
+  try:
+    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(b=value))
+  except KeyError:
+    pass
+
+
+def set_attr_int(node, key, value):
+  try:
+    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(i=value))
+  except KeyError:
+    pass
+
+
+def set_attr_float(node, key, value):
+  try:
+    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(f=value))
+  except KeyError:
+    pass
+
+
+def node_name_from_input(node_name):
+  """Strips off ports and other decorations to get the underlying node name."""
+  if node_name.startswith("^"):
+    node_name = node_name[1:]
+  m = re.search(r"(.*):\d+$", node_name)
+  if m:
+    node_name = m.group(1)
+  return node_name
+
+
+def ensure_tensor_name_has_port(node_name):
+  """Makes sure that a tensor name has :0 if no explicit port exists."""
+  m = re.search(r"(.*):\d+$", node_name)
+  if m:
+    name_with_port = node_name
+  else:
+    name_with_port = node_name + ":0"
+  return name_with_port
+
+
+def unique_node_name_from_input(node_name):
+  """Replaces invalid characters in input names to get a unique node name."""
+  return node_name.replace(":", "__port__").replace("^", "__hat__")
+
+
+def quantize_array(arr, num_buckets):
+  """Quantizes a numpy array.
+  This function maps each scalar in arr to the center of one of num_buckets
+  buckets. For instance,
+  quantize_array([0, 0.3, 0.6, 1], 2) => [0.25, 0.25, 0.75, 0.75]
+  Args:
+    arr: The numpy array to quantize.
+    num_buckets: The number of buckets to map "var" to.
+  Returns:
+    The quantized numpy array.
+  Raises:
+    ValueError: when num_buckets < 1.
+  """
+  if num_buckets < 1:
+    raise ValueError("num_buckets must be >= 1")
+  arr_max = arr.max()
+  arr_min = arr.min()
+  if arr_max == arr_min:
+    return arr
+  bucket_width = (arr_max - arr_min) / num_buckets
+  # Map scalars to bucket indices. Take special care of max(arr).
+  bucket_indices = np.floor((arr - arr_min) / bucket_width)
+  bucket_indices[bucket_indices == num_buckets] = num_buckets - 1
+  # Map each scalar to the center of a bucket.
+  arr = arr_min + bucket_width * (bucket_indices + 0.5)
+  return arr
+
+
+def quantize_weight_rounded(input_node):
+  """Returns a replacement node for input_node containing bucketed floats."""
+  input_tensor = input_node.attr["value"].tensor
+  tensor_value = tensor_util.MakeNdarray(input_tensor)
+  shape = input_tensor.tensor_shape
+  # Currently, the parameter FLAGS.bitdepth is used to compute the
+  # number of buckets as 1 << FLAGS.bitdepth, meaning the number of
+  # buckets can only be a power of 2.
+  # This could be fixed by introducing a new parameter, num_buckets,
+  # which would allow for more flexibility in chosing the right model
+  # size/accuracy tradeoff. But I didn't want to add more parameters
+  # to this script than absolutely necessary.
+  num_buckets = 1 << FLAGS.bitdepth
+  tensor_value_rounded = quantize_array(tensor_value, num_buckets)
+  tensor_shape_list = tensor_util.TensorShapeProtoToList(shape)
+  return [
+      create_constant_node(
+          input_node.name,
+          tensor_value_rounded,
+          dtypes.float32,
+          shape=tensor_shape_list)
+  ]
+
+
+def quantize_weight_eightbit(input_node, quantization_mode):
+  """Returns replacement nodes for input_node using the Dequantize op."""
+  base_name = input_node.name + "_"
+  quint8_const_name = base_name + "quint8_const"
+  min_name = base_name + "min"
+  max_name = base_name + "max"
+  float_tensor = tensor_util.MakeNdarray(input_node.attr["value"].tensor)
+  min_value = np.min(float_tensor.flatten())
+  max_value = np.max(float_tensor.flatten())
+  # Make sure that the range includes zero.
+  if min_value > 0.0:
+    min_value = 0.0
+  # min_value == max_value is a tricky case. It can occur for general
+  # tensors, and of course for scalars. The quantized ops cannot deal
+  # with this case, so we set max_value to something else.
+  # It's a tricky question what is the numerically best solution to
+  # deal with this degeneracy.
+  # TODO(petewarden): Better use a tolerance than a hard comparison?
+  if min_value == max_value:
+    if abs(min_value) < 0.000001:
+      max_value = min_value + 1.0
+    elif min_value > 0:
+      max_value = 2 * min_value
+    else:
+      max_value = min_value / 2.0
+
+  sess = session.Session()
+  with sess.as_default():
+    quantize_op = array_ops.quantize_v2(
+        float_tensor,
+        min_value,
+        max_value,
+        dtypes.quint8,
+        mode=quantization_mode)
+    quint8_tensor = quantize_op[0].eval()
+    min_value = quantize_op[1].eval()
+    max_value = quantize_op[2].eval()
+  shape = tensor_util.TensorShapeProtoToList(input_node.attr["value"]
+                                             .tensor.tensor_shape)
+  quint8_const_node = create_constant_node(
+      quint8_const_name, quint8_tensor, dtypes.quint8, shape=shape)
+  min_node = create_constant_node(min_name, min_value, dtypes.float32)
+  max_node = create_constant_node(max_name, max_value, dtypes.float32)
+  dequantize_node = create_node("Dequantize", input_node.name,
+                                [quint8_const_name, min_name, max_name])
+  set_attr_dtype(dequantize_node, "T", dtypes.quint8)
+  set_attr_string(dequantize_node, "mode", quantization_mode)
+  return [quint8_const_node, min_node, max_node, dequantize_node]
+
+# TODO(intel-tf): Current Intel-CPU quantized Conv2D and Matmul supports only
+# signed scaled mode of weight quantization.
+def intel_cpu_quantize_weight_eightbit(input_node, quantization_mode="SCALED"):
+  """Returns replacement of constant weight node.
+  This function creates (i) a quantized constant node, (ii) a float min node
+  (iii) a float max node, and (iv) a dequantize node."""
+  base_name = input_node.name + "_"
+  qint8_const_name = base_name + "qint8_const"
+  min_name = base_name + "min"
+  max_name = base_name + "max"
+  float_tensor = tensor_util.MakeNdarray(input_node.attr["value"].tensor)
+  min_value = np.min(float_tensor.flatten())
+  max_value = np.max(float_tensor.flatten())
+  # Same processing of min-max as in quantize_weight_eightbit function.
+  if min_value > 0.0:
+    min_value = 0.0
+  if min_value == max_value:
+    if abs(min_value) < 0.000001:
+      max_value = min_value + 1.0
+    elif min_value > 0:
+      max_value = 2 * min_value
+    else:
+      max_value = min_value / 2.0
+
+  sess = session.Session()
+  with sess.as_default():
+    quantize_op = array_ops.quantize_v2(
+        float_tensor,
+        min_value,
+        max_value,
+        dtypes.qint8,
+        mode=quantization_mode,
+        round_mode="HALF_TO_EVEN")
+    qint8_tensor = quantize_op[0].eval()
+    # Updated min-max values should be passed to the next feeding node.
+    min_value = quantize_op[1].eval()
+    max_value = quantize_op[2].eval()
+  shape = tensor_util.TensorShapeProtoToList(input_node.attr["value"]
+                                             .tensor.tensor_shape)
+  qint8_const_node = create_constant_node(
+      qint8_const_name, qint8_tensor,
+      dtypes.qint8,
+      shape=shape)
+  min_node = create_constant_node(min_name, min_value, dtypes.float32)
+  max_node = create_constant_node(max_name, max_value, dtypes.float32)
+  dequantize_node = create_node("Dequantize", input_node.name,
+                                [qint8_const_name, min_name, max_name])
+  set_attr_dtype(dequantize_node, "T", dtypes.quint8)
+  set_attr_string(dequantize_node, "mode", b'SCALED')
+  return [qint8_const_node, min_node, max_node, dequantize_node]
+
+EightbitizeRecursionState = collections.namedtuple(
+    "EightbitizeRecursionState",
+    ["already_visited", "output_node_stack", "merged_with_fake_quant"])
+
+
+class GraphRewriter(object):
+  """Takes a float graph, and rewrites it in quantized form."""
+
+  def __init__(self,
+               input_graph,
+               mode,
+               quantized_input_range,
+               fallback_quantization_range=None,
+               intel_cpu_eightbitize=False):
+    """Sets up the class to rewrite a float graph.
+    Args:
+      input_graph: A float graph to transform.
+      mode: A string controlling how quantization is performed -
+        round, quantize, eightbit, or weights.
+      quantized_input_range: if set, assume the input is
+        quantized and represents the range
+        [quantized_input_range[0], quantized_input_range[1]]
+      fallback_quantization_range: if set, then for nodes where the quantization
+        range can't be inferred from the graph, use the range
+        [fallback_quantization_range[0], fallback_quantization_range[1]) instead
+        of using a RequantizationRange node in the graph.
+    Raises:
+      ValueError: Two nodes with the same name were found in the graph.
+    """
+    self.input_graph = input_graph
+    self.nodes_map = self.create_nodes_map(input_graph)
+    self.output_graph = None
+    self.mode = mode
+    self.intel_cpu_eightbitize = intel_cpu_eightbitize
+    self.final_node_renames  = {}
+    self.quantized_node_dict = {}
+    if quantized_input_range:
+      self.input_range = (quantized_input_range[0], quantized_input_range[1])
+      if self.input_range[0] >= self.input_range[1]:
+        raise ValueError("Invalid quantized_input_range: [%s,%s]" %
+                         self.input_range)
+      if self.mode != "eightbit":
+        raise ValueError(
+            "quantized_input_range can only be specified in eightbit mode")
+    else:
+      self.input_range = None
+
+    if fallback_quantization_range:
+      self.fallback_quantization_range = [
+          fallback_quantization_range[0], fallback_quantization_range[1]
+      ]
+      if (self.fallback_quantization_range[0] >=
+          self.fallback_quantization_range[1]):
+        raise ValueError("Invalid fallback_quantization_range: [%s,%s]" %
+                         self.fallback_quantization_range)
+      if self.mode != "eightbit":
+        raise ValueError("fallback_quantization_range can only be "
+                         "specified in eightbit mode")
+    else:
+      self.fallback_quantization_range = None
+
+    # Data that is valid only during the recursive call to rewrite the graph.
+    self.state = None
+
+  def create_nodes_map(self, graph):
+    """Builds a mapping of node names to their defs from the graph."""
+    nodes_map = {}
+    for node in graph.node:
+      if node.name not in nodes_map.keys():
+        nodes_map[node.name] = node
+      else:
+        raise ValueError("Duplicate node names detected.")
+    return nodes_map
+
+  def rewrite(self, output_node_names):
+    """Triggers rewriting of the float graph.
+    Args:
+      output_node_names: A list of names of the nodes that produce the final
+        results.
+    Returns:
+      A quantized version of the float graph.
+    """
+    self.output_graph = graph_pb2.GraphDef()
+    output_nodes = [
+        self.nodes_map[output_node_name]
+        for output_node_name in output_node_names
+    ]
+    if self.mode == "round":
+      self.already_visited = {}
+      for output_node in output_nodes:
+        self.round_nodes_recursively(output_node)
+    elif self.mode == "quantize":
+      self.already_visited = {}
+      self.already_quantized = {}
+      for output_node in output_nodes:
+        self.quantize_nodes_recursively(output_node)
+    elif self.mode == "eightbit":
+      # When function graph_util.remove_training_nodes remove
+      # "Identity" ops in the graph, it does not replace the
+      # control input properly, so the control input becomes
+      # the regular input. Disable this function until the
+      # the bug is fixed.
+      self.set_input_graph(graph_util.remove_training_nodes(
+          self.input_graph, protected_nodes=output_node_names))
+      output_nodes = [
+          self.nodes_map[output_node_name]
+          for output_node_name in output_node_names
+      ]
+
+      self.state = EightbitizeRecursionState(
+          already_visited={}, output_node_stack=[], merged_with_fake_quant={})
+
+      if self.intel_cpu_eightbitize:
+        # TODO(intel-tf): Enables fused quantized node for intel cpu.
+        for output_node in output_nodes:
+          # Intiailize output_node_stack with output node.
+          # Each element in the stack is a mutable list containing
+          # [parent_node, index_to_parent, quantization_flag, fusion_flag].
+          # In case of root node, make self as parent.
+          self.state.output_node_stack.append(
+              [output_node, None, False, False])
+          self.intel_cpu_eightbitize_nodes_recursively(output_node)
+          self.state.output_node_stack.pop()
+      else:
+        for output_node in output_nodes:
+          self.eightbitize_nodes_recursively(output_node)
+
+      self.state = None
+      if self.input_range:
+        self.add_output_graph_node(
+            create_constant_node("quantized_input_min_value", self.input_range[
+                0], dtypes.float32, []))
+        self.add_output_graph_node(
+            create_constant_node("quantized_input_max_value", self.input_range[
+                1], dtypes.float32, []))
+      if self.fallback_quantization_range:
+        self.add_output_graph_node(
+            create_constant_node("fallback_quantization_min_value",
+                                 self.fallback_quantization_range[0],
+                                 dtypes.float32, []))
+        self.add_output_graph_node(
+            create_constant_node("fallback_quantization_max_value",
+                                 self.fallback_quantization_range[1],
+                                 dtypes.float32, []))
+      if True:
+        self.output_graph = self.remove_redundant_quantization(
+            self.output_graph)
+        self.remove_dead_nodes(output_node_names)
+      self.apply_final_node_renames()
+    elif self.mode == "weights":
+      self.output_graph = self.quantize_weights(self.input_graph,
+                                                b"MIN_COMBINED")
+      self.remove_dead_nodes(output_node_names)
+    elif self.mode == "weights_rounded":
+      self.output_graph = self.quantize_weights(self.input_graph, self.mode)
+      self.remove_dead_nodes(output_node_names)
+    else:
+      print("Bad mode - " + self.mode + ".")
+    return self.output_graph
+
+  def round_nodes_recursively(self, current_node):
+    """The entry point for simple rounding quantization."""
+    if (current_node.name in self.already_visited
+       ) and self.already_visited[current_node.name]:
+      return
+    self.already_visited[current_node.name] = True
+    for input_node_name in current_node.input:
+      input_node_name = node_name_from_input(input_node_name)
+      input_node = self.nodes_map[input_node_name]
+      self.round_nodes_recursively(input_node)
+    nodes_to_quantize = ["Conv2D", "BiasAdd", "MatMul"]
+    if any(current_node.op in s for s in nodes_to_quantize):
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(current_node)
+      new_node.name = current_node.name + "_original"
+      self.add_output_graph_node(new_node)
+      levels = 1 << FLAGS.bitdepth
+      constant_name = current_node.name + "_round_depth"
+      constant_tensor = constant_op.constant(
+          levels, dtype=dtypes.int32, name=constant_name)
+      constant_node = constant_tensor.op.node_def
+      self.add_output_graph_node(constant_node)
+      quantize_node = node_def_pb2.NodeDef()
+      quantize_node.op = "RoundToSteps"
+      quantize_node.name = current_node.name
+      quantize_node.input.extend([current_node.name + "_original"])
+      quantize_node.input.extend([constant_node.name])
+      self.add_output_graph_node(quantize_node)
+    else:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(current_node)
+      self.add_output_graph_node(new_node)
+
+  def quantize_nodes_recursively(self, current_node):
+    """The entry point for quantizing nodes to eight bit and back."""
+    if self.already_visited[current_node.name]:
+      return
+    self.already_visited[current_node.name] = True
+    for input_node_name in current_node.input:
+      input_node_name = node_name_from_input(input_node_name)
+      input_node = self.nodes_map[input_node_name]
+      self.quantize_nodes_recursively(input_node)
+    nodes_to_quantize = ["Conv2D", "BiasAdd", "MatMul"]
+    if any(current_node.op in s for s in nodes_to_quantize):
+      for input_name in current_node.input:
+        input_name = node_name_from_input(input_name)
+        input_node = self.nodes_map[input_name]
+        self.quantize_node(input_node)
+      self.quantize_node(current_node)
+    else:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(current_node)
+      self.add_output_graph_node(new_node)
+
+  def quantize_node(self, input_node):
+    """Handles quantizing a single node."""
+    input_name = input_node.name
+    if input_name in self.already_quantized:
+      return
+    self.already_quantized[input_name] = True
+    original_input_name = input_name + "_original"
+    reshape_name = input_name + "_reshape"
+    reshape_dims_name = input_name + "_reshape_dims"
+    max_name = input_name + "_max"
+    min_name = input_name + "_min"
+    dims_name = input_name + "_dims"
+    quantize_name = input_name + "_quantize"
+    dequantize_name = input_name
+    original_input_node = node_def_pb2.NodeDef()
+    original_input_node.CopyFrom(input_node)
+    original_input_node.name = original_input_name
+    self.add_output_graph_node(original_input_node)
+    reshape_dims_node = create_constant_node(reshape_dims_name, -1,
+                                             dtypes.int32, [1])
+    self.add_output_graph_node(reshape_dims_node)
+    reshape_node = create_node("Reshape", reshape_name,
+                               [original_input_name, reshape_dims_name])
+    set_attr_dtype(reshape_node, "T", dtypes.float32)
+    self.add_output_graph_node(reshape_node)
+    dims_node = create_constant_node(dims_name, 0, dtypes.int32, [1])
+    self.add_output_graph_node(dims_node)
+    max_node = create_node("Max", max_name, [reshape_name, dims_name])
+    set_attr_dtype(max_node, "T", dtypes.float32)
+    set_attr_bool(max_node, "keep_dims", False)
+    self.add_output_graph_node(max_node)
+    min_node = create_node("Min", min_name, [reshape_name, dims_name])
+    set_attr_dtype(min_node, "T", dtypes.float32)
+    set_attr_bool(min_node, "keep_dims", False)
+    self.add_output_graph_node(min_node)
+    quantize_node = create_node("Quantize", quantize_name,
+                                [original_input_name, min_name, max_name])
+    set_attr_dtype(quantize_node, "T", dtypes.quint8)
+    set_attr_string(quantize_node, "mode", b"MIN_FIRST")
+    self.add_output_graph_node(quantize_node)
+    dequantize_node = create_node("Dequantize", dequantize_name,
+                                  [quantize_name, min_name, max_name])
+    set_attr_dtype(dequantize_node, "T", dtypes.quint8)
+    set_attr_string(dequantize_node, "mode", b"MIN_FIRST")
+    self.add_output_graph_node(dequantize_node)
+
+  def should_merge_with_fake_quant_node(self):
+    """Should the current node merge with self.state.output_node_stack[-1]?"""
+    if not self.state.output_node_stack:
+      return False
+    top = self.state.output_node_stack[-1]
+    return top[1] == 0 and top[0].op in ["FakeQuantWithMinMaxVars"]
+
+  def should_quantize_const(self, node):
+    if not self.state.output_node_stack:
+      return False
+    top = self.state.output_node_stack[-1]
+    if not top[2]:
+      return False
+    dtype = dtypes.as_dtype(node.attr["dtype"].type)
+    assert dtype == dtypes.float32, (
+        "Failed to quantized constant %s of type %s" % (node.name, dtype))
+    return True
+
+  def eightbitize_nodes_recursively(self, current_node):
+    """The entry point for transforming a graph into full eight bit."""
+    if current_node.name in self.state.already_visited:
+      if (self.should_merge_with_fake_quant_node() or
+          current_node.name in self.state.merged_with_fake_quant):
+        raise ValueError("Unsupported graph structure: output of node %s "
+                         "is processed by a FakeQuant* node and should have "
+                         "no other outputs.", current_node.name)
+      return
+    self.state.already_visited[current_node.name] = True
+
+    for i, input_node_name in enumerate(current_node.input):
+      quantize_input = False
+      if current_node.op in ("MatMul", "Conv2D", "BiasAdd", "MaxPool",
+                             "AvgPool", "Relu", "Relu6",
+                             "BatchNormWithGlobalNormalization"):
+        quantize_input = True
+      elif current_node.op == "Concat" and i > 0:
+        quantize_input = (
+            dtypes.as_dtype(current_node.attr["T"].type) == dtypes.float32)
+      elif current_node.op == "Reshape" and i == 0:
+        quantize_input = (
+            dtypes.as_dtype(current_node.attr["T"].type) == dtypes.float32)
+
+      self.state.output_node_stack.append((current_node, i, quantize_input))
+
+      input_node_name = node_name_from_input(input_node_name)
+      input_node = self.nodes_map[input_node_name]
+      self.eightbitize_nodes_recursively(input_node)
+
+      self.state.output_node_stack.pop()
+
+    if current_node.op == "MatMul":
+      self.eightbitize_mat_mul_node(current_node)
+    elif current_node.op == "Conv2D":
+      self.eightbitize_conv_node(current_node)
+    elif current_node.op == "BiasAdd":
+      self.eightbitize_bias_add_node(current_node)
+    elif current_node.op == "MaxPool" or current_node.op == "AvgPool":
+      self.eightbitize_single_input_tensor_node(current_node,
+                                                self.add_pool_function)
+    elif current_node.op == "Relu" or current_node.op == "Relu6":
+      self.eightbitize_single_input_tensor_node(current_node,
+                                                self.add_relu_function)
+    elif (current_node.op == "Concat" and
+          dtypes.as_dtype(current_node.attr["T"].type) == dtypes.float32):
+      self.eightbitize_concat_node(current_node)
+    elif current_node.op == "BatchNormWithGlobalNormalization":
+      self.eightbitize_batch_norm_node(current_node)
+    elif (current_node.op == "Reshape" and
+          dtypes.as_dtype(current_node.attr["T"].type) == dtypes.float32):
+      self.eightbitize_reshape_node(current_node)
+    elif (self.input_range and
+          current_node.op in ("Placeholder", "PlaceholderV2")):
+      self.eightbitize_placeholder_node(current_node)
+    elif current_node.op == "FakeQuantWithMinMaxVars":
+      # It will have been merged into the underlying node.
+      pass
+    elif current_node.op == "Const":
+      if self.should_quantize_const(current_node):
+        for n in quantize_weight_eightbit(current_node, b"MIN_FIRST"):
+          self.add_output_graph_node(n)
+      else:
+        new_node = node_def_pb2.NodeDef()
+        new_node.CopyFrom(current_node)
+        self.add_output_graph_node(new_node)
+
+    ###################################################################
+    # Note: if more cases are added here, you may need to update the op
+    # name lists in the loop over children at the start of the function.
+    ###################################################################
+    else:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(current_node)
+      self.add_output_graph_node(new_node)
+
+    if (self.should_merge_with_fake_quant_node() and
+        current_node.name not in self.state.merged_with_fake_quant):
+      raise ValueError(
+          "FakeQuant* node %s failed to merge with node %s of type %s" %
+          (self.state.output_node_stack[-1][0], current_node.name,
+           current_node.op))
+
+  # TODO(intel-tf): Quantized Conv2D could be fused with few other succeeding
+  # ops. Current support is for BiasAdd and Relu. Future implementation will
+  # include:
+  # (i)   Conv2D + {BiasAdd} + Relu + Add + Relu
+  # (ii)  Conv2D + {BiasAdd} + Relu + Add
+  # (ii)  Conv2D + {BiasAdd} + Add + Relu
+  # (iii) Conv2D + {BiasAdd} + Add
+  def intel_cpu_eightbitize_conv_node(self, original_node, bias_node=None,
+                                      bias_add_name=None, add_node_name=None,
+                                      relu_node_name=None):
+    """Replaces a Conv2D node with the eight bit equivalent sub-graph."""
+    all_input_names = self.add_eightbit_prologue_nodes(original_node)
+    control_input_names = []
+    real_input_names = []
+    for input_name in all_input_names:
+      if input_name[0] == '^':
+        control_input_names.append(input_name)
+      else:
+        real_input_names.append(input_name)
+
+    if bias_node and add_node_name and relu_node_name:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(bias_node)
+      self.add_output_graph_node(new_node)
+      all_input_names = real_input_names[:2] + [bias_node.name] + \
+          real_input_names[2:] + [add_node_name] + control_input_names
+      quantized_conv_name = original_node.name + "_eightbit_quantized_conv"
+      quantized_conv_node = create_node("QuantizedConv2DWithBiasSumAndRelu",
+                              quantized_conv_name, all_input_names)
+    elif bias_node and (not add_node_name) and relu_node_name:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(bias_node)
+      self.add_output_graph_node(new_node)
+      all_input_names = real_input_names[:2] + [bias_node.name] + \
+          real_input_names[2:] + control_input_names
+      quantized_conv_name = original_node.name + "_eightbit_quantized_conv"
+      quantized_conv_node = create_node("QuantizedConv2DWithBiasAndRelu",
+                              quantized_conv_name, all_input_names)
+    elif bias_node and bias_add_name  and \
+        (not add_node_name) and (not relu_node_name):
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(bias_node)
+      self.add_output_graph_node(new_node)
+      all_input_names = real_input_names[:2] + [bias_node.name] + \
+          real_input_names[2:] + control_input_names
+      quantized_conv_name = original_node.name + "_eightbit_quantized_conv"
+      quantized_conv_node = create_node("QuantizedConv2DWithBias",
+                              quantized_conv_name, all_input_names) 
+    else:
+      quantized_conv_name = original_node.name + "_eightbit_quantized_conv"
+      quantized_conv_node = create_node("QuantizedConv2D", quantized_conv_name,
+                                        all_input_names)
+    copy_attr(quantized_conv_node, "strides", original_node.attr["strides"])
+    copy_attr(quantized_conv_node, "padding", original_node.attr["padding"])
+    copy_attr(quantized_conv_node, "dilations", original_node.attr["dilations"])
+    set_attr_dtype(quantized_conv_node, "Tinput", dtypes.quint8)
+    set_attr_dtype(quantized_conv_node, "Tfilter", dtypes.qint8)
+    set_attr_dtype(quantized_conv_node, "out_type", dtypes.qint32)
+    self.add_output_graph_node(quantized_conv_node)
+    quantize_down_name = self.add_quantize_down_nodes(original_node,
+                                                      quantized_conv_name)
+    if bias_node and relu_node_name:
+      self.add_dequantize_result_node(quantize_down_name, relu_node_name)
+    elif bias_node and bias_add_name and \
+        (not add_node_name) and (not relu_node_name):
+      self.add_dequantize_result_node(quantize_down_name, bias_add_name)
+    else:
+      self.add_dequantize_result_node(quantize_down_name, original_node.name)
+
+  # TODO(intel-tf): To check whether Conv2D is fed by relu directly or via
+  # pooling ops. This is required as intel cpu requires input tensor for Conv2D
+  # to be non-negative.
+  def intel_cpu_find_relu_recursively(self, current_node):
+    """Helper function to check if Conv2D is fed by Relu."""
+    if current_node.op == "Relu" or current_node.op == "Relu6":
+      return True
+    else:
+      first_input_node_name = node_name_from_input(current_node.input[0])
+      input_node = self.nodes_map[first_input_node_name]
+      if input_node.op in ("ConcatV2", "MaxPool", "AvgPool", "Relu", "Relu6"):
+        return self.intel_cpu_find_relu_recursively(input_node)
+      else:
+        return False
+
+  # def intel_cpu_find_switch_input_any(self, current_node):
+  #   should_quantize_concat = True
+  #   for input_name in current_node.input:
+  #     if self.nodes_map[node_name_from_input(input_name)].op == "Switch":
+  #       should_quantize_concat = False
+  #       break
+  #   return should_quantize_concat
+
+  # TODO(intel-tf): We leave the output graph partially quantized for
+  # intel cpu. Current quantization support is for Conv2D and its fusion.
+  # More quantized operations will be included as more implementations are
+  # completed.
+  def intel_cpu_eightbitize_nodes_recursively(self, current_node):
+    """The entry point for transforming a graph into full eight bit."""
+    if current_node.name in self.state.already_visited:
+      if (self.should_merge_with_fake_quant_node() or
+          current_node.name in self.state.merged_with_fake_quant):
+        raise ValueError("Unsupported graph structure: output of node %s "
+                         "is processed by a FakeQuant* node and should have "
+                         "no other outputs.", current_node.name)
+      return
+    self.state.already_visited[current_node.name] = True
+    quantize_input, should_quantize_conv, \
+        fuse_with_conv = (False, False, False)
+
+    if current_node.op == "Conv2D":
+      should_quantize_conv = self.intel_cpu_find_relu_recursively(current_node)
+    if current_node.op == "ConcatV2":
+      should_quantize_concat = not ('map/while' in current_node.name)
+      # should_quantize_concat = self.intel_cpu_find_switch_input_any(current_node)
+
+    inputs = list(enumerate(current_node.input))
+    if current_node.op in ("AddN", "Add"):
+      inputs = reversed(inputs)
+
+    for i, input_node_name in inputs:
+      input_node_name = node_name_from_input(input_node_name)
+      input_node = self.nodes_map[input_node_name]
+
+      if should_quantize_conv and i == 1 and input_node.op == "Const":
+        quantize_input = True
+
+      self.state.output_node_stack.append([current_node, i, quantize_input,
+                                           fuse_with_conv])
+      self.intel_cpu_eightbitize_nodes_recursively(input_node)
+      self.state.output_node_stack.pop()
+
+    if current_node.op == "Conv2D" and should_quantize_conv and quantize_input:
+      # match pattern for fusion with bias and relu
+      grand_parent, parent = self.state.output_node_stack[-2:]
+      if parent[0].op == "BiasAdd" and \
+        (grand_parent[0].op == "Relu" or grand_parent[0].op == "Relu6"):
+        self.state.output_node_stack[-2][3] = True # BiasAdd to be fused
+        self.state.output_node_stack[-3][3] = True # Relu to be fused
+        bias_node_name = node_name_from_input(parent[0].input[1])
+        bias_node = self.nodes_map[bias_node_name]
+        self.intel_cpu_eightbitize_conv_node(current_node, bias_node, None,
+                                             None, grand_parent[0].name)
+      elif parent[0].op == "BiasAdd" and grand_parent[0].op in ("AddN", "Add"):
+        grand_grand_parent = self.state.output_node_stack[-3]
+        if grand_grand_parent[0].op in ("Relu", "Relu6") \
+            and (not self.state.output_node_stack[-3][3]) \
+            and (not self.state.output_node_stack[-4][3]):
+          self.state.output_node_stack[-2][3] = True # BiasAdd to be fused
+          self.state.output_node_stack[-3][3] = True # AddN to be fused
+          self.state.output_node_stack[-4][3] = True # Relu to be fused
+          bias_node_name = node_name_from_input(parent[0].input[1])
+          bias_node = self.nodes_map[bias_node_name]
+          add_node_name = node_name_from_input(grand_parent[0].input[0])
+          self.intel_cpu_eightbitize_conv_node(current_node, bias_node, None,
+                                               add_node_name,
+                                               grand_grand_parent[0].name)
+        elif (not self.state.output_node_stack[-2][3]): # Fuse BiasAdd then 
+          self.state.output_node_stack[-2][3] = True # BiasAdd to be fused
+          bias_node_name = node_name_from_input(parent[0].input[1])
+          bias_node = self.nodes_map[bias_node_name]
+          self.intel_cpu_eightbitize_conv_node(current_node, bias_node,
+                                               parent[0].name)
+        else:
+          self.intel_cpu_eightbitize_conv_node(current_node)
+      elif parent[0].op == "BiasAdd" and \
+          (not self.state.output_node_stack[-2][3]):
+        self.state.output_node_stack[-2][3] = True # BiasAdd to be fused
+        bias_node_name = node_name_from_input(parent[0].input[1])
+        bias_node = self.nodes_map[bias_node_name]
+        self.intel_cpu_eightbitize_conv_node(current_node, bias_node,
+                                             parent[0].name)
+      else:
+        self.intel_cpu_eightbitize_conv_node(current_node)
+    elif current_node.op == "BiasAdd" and \
+        self.state.output_node_stack[-1][3] == True:
+      pass # This op is already processed by fused quantization
+    elif (current_node.op == "Relu" or current_node.op == "Relu6")  \
+        and self.state.output_node_stack[-1][3] == True:
+      pass # This op is already processed by fused quantization
+    elif current_node.op in ("AddN", "Add") and \
+        self.state.output_node_stack[-1][3] == True:
+      pass # AddN op is already processed by fused quatization
+    elif current_node.op == "MaxPool" or current_node.op == "AvgPool":
+      self.eightbitize_single_input_tensor_node(current_node,
+                                                self.add_pool_function)
+    elif (current_node.op == "ConcatV2" and should_quantize_concat and
+        dtypes.as_dtype(current_node.attr["T"].type) == dtypes.float32):
+      self.eightbitize_concatv2_node(current_node)
+    elif current_node.op == "Const":
+      parent = self.state.output_node_stack[-1]
+      if parent[0].op == "Conv2D" and parent[2]:
+        for n in intel_cpu_quantize_weight_eightbit(current_node, b"SCALED"):
+          self.add_output_graph_node(n)
+      elif parent[0].op == "BiasAdd" and \
+           self.state.output_node_stack[-2][3]:
+        pass # This constant is already process by fused quantization
+      else:
+        new_node = node_def_pb2.NodeDef()
+        new_node.CopyFrom(current_node)
+        self.add_output_graph_node(new_node)
+    else:
+      new_node = node_def_pb2.NodeDef()
+      new_node.CopyFrom(current_node)
+      self.add_output_graph_node(new_node)
+
+    if (self.should_merge_with_fake_quant_node() and
+        current_node.name not in self.state.merged_with_fake_quant):
+      raise ValueError(
+          "FakeQuant* node %s failed to merge with node %s of type %s" %
+          (self.state.output_node_stack[-1][0], current_node.name,
+           current_node.op))
+
+  def add_eightbit_prologue_nodes(self, original_node):
+    """Adds input conversion nodes to handle quantizing the underlying node."""
+    namespace_prefix = original_node.name + "_eightbit"
+
+    # Use the name of the first input as the control input name 
+    # for reshape_dim and reduction_dim to slove the different frame issue
+    # in quantized graph
+    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(
+        namespace_prefix, node_name_from_input(original_node.input[0]))
+    input_names = []
+    min_max_names = []
+    for original_input_name in original_node.input:
+      # Do not quantize control input
+      if original_input_name[0] == '^':
+        continue
+      quantize_input_name, min_input_name, max_input_name = (
+          self.eightbitize_input_to_node(namespace_prefix, original_input_name,
+                                         reshape_dims_name,
+                                         reduction_dims_name))
+      input_names.append(quantize_input_name)
+      min_max_names.append(min_input_name)
+      min_max_names.append(max_input_name)
+    all_input_names = []
+    all_input_names.extend(input_names)
+    all_input_names.extend(min_max_names)
+
+    # add back control input name
+    for original_input_name in original_node.input:
+      if original_input_name[0] == '^':
+        all_input_names.append(original_input_name)
+
+    return all_input_names
+
+  def add_common_quantization_nodes(self, namespace_prefix, control_input_name=None):
+    """Builds constant nodes needed for quantization of inputs."""
+    reshape_dims_name = namespace_prefix + "_reshape_dims"
+    reduction_dims_name = namespace_prefix + "_reduction_dims"
+
+    reshape_dims_node = create_constant_node(reshape_dims_name, -1,
+                                             dtypes.int32, [1])
+    if control_input_name:
+      reshape_dims_node.input.append("^" + control_input_name)
+    self.add_output_graph_node(reshape_dims_node)
+    reduction_dims_node = create_constant_node(reduction_dims_name, 0,
+                                               dtypes.int32, [1])
+    if control_input_name:
+      reduction_dims_node.input.append("^" + control_input_name)
+    self.add_output_graph_node(reduction_dims_node)
+    return reshape_dims_name, reduction_dims_name
+
+  def eightbitize_input_to_node(self, namespace_prefix, original_input_name,
+                                reshape_dims_name, reduction_dims_name):
+    """Takes one float input to an op, and converts it to quantized form."""
+    unique_input_name = unique_node_name_from_input(original_input_name)
+    if unique_input_name in self.quantized_node_dict:
+        quantized_tuple = self.quantized_node_dict[unique_input_name];
+        return quantized_tuple[0], quantized_tuple[1], quantized_tuple[2]
+
+    reshape_input_name = namespace_prefix + "_reshape_" + unique_input_name
+    min_input_name = namespace_prefix + "_min_" + unique_input_name
+    max_input_name = namespace_prefix + "_max_" + unique_input_name
+    quantize_input_name = namespace_prefix + "_quantize_" + unique_input_name
+    reshape_input_node = create_node("Reshape", reshape_input_name,
+                                     [original_input_name, reshape_dims_name])
+    set_attr_dtype(reshape_input_node, "T", dtypes.float32)
+    self.add_output_graph_node(reshape_input_node)
+    min_input_node = create_node("Min", min_input_name,
+                                 [reshape_input_name, reduction_dims_name])
+    set_attr_dtype(min_input_node, "T", dtypes.float32)
+    set_attr_bool(min_input_node, "keep_dims", False)
+    self.add_output_graph_node(min_input_node)
+    max_input_node = create_node("Max", max_input_name,
+                                 [reshape_input_name, reduction_dims_name])
+    set_attr_dtype(max_input_node, "T", dtypes.float32)
+    set_attr_bool(max_input_node, "keep_dims", False)
+    self.add_output_graph_node(max_input_node)
+    quantize_input_node = create_node(
+        "QuantizeV2", quantize_input_name,
+        [original_input_name, min_input_name, max_input_name])
+    set_attr_dtype(quantize_input_node, "T", dtypes.quint8)
+    set_attr_string(quantize_input_node, "mode",
+                    b"SCALED" if self.intel_cpu_eightbitize else  b"MIN_FIRST")
+    set_attr_string(quantize_input_node, "round_mode",
+                    b"HALF_TO_EVEN" if self.intel_cpu_eightbitize
+                    else  b"HALF_AWAY_FROM_ZERO")
+    self.add_output_graph_node(quantize_input_node)
+    min_output_name = quantize_input_name + ":1"
+    max_output_name = quantize_input_name + ":2"
+
+    self.quantized_node_dict[unique_input_name] = (quantize_input_name, 
+                                        min_output_name, max_output_name)
+    return quantize_input_name, min_output_name, max_output_name
+
+  def add_quantize_down_nodes(self, original_node, quantized_output_name):
+    quantized_outputs = [
+        quantized_output_name, quantized_output_name + ":1",
+        quantized_output_name + ":2"
+    ]
+    min_max_inputs = None
+    if self.should_merge_with_fake_quant_node():
+      # Use the inputs to the FakeQuantWithMinMaxVars node as the inputs to
+      # Requantize.
+      fake_quant_node = self.state.output_node_stack[-1][0]
+      min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]
+      assert original_node.name not in self.state.merged_with_fake_quant
+      self.state.merged_with_fake_quant[original_node.name] = True
+    elif self.fallback_quantization_range:
+      min_max_inputs = [
+          "fallback_quantization_min_value:0",
+          "fallback_quantization_max_value:0"
+      ]
+    else:
+      # Add a RequantizationRange node for finding the min and max values.
+      requant_range_node = create_node(
+          "RequantizationRange", original_node.name + "_eightbit_requant_range",
+          quantized_outputs)
+      set_attr_dtype(requant_range_node, "Tinput", dtypes.qint32)
+      self.add_output_graph_node(requant_range_node)
+      min_max_inputs = [
+          requant_range_node.name + ":0", requant_range_node.name + ":1"
+      ]
+    requantize_node = create_node("Requantize",
+                                  original_node.name + "_eightbit_requantize",
+                                  quantized_outputs + min_max_inputs)
+    set_attr_dtype(requantize_node, "Tinput", dtypes.qint32)
+    set_attr_dtype(requantize_node, "out_type", dtypes.quint8)
+    self.add_output_graph_node(requantize_node)
+    return requantize_node.name
+
+  def add_dequantize_result_node(self,
+                                 quantized_output_name,
+                                 original_node_name,
+                                 min_tensor_index=1):
+    min_max_inputs = [
+        "%s:%s" % (quantized_output_name, min_tensor_index),
+        "%s:%s" % (quantized_output_name, (min_tensor_index + 1))
+    ]
+    dequantize_name = original_node_name
+    if self.should_merge_with_fake_quant_node():
+      fake_quant_node = self.state.output_node_stack[-1][0]
+      if original_node_name not in self.state.merged_with_fake_quant:
+        min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]
+        self.state.merged_with_fake_quant[original_node_name] = True
+      dequantize_name = fake_quant_node.name
+
+    dequantize_node = create_node(
+        "Dequantize", dequantize_name,
+        [quantized_output_name, min_max_inputs[0], min_max_inputs[1]])
+    set_attr_dtype(dequantize_node, "T", dtypes.quint8)
+    set_attr_string(dequantize_node, "mode", b"MIN_FIRST")
+    self.add_output_graph_node(dequantize_node)
+
+  def eightbitize_mat_mul_node(self, original_node):
+    """Replaces a MatMul node with the eight bit equivalent sub-graph."""
+    quantized_mat_mul_name = original_node.name + "_eightbit_quantized_mat_mul"
+    all_input_names = self.add_eightbit_prologue_nodes(original_node)
+    quantized_mat_mul_node = create_node("QuantizedMatMul",
+                                         quantized_mat_mul_name,
+                                         all_input_names)
+    set_attr_dtype(quantized_mat_mul_node, "T1", dtypes.quint8)
+    set_attr_dtype(quantized_mat_mul_node, "T2", dtypes.quint8)
+    set_attr_dtype(quantized_mat_mul_node, "Toutput", dtypes.qint32)
+    copy_attr(quantized_mat_mul_node, "transpose_a",
+              original_node.attr["transpose_a"])
+    copy_attr(quantized_mat_mul_node, "transpose_b",
+              original_node.attr["transpose_b"])
+    self.add_output_graph_node(quantized_mat_mul_node)
+    quantize_down_name = self.add_quantize_down_nodes(original_node,
+                                                      quantized_mat_mul_name)
+    self.add_dequantize_result_node(quantize_down_name, original_node.name)
+
+  def eightbitize_conv_node(self, original_node):
+    """Replaces a Conv2D node with the eight bit equivalent sub-graph."""
+    all_input_names = self.add_eightbit_prologue_nodes(original_node)
+    quantized_conv_name = original_node.name + "_eightbit_quantized_conv"
+    quantized_conv_node = create_node("QuantizedConv2D", quantized_conv_name,
+                                      all_input_names)
+    copy_attr(quantized_conv_node, "strides", original_node.attr["strides"])
+    copy_attr(quantized_conv_node, "padding", original_node.attr["padding"])
+    set_attr_dtype(quantized_conv_node, "Tinput", dtypes.quint8)
+    set_attr_dtype(quantized_conv_node, "Tfilter", dtypes.quint8)
+    set_attr_dtype(quantized_conv_node, "out_type", dtypes.qint32)
+    self.add_output_graph_node(quantized_conv_node)
+    quantize_down_name = self.add_quantize_down_nodes(original_node,
+                                                      quantized_conv_name)
+    self.add_dequantize_result_node(quantize_down_name, original_node.name)
+
+  def eightbitize_bias_add_node(self, original_node):
+    """Replaces a BiasAdd node with the eight bit equivalent sub-graph."""
+    quantized_bias_add_name = (
+        original_node.name + "_eightbit_quantized_bias_add")
+    all_input_names = self.add_eightbit_prologue_nodes(original_node)
+    quantized_bias_add_node = create_node("QuantizedBiasAdd",
+                                          quantized_bias_add_name,
+                                          all_input_names)
+    set_attr_dtype(quantized_bias_add_node, "T1", dtypes.quint8)
+    set_attr_dtype(quantized_bias_add_node, "T2", dtypes.quint8)
+    set_attr_dtype(quantized_bias_add_node, "out_type", dtypes.qint32)
+    self.add_output_graph_node(quantized_bias_add_node)
+    quantize_down_name = self.add_quantize_down_nodes(original_node,
+                                                      quantized_bias_add_name)
+    self.add_dequantize_result_node(quantize_down_name, original_node.name)
+
+  def eightbitize_single_input_tensor_node(self, original_node,
+                                           add_op_function):
+    """Replaces a single-tensor node with the eight bit equivalent sub-graph.
+    Converts a node like this:
+       Shape(f)   Input(f)
+         |          |
+         +--------v v
+                Operation
+                    |
+                    v
+                   (f)
+     Into a quantized equivalent:
+                    Input(f)              ReshapeDims
+                       +------v v-------------+
+                       |    Reshape
+                       |      |
+                       |      |          ReductionDims
+                       |      +-----+         |
+                       |      | +---c---------+
+                       |      v v   v v-------+
+                       |      Min   Max
+                       |  +----+      |
+                       v  v  v--------+
+                      Quantize
+                          |
+                          v
+                   QuantizedOperation
+                      |   |   |
+                      v   v   v
+                      Dequantize
+                          |
+                          v
+                         (f)
+    Args:
+      original_node: Float node to be converted.
+      add_op_function: Function to create the actual node.
+    Returns:
+      Subgraph representing the quantized version of the original node.
+    """
+    quantized_op_name = original_node.name + "_eightbit_quantized"
+    quantized_op_type = "Quantized" + original_node.op
+    all_input_names = self.add_eightbit_prologue_nodes(original_node)
+    quantized_op_node = create_node(quantized_op_type, quantized_op_name,
+                                    all_input_names)
+    add_op_function(original_node, quantized_op_node)
+    self.add_output_graph_node(quantized_op_node)
+    self.add_dequantize_result_node(quantized_op_name, original_node.name)
+
+  def add_pool_function(self, original_node, quantized_op_node):
+    set_attr_dtype(quantized_op_node, "T", dtypes.quint8)
+    copy_attr(quantized_op_node, "ksize", original_node.attr["ksize"])
+    copy_attr(quantized_op_node, "strides", original_node.attr["strides"])
+    copy_attr(quantized_op_node, "padding", original_node.attr["padding"])
+
+  def add_relu_function(self, unused_arg_node, quantized_op_node):
+    set_attr_dtype(quantized_op_node, "Tinput", dtypes.quint8)
+
+  def eightbitize_concat_node(self, original_node):
+    """Replaces a Concat node with the eight bit equivalent sub-graph.
+    Converts a node like this:
+       Shape(f)   Input0(f)   Input1(f)
+         |          |            |
+         +--------v v v----------+
+                  Concat
+                    |
+                    v
+                   (f)
+     Into a quantized equivalent:
+       Shape(f)     Input0(f)             ReshapeDims                  Input1(f)
+         |             +------v v--------------+------------------v v------+
+         |             |    Reshape                             Reshape    |
+         |             |      |                                     |      |
+         |             |      |           ReductionDims             |      |
+         |             |      +------+         |           +--------+      |
+         |             |      |  +---c---------+-----------c-----+  |      |
+         |             |      +v v   v v-------+---------v v     v v+      |
+         |             |       Min   Max                 Min     Max       |
+         |             |  +----+      |                   |       +-----+  |
+         |             v  v  v--------+                   +----------v  v  v
+         |            Quantize                                       Quantize
+         |                +------------------+   +----------------------+
+         +-------------------------------+   |   |
+                                         v   v   v
+                                      QuantizedConcat
+                                         |   |   |
+                                         v   v   v
+                                        Dequantize
+                                             |
+                                             v
+                                            (f)
+    Args:
+      original_node: Float node to be converted.
+    Returns:
+      Subgraph representing the quantized version of the original node.
+    """
+    namespace_prefix = original_node.name + "_eightbit"
+    quantized_concat_name = namespace_prefix + "_quantized_concat"
+    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(
+        namespace_prefix, node_name_from_input(original_node.input[1]))
+    shape_input_name = original_node.input[0]
+    original_inputs = original_node.input[1:]
+    input_names = []
+    min_names = []
+    max_names = []
+    for original_input_name in original_inputs:
+      quantize_input_name, min_input_name, max_input_name = (
+          self.eightbitize_input_to_node(namespace_prefix, original_input_name,
+                                         reshape_dims_name,
+                                         reduction_dims_name))
+      input_names.append(quantize_input_name)
+      min_names.append(min_input_name)
+      max_names.append(max_input_name)
+    all_input_names = [shape_input_name]
+    all_input_names.extend(input_names)
+    all_input_names.extend(min_names)
+    all_input_names.extend(max_names)
+    quantized_concat_node = create_node("QuantizedConcat",
+                                        quantized_concat_name, all_input_names)
+    set_attr_int(quantized_concat_node, "N", len(original_inputs))
+    set_attr_dtype(quantized_concat_node, "T", dtypes.quint8)
+    self.add_output_graph_node(quantized_concat_node)
+    self.add_dequantize_result_node(quantized_concat_name, original_node.name)
+
+  def eightbitize_concatv2_node(self, original_node):
+    """
+    Args:
+      original_node: Float node to be converted.
+    Returns:
+      Subgraph representing the quantized version of the original node.
+    """
+    namespace_prefix = original_node.name + "_eightbit"
+    quantized_concat_name = namespace_prefix + "_quantized_concatv2"
+    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(
+        namespace_prefix, node_name_from_input(original_node.input[-1]))
+    num_input = len(original_node.input)
+    shape_input_name = original_node.input[num_input-1]
+    original_inputs = original_node.input[0:num_input-1]
+    input_names = []
+    min_names = []
+    max_names = []
+    for original_input_name in original_inputs:
+      quantize_input_name, min_input_name, max_input_name = (
+          self.eightbitize_input_to_node(namespace_prefix, original_input_name,
+                                         reshape_dims_name,
+                                         reduction_dims_name))
+      input_names.append(quantize_input_name)
+      min_names.append(min_input_name)
+      max_names.append(max_input_name)
+    all_input_names = input_names
+    all_input_names.append(shape_input_name)
+    all_input_names.extend(min_names)
+    all_input_names.extend(max_names)
+    quantized_concat_node = create_node("QuantizedConcatV2",
+                                        quantized_concat_name, all_input_names)
+    set_attr_int(quantized_concat_node, "N", len(original_inputs))
+    set_attr_dtype(quantized_concat_node, "T", dtypes.quint8)
+    self.add_output_graph_node(quantized_concat_node)
+    self.add_dequantize_result_node(quantized_concat_name, original_node.name)
+
+  def eightbitize_placeholder_node(self, current_node):
+    """Replaces a placeholder node with a quint8 placeholder node+dequantize."""
+    name = current_node.name
+
+    # Convert the placeholder into a quantized type.
+    output_node = node_def_pb2.NodeDef()
+    output_node.CopyFrom(current_node)
+    set_attr_dtype(output_node, "dtype", dtypes.quint8)
+    output_node.name += "_original_input"
+    self.add_output_graph_node(output_node)
+
+    # Add a dequantize to convert back to float.
+    dequantize_node = create_node("Dequantize", name, [
+        output_node.name, "quantized_input_min_value",
+        "quantized_input_max_value"
+    ])
+    set_attr_dtype(dequantize_node, "T", dtypes.quint8)
+    set_attr_string(dequantize_node, "mode", b"MIN_FIRST")
+    self.add_output_graph_node(dequantize_node)
+
+    # For the descent over the graph to work, the dequantize node must be named
+    # current_node.name.  However, for the feeding of the graph to work, the
+    # placeholder must have the name current_node.name; so record a final set
+    # of renames to apply after all processing has been done.
+    self.final_node_renames[output_node.name] = name
+    self.final_node_renames[dequantize_node.name] = name + "_dequantize"
+
+  def eightbitize_reshape_node(self, original_node):
+    """Replaces a Reshape node with the eight bit equivalent sub-graph.
+    Args:
+      original_node: Float node to be converted.
+    Returns:
+      Subgraph representing the quantized version of the original node.
+    """
+    namespace_prefix = original_node.name + "_eightbit"
+    quantized_reshape_name = namespace_prefix + "_quantized_reshape"
+    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(
+        namespace_prefix, node_name_from_input(original_node.input[0]))
+    shape_input_name = original_node.input[1]
+    quantize_input_name, min_input_name, max_input_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_node.input[0],
+                                       reshape_dims_name, reduction_dims_name))
+    quantized_reshape_node = create_node(
+        "QuantizedReshape", quantized_reshape_name,
+        [quantize_input_name, shape_input_name, min_input_name, max_input_name])
+    set_attr_dtype(quantized_reshape_node, "T", dtypes.quint8)
+    self.add_output_graph_node(quantized_reshape_node)
+    self.add_dequantize_result_node(quantized_reshape_name, original_node.name)
+
+  def eightbitize_batch_norm_node(self, original_node):
+    """Replaces a MatMul node with the eight bit equivalent sub-graph."""
+    namespace_prefix = original_node.name + "_eightbit"
+    original_input_name = original_node.input[0]
+    original_mean_name = original_node.input[1]
+    original_variance_name = original_node.input[2]
+    original_beta_name = original_node.input[3]
+    original_gamma_name = original_node.input[4]
+    quantized_batch_norm_name = namespace_prefix + "_quantized_batch_norm"
+
+    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(
+        namespace_prefix, node_name_from_input(original_input_name))
+    quantize_input_name, min_input_name, max_input_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_input_name,
+                                       reshape_dims_name, reduction_dims_name))
+    quantize_mean_name, min_mean_name, max_mean_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_mean_name,
+                                       reshape_dims_name, reduction_dims_name))
+    quantize_variance_name, min_variance_name, max_variance_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_variance_name,
+                                       reshape_dims_name, reduction_dims_name))
+    quantize_beta_name, min_beta_name, max_beta_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_beta_name,
+                                       reshape_dims_name, reduction_dims_name))
+    quantize_gamma_name, min_gamma_name, max_gamma_name = (
+        self.eightbitize_input_to_node(namespace_prefix, original_gamma_name,
+                                       reshape_dims_name, reduction_dims_name))
+    quantized_batch_norm_node = create_node(
+        "QuantizedBatchNormWithGlobalNormalization", quantized_batch_norm_name,
+        [
+            quantize_input_name, min_input_name, max_input_name,
+            quantize_mean_name, min_mean_name, max_mean_name,
+            quantize_variance_name, min_variance_name, max_variance_name,
+            quantize_beta_name, min_beta_name, max_beta_name,
+            quantize_gamma_name, min_gamma_name, max_gamma_name
+        ])
+    set_attr_dtype(quantized_batch_norm_node, "Tinput", dtypes.quint8)
+    set_attr_dtype(quantized_batch_norm_node, "out_type", dtypes.qint32)
+    copy_attr(quantized_batch_norm_node, "scale_after_normalization",
+              original_node.attr["scale_after_normalization"])
+    copy_attr(quantized_batch_norm_node, "variance_epsilon",
+              original_node.attr["variance_epsilon"])
+    self.add_output_graph_node(quantized_batch_norm_node)
+    quantize_down_name = self.add_quantize_down_nodes(original_node,
+                                                      quantized_batch_norm_name)
+    self.add_dequantize_result_node(quantize_down_name, original_node.name)
+
+  def add_output_graph_node(self, output_node):
+    """Inserts one node into the new graph."""
+    self.output_graph.node.extend([output_node])
+
+  def remove_redundant_quantization(self, old_graph):
+    """Removes unneeded pairs of quantize/dequantize ops from the graph.
+    This is a bit of a tricky function, because it's attempting to spot the
+    pattern of dequantizing from eight-bit up to float, and then immediately
+    quantizing back down to eight bits again, that's introduced by previous
+    passes that do 'key-hole' conversions of individual nodes but have to
+    convert back to float to match the previous output interface, since they
+    don't know that the next op can handle quantized tensors.
+    It works by:
+     - Looking for Quantize nodes.
+     - Checking to see if their first input is a Dequantize node.
+     - Seeing if their min/max inputs come from Min/Max nodes.
+     - Making sure those Min/Max nodes are being fed from the same Dequantize.
+     - Or that the Min is indirectly being fed from the same Dequantize as Max.
+     - Making sure the Dequantize is going through a Reshape (which we add
+       during the previous pass when we create the quantize sub-graph).
+     - Looking for the dims Const op for the Min/Max dims.
+    If all of these conditions are met, then it's a sub-graph pattern that
+    we know how to optimize out (and is likely the common one we've introduced).
+    We then rewire the graph to skip it entirely, and then rely on the dead node
+    removal pass to get rid of any nodes that are no longer needed.
+    Args:
+      old_graph: The model we'll be stripping redundant nodes from.
+    Returns:
+      A graph with the unnecessary nodes removed.
+    Raises:
+      ValueError: Two nodes with the same name were found in the graph.
+    """
+    old_nodes_map = self.create_nodes_map(old_graph)
+    self.output_graph = graph_pb2.GraphDef()
+    inputs_to_rename = {}
+    # We go through all the nodes, looking for any that match the patterns we
+    # know how to optimize away.
+    for node in old_graph.node:
+      # We always start with a Quantize node, and examine its inputs to see if
+      # they are in a form that can be removed.
+      if node.op not in ["Quantize", "QuantizeV2"]:
+        continue
+      dequantize_node_name = node_name_from_input(node.input[0])
+      if dequantize_node_name not in old_nodes_map:
+        raise ValueError("Input node name '" + dequantize_node_name +
+                         "' not found in node '" + node.name + "'")
+      dequantize_node = old_nodes_map[dequantize_node_name]
+      # Do we have a Dequantize feeding in, with the same type as the Quantize?
+      if dequantize_node.op != "Dequantize":
+        continue
+      if node.attr["T"] != dequantize_node.attr["T"]:
+        continue
+      # Now look at the other inputs, and ensure they're Min/Max nodes.
+      min_node_name = node_name_from_input(node.input[1])
+      max_node_name = node_name_from_input(node.input[2])
+      min_node = old_nodes_map[min_node_name]
+      max_node = old_nodes_map[max_node_name]
+      is_min_right_type = (min_node.op in ["Min", "Dequantize"])
+      is_max_right_type = (max_node.op in ["Max", "Dequantize"])
+      if not is_min_right_type or not is_max_right_type:
+        print("Didn't find expected types on inputs : %s, %s." % (min_node.op,
+                                                                  max_node.op))
+        continue
+      min_node_input_name = node_name_from_input(min_node.input[0])
+      max_node_input_name = node_name_from_input(max_node.input[0])
+      # There are two different patterns for Min nodes we can recognize, one
+      # where the input comes directly from the same one as the Max, and
+      # another where we run it through another Min first, so check for both.
+      is_same_input = False
+      if min_node_input_name == max_node_input_name:
+        is_same_input = True
+      else:
+        first_min_node_input = old_nodes_map[min_node_input_name]
+        if first_min_node_input.op == "Concat":
+          second_min_node_name = node_name_from_input(
+              first_min_node_input.input[1])
+          second_min_node = old_nodes_map[second_min_node_name]
+          if second_min_node.op == "Min":
+            second_min_node_input_name = node_name_from_input(
+                second_min_node.input[0])
+            is_same_input = (second_min_node_input_name == max_node_input_name)
+      if not is_same_input:
+        print("Different min/max inputs: " + min_node_input_name)
+        continue
+      # We recognize this pattern, so mark the graph edges to be rewired to
+      # route around it entirely, since we know it's a no-op.
+      dequantize_source_name = node_name_from_input(dequantize_node.input[0])
+      node_tensor_name = ensure_tensor_name_has_port(node.name)
+      min_tensor_name = node.name + ":1"
+      max_tensor_name = node.name + ":2"
+      inputs_to_rename[node_tensor_name] = dequantize_source_name
+      inputs_to_rename[min_tensor_name] = dequantize_node.input[1]
+      inputs_to_rename[max_tensor_name] = dequantize_node.input[2]
+    # Finally we apply all the rewiring we've marked to the graph.
+    for node in old_graph.node:
+      for index, input_full_name in enumerate(node.input):
+        input_name = ensure_tensor_name_has_port(input_full_name)
+        if input_name in inputs_to_rename:
+          node.input[index] = inputs_to_rename[input_name]
+      self.add_output_graph_node(node)
+    return self.output_graph
+
+  def apply_final_node_renames(self):
+    """Applies node renames in self.final_node_renames to self.output_graph."""
+    old_graph = self.output_graph
+    self.output_graph = graph_pb2.GraphDef()
+    for node in old_graph.node:
+      node.name = self.final_node_renames.get(node.name, node.name)
+      for index, input_name in enumerate(node.input):
+        node_name = node_name_from_input(input_name)
+        input_full_name = ensure_tensor_name_has_port(input_name)
+        if node_name in self.final_node_renames:
+          node.input[index] = "%s%s" % (self.final_node_renames[node_name],
+                                        input_full_name[len(node_name):])
+      self.add_output_graph_node(node)
+    return self.output_graph
+
+  def remove_dead_nodes(self, output_names):
+    """Removes nodes that are no longer needed for inference from the graph."""
+    old_output_graph = self.output_graph
+    self.output_graph = graph_util.extract_sub_graph(old_output_graph,
+                                                     output_names)
+
+  def quantize_weights(self, input_graph, quantization_mode):
+    """Quantize float Const ops.
+    There are two modes of operations, both replace float Const ops with
+    quantized values.
+    1. If quantization_mode is "weights_rounded", this function replaces float
+    Const ops with quantized float Const ops - same as the original op, but
+    float values being mapped to the center of one of 1<<FLAGS.bitdepth buckets.
+    This does not change the raw model size, but compression algorithms such as
+    zip (as used for compressing apks) or bzip2 will achieve a very good
+    compression ratio.
+    2. For other quantization modes ("MIN_COMBINED" or "MIN_FIRST"), float
+    Const ops are quantized and replaced by a tuple of four ops to perform
+    the dequantization at runtime:
+    * eight-bit Const (bucket indices, same shape as original float Const op
+    * two float Const ops (min and max value of original float Const op)
+    * Dequantize op to convert the eight-bit consts to float tensors.
+    The quantization mode is important because we see accuracy problems when
+    quantizing weights for different situations depending on the algorithm
+    used. We haven't figured out exactly what the underlying cause is yet,
+    unfortunately.
+    Args:
+      input_graph: A GraphDef of the model containing float Const ops.
+      quantization_mode: How to quantize and dequantize the values.
+    Returns:
+      A GraphDef of the converted graph.
+    Raises:
+      ValueError: If quantization_mode is unsupported.
+    """
+    output_graph = graph_pb2.GraphDef()
+    for input_node in input_graph.node:
+      should_quantize = False
+      if input_node.op == "Const":
+        dtype = dtypes.as_dtype(input_node.attr["dtype"].type)
+        if dtype == dtypes.float32:
+          should_quantize = True
+      if should_quantize:
+        if quantization_mode == "weights_rounded":
+          output_graph.node.extend(quantize_weight_rounded(input_node))
+        elif quantization_mode in (b"MIN_COMBINED", b"MIN_FIRST"):
+          output_graph.node.extend(
+              quantize_weight_eightbit(input_node, quantization_mode))
+        else:
+          raise ValueError("Unsupported quantization mode %s." %
+                           quantization_mode)
+      else:
+        output_node = node_def_pb2.NodeDef()
+        output_node.CopyFrom(input_node)
+        output_graph.node.extend([output_node])
+    return output_graph
+
+  def set_input_graph(self, new_input_graph):
+    self.input_graph = new_input_graph
+    self.nodes_map = self.create_nodes_map(self.input_graph)
+
+def main(unused_args):
+  if not gfile.Exists(FLAGS.input):
+    print("Input graph file '" + FLAGS.input + "' does not exist!")
+    return -1
+
+  known_modes = [
+      "round", "quantize", "eightbit", "weights", "test", "weights_rounded"
+  ]
+  if not any(FLAGS.mode in s for s in known_modes):
+    print("mode is '" + FLAGS.mode + "', not in " + ", ".join(known_modes) +
+          ".")
+    return -1
+
+  tf_graph = graph_pb2.GraphDef()
+  # TODO(intel-tf): Enabling user to work with both binary and text format.
+  mode = "rb" if FLAGS.input_binary else "r"
+  with gfile.Open(FLAGS.input, mode) as f:
+    data = f.read()
+    if FLAGS.input_binary:
+      tf_graph.ParseFromString(data)
+    else:
+      text_format.Merge(data, tf_graph)
+
+  graph = ops.Graph()
+  with graph.as_default():
+    importer.import_graph_def(tf_graph, input_map={}, name="")
+  quantized_input_range = None
+  if FLAGS.quantized_input:
+    quantized_input_range = [
+        FLAGS.quantized_input_min, FLAGS.quantized_input_max
+    ]
+
+  fallback_quantization_range = None
+  if (FLAGS.quantized_fallback_min is not None or
+      FLAGS.quantized_fallback_max is not None):
+    assert FLAGS.quantized_fallback_min is not None
+    assert FLAGS.quantized_fallback_max is not None
+    fallback_quantization_range = [
+        FLAGS.quantized_fallback_min, FLAGS.quantized_fallback_max
+    ]
+
+  rewriter = GraphRewriter(tf_graph, FLAGS.mode,
+                           quantized_input_range, fallback_quantization_range,
+                           FLAGS.intel_cpu_eightbitize)
+
+  output_graph = rewriter.rewrite(FLAGS.output_node_names.split(","))
+
+  # TODO(intel-tf): Enabling user to work with both binary and text format.
+  mode = "wb" if FLAGS.output_binary else "w"
+  f = gfile.FastGFile(FLAGS.output, mode)
+  if FLAGS.output_binary:
+    f.write(output_graph.SerializeToString())
+  else:
+    f.write(str(output_graph))
+
+  return 0
+
+if __name__ == "__main__":
+  app.run()
diff --git a/rl_loop/example_buffer.py b/rl_loop/example_buffer.py
index 14c7cef..40b467f 100644
--- a/rl_loop/example_buffer.py
+++ b/rl_loop/example_buffer.py
@@ -75,6 +75,10 @@ def file_timestamp(filename):
 def _ts_to_str(timestamp):
     return dt.datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
 
+def parallel_shuffle(i, example_list):
+    # random.shuffle on deque is O(n^2) convert to list for O(n)
+    random.shuffle(example_list)
+
 
 class ExampleBuffer():
     def __init__(self, max_size=2**21, sampling_frac=0.02):
@@ -92,10 +96,11 @@ class ExampleBuffer():
         if len(games) > max_games:
             games = games[-max_games:]
 
-        with mp.Pool(threads) as pool:
+        with mp.pool.ThreadPool(threads) as pool:
             res = tqdm(pool.imap(self.func, games), total=len(games))
             self.examples.extend(itertools.chain.from_iterable(res))
         print("Got", len(self.examples), "examples")
+        return len(self.examples)
 
     def update(self, new_games):
         """ new_games is a list of .tfrecord.zz new game records. """
@@ -126,6 +131,32 @@ class ExampleBuffer():
         self.examples.clear()
         self.examples = deque(maxlen=self.max_size)
 
+    def flush_new(self, path, example_num, num_out = 1, threads = 8):
+        # random.shuffle on deque is O(n^2) convert to list for O(n)
+        self.examples = list(self.examples)
+        example_list = [ex[1] for ex in self.examples]
+        length = example_num // num_out
+        example_list = example_list[:length*num_out]
+
+        i_list = []
+        for i in range(num_out):
+            i_list.append((i, example_list[i*length:(i+1)*length]))
+
+        with timer("Writing examples to " + path):
+            with mp.pool.ThreadPool(threads) as pool:
+                pool.starmap(parallel_shuffle, i_list)
+
+        i_list = []
+        for i in range(num_out):
+            i_list.append((path+'_'+str(i), example_list[i*length:(i+1)*length], False))
+
+        with timer("Writing examples to " + path):
+            with mp.pool.ThreadPool(num_out) as pool:
+                pool.starmap(preprocessing.write_tf_examples, i_list)
+
+        self.examples.clear()
+        self.examples = deque(maxlen=self.max_size)
+
     @property
     def count(self):
         return len(self.examples)
diff --git a/rl_loop/fsdb.py b/rl_loop/fsdb.py
index ab9d107..442692c 100644
--- a/rl_loop/fsdb.py
+++ b/rl_loop/fsdb.py
@@ -62,6 +62,7 @@ models_dir = _with_base('models')
 selfplay_dir = _with_base('data', 'selfplay')
 holdout_dir = _with_base('data', 'holdout')
 sgf_dir = _with_base('sgf')
+mpi_log_dir = _with_base('mpi')
 eval_dir = _with_base('sgf', 'eval')
 golden_chunk_dir = _with_base('data', 'golden_chunks')
 flags_path = _with_base('flags.txt')
diff --git a/run.sh b/run.sh
new file mode 100755
index 0000000..7cc74e7
--- /dev/null
+++ b/run.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+echo Physical cores = $PHY_CORES
+echo Virtual cores = $VIRT_CORES
+echo NUMA cores = $NUMA_CORES
+
+export KMP_HW_SUBSET=2T
+echo KMP_HW_SUBSET = $KMP_HW_SUBSET
+
+output_dir=${SCRATCH:-$(pwd)}
+echo Output to ${output_dir}
+
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY=compact,granularity=fine
+export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/cc/tensorflow
+ulimit -u 760000
+
+export PYTHONPATH=$(pwd)/ml_perf/tools/tensorflow_quantization/quantization:$PYTHONPATH
+
+./run_minigo.sh ${output_dir}/results/$(hostname) ml_perf/flags/9 $1
diff --git a/run_minigo.sh b/run_minigo.sh
new file mode 100755
index 0000000..d319d2e
--- /dev/null
+++ b/run_minigo.sh
@@ -0,0 +1,34 @@
+#!/bin/bash
+BASE_DIR=$1
+FLAG_DIR=$2
+
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$3 \
+  --train_node=localhost \
+  --setup_train_workers=True &> train_workers.log &
+
+# Run training loop
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$3 \
+  --train_node=localhost
+
+# Once the training loop has finished, run model evaluation to find the
+# first trained model that's better than the target
+BOARD_SIZE=9  python3  ml_perf/eval_models.py \
+  --base_dir=$BASE_DIR \
+  --flags_dir=$FLAG_DIR
diff --git a/run_minigo_mn.sh b/run_minigo_mn.sh
new file mode 100755
index 0000000..06e0633
--- /dev/null
+++ b/run_minigo_mn.sh
@@ -0,0 +1,51 @@
+#!/bin/bash
+BASE_DIR=$1
+FLAG_DIR=$2
+
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+NUM_NODES=`ml_perf/hostlist.sh|wc -l`
+TRAIN_NODES=$3
+EVAL_NODES=$4
+PLAY_NODES=$(expr $NUM_NODES - $TRAIN_NODES - $EVAL_NODES - 1)
+#EVAL_NODES=$PLAY_NODES
+TRAIN_PLUS_EVAL_NODES=$(expr $TRAIN_NODES + $EVAL_NODES)
+PLAY_NODES_PLUS_ONE=$(expr $PLAY_NODES + 1)
+echo train nodes $TRAIN_NODES
+echo eval nodes $EVAL_NODES
+echo play nodes $PLAY_NODES
+
+echo "BOARD_SIZE=9  python3  ml_perf/reference_implementation.py --setup_train_workers=True &> train_workers.log &"
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$5 \
+  `ml_perf/hostlist.sh |head -n $PLAY_NODES_PLUS_ONE|tail -n $PLAY_NODES|awk '/./{print "--selfplay_node="$0}'` \
+  `ml_perf/hostlist.sh |tail -n $TRAIN_NODES|awk '/./{print "--train_node="$0}'` \
+  `ml_perf/hostlist.sh |tail -n $TRAIN_PLUS_EVAL_NODES|head -n $EVAL_NODES |awk '/./{print "--eval_node="$0}'` \
+  --setup_train_workers=True &> train_workers.log &
+
+echo "BOARD_SIZE=9  python3  ml_perf/reference_implementation.py"
+# Run training loop
+BOARD_SIZE=9  python3  ml_perf/reference_implementation.py \
+  --base_dir=$BASE_DIR \
+  --flagfile=$FLAG_DIR/rl_loop.flags \
+  --physical_cores=$PHY_CORES \
+  --virtual_cores=$VIRT_CORES \
+  --numa_cores=$NUMA_CORES \
+  --quantization=$5 \
+  `ml_perf/hostlist.sh |head -n $PLAY_NODES_PLUS_ONE|tail -n $PLAY_NODES|awk '/./{print "--selfplay_node="$0}'` \
+  `ml_perf/hostlist.sh |tail -n $TRAIN_NODES|awk '/./{print "--train_node="$0}'` \
+  `ml_perf/hostlist.sh |tail -n $TRAIN_PLUS_EVAL_NODES|head -n $EVAL_NODES |awk '/./{print "--eval_node="$0}'`
+
+# Once the training loop has finished, run model evaluation to find the
+# first trained model that's better than the target
+BOARD_SIZE=9  python3  ml_perf/eval_models.py \
+  --base_dir=$BASE_DIR \
+  --flags_dir=$FLAG_DIR
diff --git a/run_mn.sh b/run_mn.sh
new file mode 100755
index 0000000..2628118
--- /dev/null
+++ b/run_mn.sh
@@ -0,0 +1,22 @@
+NUMA_COUNT=`cat /proc/cpuinfo |grep physical\ id|sort -u |wc -l`
+VIRT_CORES=`cat /proc/cpuinfo |grep physical\ id|wc -l`
+NUMA_CORES=`cat /proc/cpuinfo |grep cpu\ cores|head -n 1|awk '//{print $4}'`
+PHY_CORES=$(expr $NUMA_CORES \* $NUMA_COUNT)
+
+echo Physical cores = $PHY_CORES
+echo Virtual cores = $VIRT_CORES
+echo NUMA cores = $NUMA_CORES
+
+export KMP_HW_SUBSET=2T
+echo KMP_HW_SUBSET = $KMP_HW_SUBSET
+
+output_dir=${SCRATCH:-$(pwd)}
+echo Output to ${output_dir}
+
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY=compact,granularity=fine
+export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/cc/tensorflow
+export PYTHONPATH=$(pwd):$(pwd)/ml_perf/tools/tensorflow_quantization/quantization:$PYTHONPATH
+ulimit -u 760000
+
+./run_minigo_mn.sh ${output_dir}/results/$(hostname) ml_perf/flags/9.mn $1 $2 $3
diff --git a/set_avx2_build b/set_avx2_build
new file mode 100644
index 0000000..b60a540
--- /dev/null
+++ b/set_avx2_build
@@ -0,0 +1,61 @@
+#This file exports the bazel build opts for AVX2 platforms (broadwell and haswell). By setting -march=haswell and -mtune=broadwell, the binary will run on systems haswell and newer, but will be tuned for broadwell.
+
+MIN_GCC_MAJOR_VERSION=5
+MIN_GCC_MINOR_VERSION=3
+MIN_GCC_REVISION=0
+GCC_VERSION_STR=$(gcc -dumpversion)
+echo "GCC Version: ${GCC_VERSION_STR}"
+IFS='.' read -r -a GCC_VERSION <<< ${GCC_VERSION_STR}
+
+if [ "${GCC_VERSION[0]}" -lt "${MIN_GCC_MAJOR_VERSION}" ] ;
+then
+  echo "Your MAJOR version of GCC is too old: ${GCC_VERSION_STR}; it must be at least ${MIN_GCC_MAJOR_VERSION}.${MIN_GCC_MINOR_VERSION}.${MIN_GCC_REVISION}"
+  return 1
+
+elif [ "${GCC_VERSION[0]}" -eq "${MIN_GCC_MAJOR_VERSION}" ] ;
+then
+    if [ "${GCC_VERSION[1]}" -lt "${MIN_GCC_MINOR_VERSION}" ] ;
+    then
+      echo "Your MINOR version of GCC is too old: ${GCC_VERSION_STR}; it must be at least ${MIN_GCC_MAJOR_VERSION}.${MIN_GCC_MINOR_VERSION}."
+      return 1
+    fi
+fi
+
+echo "GCC ${GCC_VERSION_STR}: OK"
+
+#Don't use the C++11 ABI; use the old one 
+#These two options should be equivalent to all the options commented out below
+BAZEL_BUILD_OPTS_BASIC="--cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 \
+        --copt=-march=haswell \
+        --copt=-mtune=broadwell \
+                --copt=-O3"
+BAZEL_SECURE_BUILD_OPTS="--copt=-Wformat \
+                --copt=-Wformat-security \
+                --copt=-fstack-protector \
+                --copt=-fPIC \
+                --copt=-fpic \
+                --linkopt=-znoexecstack \
+                --linkopt=-zrelro \
+                --linkopt=-znow \
+                --linkopt=-fstack-protector \
+                --linkopt=-pie"
+
+#basic build flags
+echo "exporting BAZEL_BUILD_OPTS_BASIC=${BAZEL_BUILD_OPTS_BASIC}"
+export BAZEL_BUILD_OPTS_BASIC="${BAZEL_BUILD_OPTS_BASIC}"
+
+#secure build flags
+BAZEL_BUILD_OPTS="${BAZEL_BUILD_OPTS_BASIC} ${BAZEL_SECURE_BUILD_OPTS}"
+echo "exporting BAZEL_BUILD_OPTS=${BAZEL_BUILD_OPTS}"
+export BAZEL_BUILD_OPTS="${BAZEL_BUILD_OPTS}"
+
+#basic mkl flags
+BAZEL_MKL_BUILD_OPTS_BASIC="--config=mkl ${BAZEL_BUILD_OPTS_BASIC}"
+echo "exporting BAZEL_MKL_BUILD_OPTS_BASIC=${BAZEL_MKL_BUILD_OPTS_BASIC}"
+export BAZEL_MKL_BUILD_OPTS_BASIC="${BAZEL_MKL_BUILD_OPTS_BASIC}"
+
+#secure mkl flags
+BAZEL_SECURE_MKL_BUILD_OPTS="--config=mkl ${BAZEL_BUILD_OPTS}"
+echo "exporting BAZEL_SECURE_MKL_BUILD_OPTS=${BAZEL_SECURE_MKL_BUILD_OPTS}"
+export BAZEL_SECURE_MKL_BUILD_OPTS="${BAZEL_SECURE_MKL_BUILD_OPTS}"
+
diff --git a/testing/bootstrap_v2.sh b/testing/bootstrap_v2.sh
old mode 100644
new mode 100755
diff --git a/train.py b/train.py
index d6b24bc..3784d14 100644
--- a/train.py
+++ b/train.py
@@ -19,6 +19,7 @@ Usage:
 """
 
 import logging
+import os
 
 from absl import app, flags
 import numpy as np
@@ -28,6 +29,13 @@ import bigtable_input
 import dual_net
 import preprocessing
 import utils
+import time
+
+import ml_perf.mlp_log as mll
+import horovod.tensorflow as hvd
+import os
+from mpi4py import MPI
+import socket
 
 # See www.moderndescartes.com/essays/shuffle_viz for discussion on sizing
 flags.DEFINE_integer('shuffle_buffer_size', 2000,
@@ -47,6 +55,9 @@ flags.DEFINE_float('filter_amount', 1.0,
 flags.DEFINE_string('export_path', None,
                     'Where to export the model after training.')
 
+flags.DEFINE_string('data_path', None,
+                    'Where to get the data for training.')
+
 flags.DEFINE_bool('use_bt', False,
                   'Whether to use Bigtable as input.  '
                   '(Only supported with --use_tpu, currently.)')
@@ -54,6 +65,16 @@ flags.DEFINE_bool('use_bt', False,
 flags.DEFINE_bool('freeze', False,
                   'Whether to freeze the graph at the end of training.')
 
+flags.DEFINE_string('host_addr', None,
+                    'host address.')
+
+flags.DEFINE_bool('quantization', True, 'Using Int8 if true.')
+
+flags.DEFINE_bool('eval_min_max_every_epoch', True, 'Genereting min max log every epoch if true.')
+
+flags.DEFINE_boolean('random_rotation', True, 'Do random rotation when running for min&max log.')
+flags.DEFINE_integer('quantize_test_steps', 5, 'The steps to run for min&max log.')
+flags.DEFINE_integer('quantize_test_batch_size', 16, 'The batch size for running inference for min&max log.')
 
 flags.register_multi_flags_validator(
     ['use_bt', 'use_tpu'],
@@ -77,6 +98,8 @@ flags.declare_key_flag('work_dir')
 flags.declare_key_flag('train_batch_size')
 flags.declare_key_flag('num_tpu_cores')
 flags.declare_key_flag('use_tpu')
+flags.declare_key_flag('dist_train')
+flags.declare_key_flag('training_seed')
 
 FLAGS = flags.FLAGS
 
@@ -145,6 +168,8 @@ def train(*tf_records: "Records to train on"):
     estimator = dual_net.get_estimator()
 
     effective_batch_size = FLAGS.train_batch_size
+    if FLAGS.dist_train:
+        effective_batch_size = int(FLAGS.train_batch_size/hvd.size())
     if FLAGS.use_tpu:
         effective_batch_size *= FLAGS.num_tpu_cores
 
@@ -172,14 +197,17 @@ def train(*tf_records: "Records to train on"):
     else:
         def _input_fn():
             return preprocessing.get_input_tensors(
-                FLAGS.train_batch_size,
+                effective_batch_size,
                 tf_records,
                 filter_amount=FLAGS.filter_amount,
                 shuffle_buffer_size=FLAGS.shuffle_buffer_size,
-                random_rotation=True)
+                random_rotation=True, seed=FLAGS.training_seed,
+                dist_train=FLAGS.dist_train)
 
         hooks = [UpdateRatioSessionHook(FLAGS.work_dir),
                  EchoStepCounterHook(output_dir=FLAGS.work_dir)]
+        if FLAGS.dist_train:
+            hooks.append(hvd.BroadcastGlobalVariablesHook(0))
 
     steps = FLAGS.steps_to_train
     logging.info("Training, steps = %s, batch = %s -> %s examples",
@@ -206,22 +234,96 @@ def train(*tf_records: "Records to train on"):
             games.require_fresh_games(0)
         raise
 
+def get_golden_chunk_records(base):
+  pattern = os.path.join(base, '*.zz*')
+  window_size = FLAGS.window_size
+  return sorted(tf.gfile.Glob(pattern), reverse=True)[:window_size]
+
+def init_socket():
+  address = (FLAGS.host_addr, 52175)
+  server = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
+  server.bind(address)
+  server.listen(1)
+  return server
+
 
 def main(argv):
     """Train on examples and export the updated model weights."""
-    tf_records = argv[1:]
-    logging.info("Training on %s records: %s to %s",
-                 len(tf_records), tf_records[0], tf_records[-1])
-    with utils.logged_timer("Training"):
-        train(*tf_records)
-    if FLAGS.export_path:
-        dual_net.export_model(FLAGS.export_path)
-    if FLAGS.freeze:
-        if FLAGS.use_tpu:
-            dual_net.freeze_graph_tpu(FLAGS.export_path)
-        else:
-            dual_net.freeze_graph(FLAGS.export_path)
-
+    socket.setdefaulttimeout(99999999)
+    if FLAGS.dist_train:
+      comm_all = MPI.COMM_WORLD
+      mpi_rank = comm_all.Get_rank()
+      mpi_size = comm_all.Get_size()
+      hvd.init(comm_all)
+      if(mpi_rank==0):
+        server = init_socket()
+      comm_all.barrier()
+
+    print(FLAGS.host_addr)
+
+    mll.global_batch_size(FLAGS.train_batch_size)
+    mll.lr_rates(FLAGS.lr_rates)
+    mll.lr_boundaries(FLAGS.lr_boundaries)
+    effective_batch_size = FLAGS.train_batch_size
+    if FLAGS.dist_train:
+      effective_batch_size = int(FLAGS.train_batch_size/hvd.size())
+
+    tf_records_ph = tf.placeholder(tf.string)
+    data_iter = preprocessing.get_input_tensors_new(
+                        effective_batch_size,
+                        tf_records_ph,
+                        filter_amount=FLAGS.filter_amount,
+                        shuffle_buffer_size=FLAGS.shuffle_buffer_size,
+                        random_rotation=True, seed=FLAGS.training_seed,
+                        dist_train=FLAGS.dist_train)
+    features, labels = data_iter.get_next()
+    train_op = dual_net.model_fn_new(features, labels, tf.estimator.ModeKeys.TRAIN, FLAGS.flag_values_dict())
+    session_config = tf.ConfigProto(
+                        intra_op_parallelism_threads=FLAGS.num_intra_threads,
+                        inter_op_parallelism_threads=FLAGS.num_inter_threads)
+    session_config.gpu_options.allow_growth = True
+    sess = tf.Session(config=session_config)
+    tf.train.Saver().restore(sess, 'ml_perf/checkpoint/9/work_dir/model.ckpt-9383')
+
+    i = -1
+    while True:
+      # i start from 0 in the loop
+      i += 1
+      if(mpi_rank==0):
+        print('waiting for client...')
+        reception,addr = server.accept()
+        export_path = reception.recv(1024).decode();
+        if export_path == 'stop training':
+          break
+      comm_all.barrier()
+      tf_records = get_golden_chunk_records(FLAGS.data_path)
+      print("Training on {} records:".format(len(tf_records)))
+      for record in tf_records:
+        print("  {}".format(record))
+      start = time.time()
+      sess.run(data_iter.initializer, {tf_records_ph: tf_records})
+      step = 0
+      while True:
+        try:
+          step_start = time.time()
+          sess.run(train_op)
+          step = step+1
+          print ('step {} -- step/sec {}'.format(step, 1/(time.time()-step_start)))
+        except tf.errors.OutOfRangeError:
+          break
+      comm_all.barrier()
+      if hvd.rank() == 0:
+        tf.train.Saver().save(sess, export_path)
+        dual_net.optimize_graph(export_path + '.pb', export_path, FLAGS.quantization, FLAGS.data_path+'/*.zz*', FLAGS.eval_min_max_every_epoch, True)
+      finish = time.time()
+      if(mpi_rank==0):
+        reception.send('finish'.encode());
+        reception.close()
+      print ('run ', i, ': {:.3f}'.format(finish-start))
+
+    if(mpi_rank==0):
+      reception.send('finish'.encode());
+      server.close()
 
 if __name__ == "__main__":
     app.run(main)
